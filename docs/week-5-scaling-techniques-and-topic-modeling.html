<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chapter 5 Week 5: Scaling Techniques and Topic Modeling | PS9594A: Computational Text Analysis</title>

    <meta name="author" content="Dr. Sebastián Vallejo Vera | Western University" />
  
   <meta name="description" content="Code, exercises, and slides for PS9594A: Computational Text Analysis" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="Chapter 5 Week 5: Scaling Techniques and Topic Modeling | PS9594A: Computational Text Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises, and slides for PS9594A: Computational Text Analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Week 5: Scaling Techniques and Topic Modeling | PS9594A: Computational Text Analysis" />
  
  <meta name="twitter:description" content="Code, exercises, and slides for PS9594A: Computational Text Analysis" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.5.0/transition.js"></script>
    <script src="libs/bs3compat-0.5.0/tabs.js"></script>
    <script src="libs/bs3compat-0.5.0/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
  <style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
    
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">PS9594A: Computational Text Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<div id="week-5-scaling-techniques-and-topic-modeling" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Week 5: Scaling Techniques and Topic Modeling</h1>
<div id="slides-4" class="section level2 unnumbered">
<h2>Slides</h2>
<ul>
<li>6 Scaling Techniques and Topic Modeling (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/6%20Scaling%20Techniques%20and%20Topic%20Modeling.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup-4" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Setup</h2>
<p>As always, we first load the packages that we’ll be using:</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-1" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;conjugateprior/austin&quot;)</span></span>
<span id="cb262-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-2" tabindex="-1"></a><span class="fu">library</span>(austin) <span class="co"># just for those sweet wordscores</span></span>
<span id="cb262-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># for wrangling data</span></span>
<span id="cb262-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-4" tabindex="-1"></a><span class="fu">library</span>(tidylog) <span class="co"># to know what we are wrangling</span></span>
<span id="cb262-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-5" tabindex="-1"></a><span class="fu">library</span>(tidytext) <span class="co"># for &#39;tidy&#39; manipulation of text data</span></span>
<span id="cb262-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-6" tabindex="-1"></a><span class="fu">library</span>(quanteda) <span class="co"># tokenization power house</span></span>
<span id="cb262-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-7" tabindex="-1"></a><span class="fu">library</span>(quanteda.textmodels)</span>
<span id="cb262-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-8" tabindex="-1"></a><span class="fu">library</span>(stm) <span class="co"># run structural topic models</span></span>
<span id="cb262-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb262-9" tabindex="-1"></a><span class="fu">library</span>(wesanderson) <span class="co"># to prettify</span></span></code></pre></div>
</div>
<div id="wordscores" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Wordscores</h2>
<p>Laver et al. (2003) propose a supervised scaling technique called <em>wordscores</em>. We learned about the intuition in this weeks lecture. We will now replicate Table 1 from Laver and Benoit (2003) using the <code>austin</code> package. The package includes some sample data we will be using:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb263-1" tabindex="-1"></a><span class="fu">data</span>(lbg)</span></code></pre></div>
<p>Let’s keep only the reference documents:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb264-1" tabindex="-1"></a>ref <span class="ot">&lt;-</span> <span class="fu">getdocs</span>(lbg, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span>
<span id="cb264-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb264-2" tabindex="-1"></a>ref</span></code></pre></div>
<pre><code>##      docs
## words  R1  R2  R3  R4  R5
##    A    2   0   0   0   0
##    B    3   0   0   0   0
##    C   10   0   0   0   0
##    D   22   0   0   0   0
##    E   45   0   0   0   0
##    F   78   2   0   0   0
##    G  115   3   0   0   0
##    H  146  10   0   0   0
##    I  158  22   0   0   0
##    J  146  45   0   0   0
##    K  115  78   2   0   0
##    L   78 115   3   0   0
##    M   45 146  10   0   0
##    N   22 158  22   0   0
##    O   10 146  45   0   0
##    P    3 115  78   2   0
##    Q    2  78 115   3   0
##    R    0  45 146  10   0
##    S    0  22 158  22   0
##    T    0  10 146  45   0
##    U    0   3 115  78   2
##    V    0   2  78 115   3
##    W    0   0  45 146  10
##    X    0   0  22 158  22
##    Y    0   0  10 146  45
##    Z    0   0   3 115  78
##    ZA   0   0   2  78 115
##    ZB   0   0   0  45 146
##    ZC   0   0   0  22 158
##    ZD   0   0   0  10 146
##    ZE   0   0   0   3 115
##    ZF   0   0   0   2  78
##    ZG   0   0   0   0  45
##    ZH   0   0   0   0  22
##    ZI   0   0   0   0  10
##    ZJ   0   0   0   0   3
##    ZK   0   0   0   0   2</code></pre>
<p>This is the same matrix from Figure 1, where we have a count for the count of each word (in this case, letters) by reference document (the documents that have been previously labelled). We can give scores <code>A_scores</code> to each reference text to place them in a ideological scale (or whatever scale we want). We then estimate the wordscores for each word.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb266-1" tabindex="-1"></a><span class="co"># We do this in the order of the reference texts:</span></span>
<span id="cb266-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb266-2" tabindex="-1"></a>A_score <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="sc">-</span><span class="fl">0.75</span>,<span class="dv">0</span>,<span class="fl">0.75</span>,<span class="fl">1.5</span>)</span>
<span id="cb266-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb266-3" tabindex="-1"></a>ws <span class="ot">&lt;-</span> <span class="fu">classic.wordscores</span>(ref, <span class="at">scores=</span>A_score)</span>
<span id="cb266-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb266-4" tabindex="-1"></a>ws<span class="sc">$</span>pi</span></code></pre></div>
<pre><code>##         Score
## A  -1.5000000
## B  -1.5000000
## C  -1.5000000
## D  -1.5000000
## E  -1.5000000
## F  -1.4812500
## G  -1.4809322
## H  -1.4519231
## I  -1.4083333
## J  -1.3232984
## K  -1.1846154
## L  -1.0369898
## M  -0.8805970
## N  -0.7500000
## O  -0.6194030
## P  -0.4507576
## Q  -0.2992424
## R  -0.1305970
## S   0.0000000
## T   0.1305970
## U   0.2992424
## V   0.4507576
## W   0.6194030
## X   0.7500000
## Y   0.8805970
## Z   1.0369898
## ZA  1.1846154
## ZB  1.3232984
## ZC  1.4083333
## ZD  1.4519231
## ZE  1.4809322
## ZF  1.4812500
## ZG  1.5000000
## ZH  1.5000000
## ZI  1.5000000
## ZJ  1.5000000
## ZK  1.5000000</code></pre>
<p>Now we get the virgin text and predict the textscore by estimating the average of the weighted wordscores for the virgin document:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb268-1" tabindex="-1"></a>vir <span class="ot">&lt;-</span> <span class="fu">getdocs</span>(lbg, <span class="st">&#39;V1&#39;</span>)</span>
<span id="cb268-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb268-2" tabindex="-1"></a>vir</span></code></pre></div>
<pre><code>##      docs
## words  V1
##    A    0
##    B    0
##    C    0
##    D    0
##    E    0
##    F    0
##    G    0
##    H    2
##    I    3
##    J   10
##    K   22
##    L   45
##    M   78
##    N  115
##    O  146
##    P  158
##    Q  146
##    R  115
##    S   78
##    T   45
##    U   22
##    V   10
##    W    3
##    X    2
##    Y    0
##    Z    0
##    ZA   0
##    ZB   0
##    ZC   0
##    ZD   0
##    ZE   0
##    ZF   0
##    ZG   0
##    ZH   0
##    ZI   0
##    ZJ   0
##    ZK   0</code></pre>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb270-1" tabindex="-1"></a><span class="co"># predict textscores for the virgin documents</span></span>
<span id="cb270-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb270-2" tabindex="-1"></a><span class="fu">predict</span>(ws, <span class="at">newdata=</span>vir)</span></code></pre></div>
<pre><code>## 37 of 37 words (100%) are scorable
## 
##     Score Std. Err. Rescaled  Lower  Upper
## V1 -0.448    0.0119   -0.448 -0.459 -0.437</code></pre>
<p>Cool.</p>
</div>
<div id="wordfish" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Wordfish</h2>
<p>Slapin and Proksch (2008) propose an unsupervised scaling model that places texts in a one-dimensional scale. The underlying assumption is that in</p>
<p><span class="math display">\[w_{ik} ∼ Poisson(\lambda _{ik})\]</span>
<span class="math display">\[\lambda_{ik} = exp(α_i +ψ_k +β_k ×θ_i)\]</span></p>
<p><span class="math inline">\(\lambda_{ik}\)</span> is generated by <span class="math inline">\(α_i\)</span> (the “loquaciousness” of politician <span class="math inline">\(i\)</span> or document fixed-effects), <span class="math inline">\(ψ_k\)</span> (the frequency of word k), <span class="math inline">\(β_k\)</span> (the discrimination parameter of word <span class="math inline">\(k\)</span>) and, most importantly, <span class="math inline">\(θ_i\)</span> (the politician’s ideological position). Let’s believe for a moment that the peer-review system works and use the <code>textmodel_wordfish()</code> function to estimate the ideological positions of U.S. Presidents using their inaugural speeches.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb272-1" tabindex="-1"></a>us_pres <span class="ot">&lt;-</span> readxl<span class="sc">::</span><span class="fu">read_xlsx</span>(<span class="at">path =</span> <span class="st">&quot;data/inaugTexts.xlsx&quot;</span>)</span>
<span id="cb272-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb272-2" tabindex="-1"></a><span class="fu">head</span>(us_pres)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
##   inaugSpeech                                               Year President party
##   &lt;chr&gt;                                                    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;
## 1 &quot;My Countrymen, It a relief to feel that no heart but m…  1853 Pierce    Demo…
## 2 &quot;Fellow citizens, I appear before you this day to take …  1857 Buchanan  Demo…
## 3 &quot;Fellow-Citizens of the United States: In compliance wi…  1861 Lincoln   Repu…
## 4 &quot;Fellow-Countrymen:\r\n\r\nAt this second appearing to …  1865 Lincoln   Repu…
## 5 &quot;Citizens of the United States:\r\n\r\nYour suffrages h…  1869 Grant     Repu…
## 6 &quot;Fellow-Citizens:\r\n\r\nUnder Providence I have been c…  1873 Grant     Repu…</code></pre>
<p>The text is pretty clean, so we can change it into a corpus object and then a <code>dfm</code> and apply <code>textmodel_wordfish()</code>:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb274-1" tabindex="-1"></a>corpus_us_pres <span class="ot">&lt;-</span> <span class="fu">corpus</span>(us_pres,</span>
<span id="cb274-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb274-2" tabindex="-1"></a>                     <span class="at">text_field =</span> <span class="st">&quot;inaugSpeech&quot;</span>,</span>
<span id="cb274-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb274-3" tabindex="-1"></a>                     <span class="at">unique_docnames =</span> <span class="cn">TRUE</span>)</span>
<span id="cb274-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb274-4" tabindex="-1"></a></span>
<span id="cb274-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb274-5" tabindex="-1"></a><span class="fu">summary</span>(corpus_us_pres)</span></code></pre></div>
<pre><code>## Corpus consisting of 41 documents, showing 41 documents:
## 
##    Text Types Tokens Sentences Year    President      party
##   text1  1164   3631       104 1853       Pierce   Democrat
##   text2   944   3080        89 1857     Buchanan   Democrat
##   text3  1074   3992       135 1861      Lincoln Republican
##   text4   359    774        26 1865      Lincoln Republican
##   text5   484   1223        40 1869        Grant Republican
##   text6   551   1469        43 1873        Grant Republican
##   text7   830   2698        59 1877        Hayes Republican
##   text8  1020   3206       111 1881     Garfield Republican
##   text9   675   1812        44 1885    Cleveland   Democrat
##  text10  1351   4720       157 1889     Harrison Republican
##  text11   821   2125        58 1893    Cleveland   Democrat
##  text12  1231   4345       130 1897     McKinley Republican
##  text13   854   2437       100 1901     McKinley Republican
##  text14   404   1079        33 1905  T Roosevelt Republican
##  text15  1437   5821       158 1909         Taft Republican
##  text16   658   1882        68 1913       Wilson   Democrat
##  text17   548   1648        59 1917       Wilson   Democrat
##  text18  1168   3717       148 1921      Harding Republican
##  text19  1220   4440       196 1925     Coolidge Republican
##  text20  1089   3855       158 1929       Hoover Republican
##  text21   742   2052        85 1933 FD Roosevelt   Democrat
##  text22   724   1981        96 1937 FD Roosevelt   Democrat
##  text23   525   1494        68 1941 FD Roosevelt   Democrat
##  text24   274    619        27 1945 FD Roosevelt   Democrat
##  text25   780   2495       116 1949       Truman   Democrat
##  text26   899   2729       119 1953   Eisenhower Republican
##  text27   620   1883        92 1957   Eisenhower Republican
##  text28   565   1516        52 1961      Kennedy   Democrat
##  text29   567   1697        93 1965      Johnson   Democrat
##  text30   742   2395       103 1969        Nixon Republican
##  text31   543   1978        68 1973        Nixon Republican
##  text32   527   1363        52 1977       Carter   Democrat
##  text33   901   2771       129 1981       Reagan Republican
##  text34   924   2897       124 1985       Reagan Republican
##  text35   794   2666       141 1989         Bush Republican
##  text36   642   1833        81 1993      Clinton   Democrat
##  text37   772   2423       111 1997      Clinton   Democrat
##  text38   620   1804        97 2001         Bush Republican
##  text39   773   2321       100 2005         Bush Republican
##  text40   937   2667       110 2009        Obama   Democrat
##  text41   814   2317        88 2013        Obama   Democrat</code></pre>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-1" tabindex="-1"></a><span class="co"># We do the whole tokenization sequence</span></span>
<span id="cb276-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-2" tabindex="-1"></a>toks_us_pres <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_us_pres,</span>
<span id="cb276-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-3" tabindex="-1"></a>                   <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>, <span class="co"># Thinks about this</span></span>
<span id="cb276-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-4" tabindex="-1"></a>                   <span class="at">remove_punct =</span> <span class="cn">TRUE</span>, <span class="co"># Remove punctuation!</span></span>
<span id="cb276-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-5" tabindex="-1"></a>                   <span class="at">remove_url =</span> <span class="cn">TRUE</span>) <span class="co"># Might be helpful</span></span>
<span id="cb276-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-6" tabindex="-1"></a></span>
<span id="cb276-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-7" tabindex="-1"></a>toks_us_pres <span class="ot">&lt;-</span> <span class="fu">tokens_remove</span>(toks_us_pres,</span>
<span id="cb276-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-8" tabindex="-1"></a>                              <span class="co"># Should we though? See Denny and Spirling (2018)</span></span>
<span id="cb276-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-9" tabindex="-1"></a>                              <span class="fu">c</span>(<span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">&quot;en&quot;</span>)),</span>
<span id="cb276-10"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-10" tabindex="-1"></a>                              <span class="at">padding =</span> F)</span>
<span id="cb276-11"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-11" tabindex="-1"></a></span>
<span id="cb276-12"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-12" tabindex="-1"></a>toks_us_pres <span class="ot">&lt;-</span> <span class="fu">tokens_wordstem</span>(toks_us_pres, <span class="at">language =</span> <span class="st">&quot;en&quot;</span>)</span>
<span id="cb276-13"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-13" tabindex="-1"></a></span>
<span id="cb276-14"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-14" tabindex="-1"></a>dfm_us_pres <span class="ot">&lt;-</span> <span class="fu">dfm</span>(toks_us_pres)</span>
<span id="cb276-15"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-15" tabindex="-1"></a></span>
<span id="cb276-16"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-16" tabindex="-1"></a>wfish_us_pres <span class="ot">&lt;-</span> <span class="fu">textmodel_wordfish</span>(dfm_us_pres, <span class="at">dir =</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">30</span>)) <span class="co">#Does not really matter what the starting values are, they just serve as anchors for the relative position of the rest of the texts. In this case, I chose Kennedy and Nixon.  </span></span>
<span id="cb276-17"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb276-17" tabindex="-1"></a><span class="fu">summary</span>(wfish_us_pres)</span></code></pre></div>
<pre><code>## 
## Call:
## textmodel_wordfish.dfm(x = dfm_us_pres, dir = c(28, 30))
## 
## Estimated Document Positions:
##           theta      se
## text1  -0.95629 0.03619
## text2  -1.27077 0.03413
## text3  -1.40878 0.02863
## text4  -0.37189 0.08907
## text5  -1.19374 0.05617
## text6  -0.98770 0.05747
## text7  -1.25058 0.03681
## text8  -1.15827 0.03506
## text9  -1.06959 0.04866
## text10 -1.37028 0.02601
## text11 -1.09563 0.04336
## text12 -1.36432 0.02716
## text13 -0.96921 0.04392
## text14  0.14969 0.07992
## text15 -1.67350 0.01837
## text16  0.04362 0.05968
## text17 -0.14872 0.06481
## text18 -0.23053 0.04050
## text19 -0.64321 0.03619
## text20 -0.81689 0.03636
## text21 -0.26532 0.05470
## text22  0.26579 0.05564
## text23  0.56628 0.06543
## text24  0.82820 0.09574
## text25  0.09676 0.04999
## text26  0.37480 0.04777
## text27  0.60678 0.05655
## text28  0.92039 0.05651
## text29  0.95967 0.05604
## text30  1.42421 0.03773
## text31  0.93162 0.05098
## text32  0.87564 0.06215
## text33  1.13335 0.04084
## text34  1.19832 0.03745
## text35  1.26112 0.03960
## text36  1.38125 0.04374
## text37  1.37962 0.03680
## text38  0.86932 0.05416
## text39  0.74023 0.04862
## text40  1.18811 0.03946
## text41  1.05044 0.04446
## 
## Estimated Feature Scores:
##      countrymen  relief    feel   heart    can   know  person  regret   bitter
## beta    -0.5490 -0.9578 -0.4909 0.80654 0.1395 0.9422 -0.1117 -0.2531  0.00357
## psi     -0.5455 -1.8438 -0.3950 0.08677 2.1155 0.6955 -0.1544 -2.8008 -2.01944
##       sorrow    born   posit suitabl  other    rather   desir circumst   call
## beta  0.5176  0.7626 -0.9823  -4.120 0.3938  0.005296 -0.6213  -0.6355 0.3744
## psi  -2.0333 -0.8259 -1.2129  -6.134 0.1718 -0.591942 -0.5404  -1.3620 0.5610
##         limit    period presid destini   republ    fill profound    sens
## beta -0.07365 -0.004602 0.4897  0.2596 -0.27095  0.6832 -0.05067 -0.1156
## psi  -0.02462 -1.040467 0.4726 -0.2598  0.08116 -1.6845 -1.52213 -0.2337
##      respons     noth    like  shrink
## beta  0.2263 -0.08437 0.01591  0.1689
## psi   0.8392 -0.25100 0.19881 -1.7082</code></pre>
<p>Let’s see if this made any sense. Since we have the party of the president, we should see the Republican cluster together and apart from Democrats (or something):</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-1" tabindex="-1"></a><span class="co"># Get predictions:</span></span>
<span id="cb278-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-2" tabindex="-1"></a>wfish_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(wfish_us_pres, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb278-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-3" tabindex="-1"></a></span>
<span id="cb278-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-4" tabindex="-1"></a><span class="co"># Tidy everything up:</span></span>
<span id="cb278-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-5" tabindex="-1"></a>posi_us_pres <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">docvars</span>(corpus_us_pres),</span>
<span id="cb278-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-6" tabindex="-1"></a>                      wfish_preds<span class="sc">$</span>fit) <span class="sc">%&gt;%</span></span>
<span id="cb278-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-7" tabindex="-1"></a>  <span class="fu">arrange</span>(fit)</span>
<span id="cb278-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-8" tabindex="-1"></a></span>
<span id="cb278-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-9" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb278-10"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-10" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb278-11"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> fit, <span class="at">y =</span> <span class="fu">reorder</span>(President,fit), <span class="at">xmin =</span> lwr, <span class="at">xmax =</span> upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb278-12"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-12" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb278-13"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-13" tabindex="-1"></a>  <span class="fu">geom_errorbarh</span>(<span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb278-14"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-14" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb278-15"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-15" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb278-16"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-16" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb278-17"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb278-17" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
<p>Two things to note. First, the <em>direction</em> of the scale is a theoretically-based decision that the researcher has to make (not the algorithm). In our case, based on the results, we could say that the positive values are more left-leaning and the negative values are more right-leaning. We can switch that (for visualization purposes) just by multiplying by -1:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-1" tabindex="-1"></a><span class="co"># Plot inverse</span></span>
<span id="cb279-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-2" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb279-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span>fit, <span class="at">y =</span> <span class="fu">reorder</span>(President,fit), <span class="at">xmin =</span> <span class="sc">-</span>lwr, <span class="at">xmax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb279-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb279-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-5" tabindex="-1"></a>  <span class="fu">geom_errorbarh</span>(<span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb279-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb279-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb279-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb279-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb279-9" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-84-1.png" width="672" />
Second, there seems to be a mismatch between our theoretical expectations and our empirical observations. We would assume that Republicans (Democrats) will talk similarly to other Republicans (Democrats) and different from Democrats (Republicans). However, this is not the case. <em>What could be happening?</em></p>
<p>One answer could be that language changes over time or that issues change over time or that what it meant to be a Democrat and Republican changed over time, and that change is being picked up by the model:</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-1" tabindex="-1"></a><span class="co"># Plot time</span></span>
<span id="cb280-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-2" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb280-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="sc">-</span>fit, <span class="at">x =</span> Year, <span class="at">ymin =</span> <span class="sc">-</span>lwr, <span class="at">ymax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb280-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb280-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-5" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>() <span class="sc">+</span></span>
<span id="cb280-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb280-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb280-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb280-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb280-9" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>That seems to be one possible explanation. The other could be that the pre-processing steps substantively modified the texts (see Denny and Spirling 2018). We can estimate the model again using a different pre-processed text:</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-1" tabindex="-1"></a><span class="co"># Tokenization only removing punctuation</span></span>
<span id="cb281-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-2" tabindex="-1"></a>toks_us_pres2 <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_us_pres,</span>
<span id="cb281-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-3" tabindex="-1"></a>                   <span class="at">remove_punct =</span> <span class="cn">TRUE</span>) </span>
<span id="cb281-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-4" tabindex="-1"></a></span>
<span id="cb281-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-5" tabindex="-1"></a>dfm_us_pres2 <span class="ot">&lt;-</span> <span class="fu">dfm</span>(toks_us_pres2)</span>
<span id="cb281-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-6" tabindex="-1"></a>wfish_us_pres <span class="ot">&lt;-</span> <span class="fu">textmodel_wordfish</span>(dfm_us_pres2, <span class="at">dir =</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">30</span>))  </span>
<span id="cb281-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-7" tabindex="-1"></a></span>
<span id="cb281-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-8" tabindex="-1"></a><span class="co"># Get predictions:</span></span>
<span id="cb281-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-9" tabindex="-1"></a>wfish_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(wfish_us_pres, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb281-10"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-10" tabindex="-1"></a></span>
<span id="cb281-11"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-11" tabindex="-1"></a><span class="co"># Tidy everything up:</span></span>
<span id="cb281-12"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-12" tabindex="-1"></a>posi_us_pres <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">docvars</span>(corpus_us_pres),</span>
<span id="cb281-13"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-13" tabindex="-1"></a>                      wfish_preds<span class="sc">$</span>fit) <span class="sc">%&gt;%</span></span>
<span id="cb281-14"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-14" tabindex="-1"></a>  <span class="fu">arrange</span>(fit)</span>
<span id="cb281-15"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-15" tabindex="-1"></a></span>
<span id="cb281-16"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-16" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb281-17"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-17" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb281-18"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-18" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span>fit, <span class="at">y =</span> <span class="fu">reorder</span>(President,fit), <span class="at">xmin =</span> <span class="sc">-</span>lwr, <span class="at">xmax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb281-19"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-19" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb281-20"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-20" tabindex="-1"></a>  <span class="fu">geom_errorbarh</span>(<span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb281-21"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-21" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb281-22"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-22" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb281-23"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-23" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb281-24"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb281-24" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions (No Pre-Processing&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-86-1.png" width="672" />
At the very least, the within president differences in estimates have narrowed, but time seems to still be the best predictor:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-1" tabindex="-1"></a><span class="co"># Plot time</span></span>
<span id="cb282-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-2" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb282-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="sc">-</span>fit, <span class="at">x =</span> Year, <span class="at">ymin =</span> <span class="sc">-</span>lwr, <span class="at">ymax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb282-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb282-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-5" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>() <span class="sc">+</span></span>
<span id="cb282-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb282-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb282-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb282-9"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb282-9" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>If time is the main predictor, then maybe we need to think about periods of time that are comparable for both parties (e.g., after the Civil Rights Act).</p>
</div>
<div id="structural-topic-models" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Structural Topic Models</h2>
<p>STM provides two ways to include contextual information to “guide” the estimation of the model. First, topic prevalence can vary by metadata (e.g. Republicans talk about military issues more than Democrats). Second, topic content can vary by metadata (e.g. Republicans talk about military issues differently from Democrats).</p>
<p>We can run STM using the <code>stm</code> package. The <code>stm</code> package includes the complete workflow (i.e. from raw text to figures), and if you are planning to use it in the future I highly encourage you to check <a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">this</a> and <a href="https://www.jstor.org/stable/pdf/24363543.pdf?casa_token=b_rJjIOUUScAAAAA:KXNQeVBQMzB7-kIEhl-1qo6uyD7vHvRTHhMinMdZVT6G3M3olzKzPv00XMJQd7mRw9Nm9UqJDmWHv3N_0cXBmbdeu2XZv8-jy1RYxvpm7Ab3WEOmApXP">this</a> and <a href="https://juliasilge.com/blog/evaluating-stm/">this</a> and <a href="https://juliasilge.com/blog/sherlock-holmes-stm/">this</a>. <code>stm()</code> takes our <em>dfm</em> and produces topics. If we do not specify any prevalence terms, then it will estimate an LDA. Since this is a Bayesian approach, it is recommended you set a seed value for future replication. We also need to set <span class="math inline">\(K\)</span> number of topics. How many topics are the right number of topics? There is no good number. Too many pre-specified topics and the categories might be meaningless. Too few, and you might be piling together two or more topics. Note that changes to a) the number of topics, b) the prevalence term, c) the omitted words, d) the seed value, can (greatly) change the outcome. Here is where validation becomes crucial (for a review see <a href="https://www.researchgate.net/profile/Andreu_Casas/publication/317140610_Large-Scale_Computerized_Text_Analysis_in_Political_Science_Opportunities_and_Challenges/links/59285e6f0f7e9b9979a35ec4/Large-Scale-Computerized-Text-Analysis-in-Political-Science-Opportunities-and-Challenges.pdf">Wilkerson and Casas 2017</a>).</p>
<p>Using our presidential speeches data, I will use <code>stm</code> to estimate the topics surrounding the inaugural addresses. As my prevalence term, I add the party of the speaker. I set my number of topics at 10 (but with a corpus this big I should probably set it at ~30 and work my way up from there).</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb283-1" tabindex="-1"></a>stm_us_pres <span class="ot">&lt;-</span> <span class="fu">stm</span>(dfm_us_pres, <span class="at">K =</span> <span class="dv">10</span>, <span class="at">seed =</span> <span class="dv">1984</span>,</span>
<span id="cb283-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb283-2" tabindex="-1"></a>                   <span class="at">prevalence =</span> <span class="sc">~</span>party,</span>
<span id="cb283-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb283-3" tabindex="-1"></a>                   <span class="at">init.type =</span> <span class="st">&quot;Spectral&quot;</span>)</span></code></pre></div>
<pre><code>## Beginning Spectral Initialization 
##   Calculating the gram matrix...
##   Finding anchor words...
##      ..........
##   Recovering initialization...
##      ..............................................
## Initialization complete.
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 1 (approx. per word bound = -7.071) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 2 (approx. per word bound = -6.881, relative change = 2.689e-02) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 3 (approx. per word bound = -6.819, relative change = 8.961e-03) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 4 (approx. per word bound = -6.790, relative change = 4.253e-03) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 5 (approx. per word bound = -6.780, relative change = 1.521e-03) 
## Topic 1: us, new, world, nation, let 
##  Topic 2: new, can, us, nation, work 
##  Topic 3: constitut, state, union, can, law 
##  Topic 4: nation, must, us, peopl, can 
##  Topic 5: govern, peopl, upon, state, law 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, shall 
##  Topic 9: world, nation, peopl, peac, can 
##  Topic 10: us, nation, govern, must, peopl 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 6 (approx. per word bound = -6.775, relative change = 6.929e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 7 (approx. per word bound = -6.771, relative change = 5.320e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 8 (approx. per word bound = -6.768, relative change = 5.267e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 9 (approx. per word bound = -6.765, relative change = 4.328e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 10 (approx. per word bound = -6.763, relative change = 3.022e-04) 
## Topic 1: us, new, world, let, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, union, can, shall 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, world, peopl, peac, can 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 11 (approx. per word bound = -6.762, relative change = 2.094e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 12 (approx. per word bound = -6.760, relative change = 1.745e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 13 (approx. per word bound = -6.759, relative change = 1.485e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 14 (approx. per word bound = -6.759, relative change = 1.147e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 15 (approx. per word bound = -6.758, relative change = 1.018e-04) 
## Topic 1: us, new, let, world, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, union, can, shall 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 16 (approx. per word bound = -6.757, relative change = 9.728e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 17 (approx. per word bound = -6.757, relative change = 8.328e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 18 (approx. per word bound = -6.756, relative change = 7.150e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 19 (approx. per word bound = -6.756, relative change = 5.364e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 20 (approx. per word bound = -6.756, relative change = 3.903e-05) 
## Topic 1: us, new, let, world, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, shall, union 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 21 (approx. per word bound = -6.755, relative change = 3.678e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 22 (approx. per word bound = -6.755, relative change = 3.378e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 23 (approx. per word bound = -6.755, relative change = 3.008e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 24 (approx. per word bound = -6.755, relative change = 3.311e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 25 (approx. per word bound = -6.754, relative change = 3.247e-05) 
## Topic 1: us, new, let, world, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 26 (approx. per word bound = -6.754, relative change = 2.886e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 27 (approx. per word bound = -6.754, relative change = 2.778e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 28 (approx. per word bound = -6.754, relative change = 2.814e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 29 (approx. per word bound = -6.754, relative change = 4.342e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 30 (approx. per word bound = -6.753, relative change = 2.111e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 31 (approx. per word bound = -6.753, relative change = 1.621e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 32 (approx. per word bound = -6.753, relative change = 1.678e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 33 (approx. per word bound = -6.753, relative change = 1.686e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 34 (approx. per word bound = -6.753, relative change = 1.705e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 35 (approx. per word bound = -6.753, relative change = 1.793e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 36 (approx. per word bound = -6.753, relative change = 2.033e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 37 (approx. per word bound = -6.753, relative change = 2.216e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 38 (approx. per word bound = -6.752, relative change = 1.878e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 39 (approx. per word bound = -6.752, relative change = 1.591e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 40 (approx. per word bound = -6.752, relative change = 1.384e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 41 (approx. per word bound = -6.752, relative change = 1.337e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 42 (approx. per word bound = -6.752, relative change = 1.381e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 43 (approx. per word bound = -6.752, relative change = 1.388e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 44 (approx. per word bound = -6.752, relative change = 1.720e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 45 (approx. per word bound = -6.752, relative change = 1.363e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 46 (approx. per word bound = -6.752, relative change = 1.725e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 47 (approx. per word bound = -6.752, relative change = 1.410e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Model Converged</code></pre>
<p>The nice thing about the <code>stm()</code> function is that it allows us to see in “real-time” what is going on within the black box. We can summarize the process in the following way (this is similar to a collapsed Gibbs sampling, which the <code>stm()</code> function sort of uses):</p>
<ol style="list-style-type: decimal">
<li><p>Go through each document, and randomly assign each word in the document to one of the topics <span class="math inline">\(\displaystyle t\in k\)</span>.</p></li>
<li><p>Notice that this random assignment already gives both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).</p></li>
<li><p>So to improve on them, for each document <span class="math inline">\(\displaystyle W\)</span> do the following:</p></li>
</ol>
<p>3.1 Go through each word <span class="math inline">\(\displaystyle w\)</span> in <span class="math inline">\(\displaystyle W\)</span></p>
<p>3.1.1 And for each topic <span class="math inline">\(\displaystyle t\)</span>, compute two things:</p>
<p>3.1.1.1 <span class="math inline">\(\displaystyle p(t|W)\)</span> = the proportion of words in document <span class="math inline">\(\displaystyle W\)</span> that are currently assigned to topic <span class="math inline">\(\displaystyle t\)</span>, and</p>
<p>3.1.1.2 <span class="math inline">\(\displaystyle p(w|t)\)</span> = the proportion of assignments to topic <span class="math inline">\(\displaystyle t\)</span> over all documents that come from this word <span class="math inline">\(\displaystyle w\)</span>. Reassign <span class="math inline">\(\displaystyle w\)</span> a new topic, where we choose topic <span class="math inline">\(\displaystyle t\)</span> with probability <span class="math inline">\(\displaystyle p(t|W)*p(w|t)\)</span>. It is worth noting that according to our generative model, this is essentially the probability that topic <span class="math inline">\(\displaystyle t\)</span> generated word <span class="math inline">\(\displaystyle w\)</span>, so it makes sense that we resample the current word’s topic with this probability. (Also, I’m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)</p>
<p>3.1.1.3 In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.</p>
<ol start="4" style="list-style-type: decimal">
<li>After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated with each topic (by counting the proportion of words assigned to each topic overall).</li>
</ol>
<p>(This explanation was taken from <a href="https://wiki.ubc.ca/Course:CPSC522/Latent_Dirichlet_Allocation#cite_note-rcode-4">here</a>). Let’s explore the topics produced:</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb285-1" tabindex="-1"></a><span class="fu">labelTopics</span>(stm_us_pres)</span></code></pre></div>
<pre><code>## Topic 1 Top Words:
##       Highest Prob: us, new, let, nation, world, can, america 
##       FREX: let, centuri, togeth, dream, new, promis, weak 
##       Lift: 200th, 20th, micah, rhetor, 18th, 19th, accident 
##       Score: role, dream, abroad, third, explor, shape, proud 
## Topic 2 Top Words:
##       Highest Prob: us, new, can, nation, work, world, great 
##       FREX: friend, mr, thing, breez, word, blow, fact 
##       Lift: breez, crucial, addict, alloc, assistanc, bacteria, bicentenni 
##       Score: breez, crucial, blow, page, manger, thank, sometim 
## Topic 3 Top Words:
##       Highest Prob: constitut, state, govern, peopl, shall, can, law 
##       FREX: case, constitut, slave, union, territori, slaveri, minor 
##       Lift: alleg, anarchi, besid, compulsori, constru, cover, dissatisfi 
##       Score: case, slaveri, territori, slave, invas, provis, fli 
## Topic 4 Top Words:
##       Highest Prob: nation, peopl, must, us, world, can, govern 
##       FREX: activ, republ, task, confid, industri, inspir, normal 
##       Lift: abnorm, acclaim, aright, changer, comiti, frugal, gaze 
##       Score: normal, activ, amid, readjust, self-reli, relationship, unshaken 
## Topic 5 Top Words:
##       Highest Prob: govern, peopl, upon, law, state, countri, nation 
##       FREX: revenu, tariff, offic, appoint, busi, consider, proper 
##       Lift: ampli, antitrust, board, box, boycott, congression, dakota 
##       Score: revenu, legisl, enforc, polici, negro, interst, tariff 
## Topic 6 Top Words:
##       Highest Prob: nation, freedom, america, peopl, govern, know, democraci 
##       FREX: democraci, ideal, million, liberti, freedom, came, seen 
##       Lift: paint, &gt;, aught, autocrat, baffl, baggag, beli 
##       Score: democraci, paint, million, magna, excus, seen, encount 
## Topic 7 Top Words:
##       Highest Prob: us, must, america, nation, american, world, peopl 
##       FREX: journey, stori, generat, storm, america, job, ideal 
##       Lift: afghanistan, aids, alongsid, anchor, anybodi, apathi, appalachia 
##       Score: stori, journey, job, capitol, storm, thank, drift 
## Topic 8 Top Words:
##       Highest Prob: upon, nation, govern, peopl, can, shall, great 
##       FREX: enforc, counsel, organ, island, thought, upon, integr 
##       Lift: cuba, eighteenth, adapt, aspect, creation, cuban, dilig 
##       Score: enforc, island, cuba, counsel, organ, eighteenth, adapt 
## Topic 9 Top Words:
##       Highest Prob: nation, peopl, world, can, peac, must, free 
##       FREX: resourc, contribut, repres, everywher, result, free, europ 
##       Lift: display, joint, likewis, mockeri, philosophi, array, barter 
##       Score: europ, philosophi, commun, contribut, precept, tax, program 
## Topic 10 Top Words:
##       Highest Prob: us, govern, nation, peopl, world, must, american 
##       FREX: weapon, tax, believ, hero, man, reduc, dream 
##       Lift: 50th, absent, adam, alamo, anger, ant, artilleri 
##       Score: weapon, hero, monument, nuclear, spend, tax, soviet</code></pre>
<p><em>FREX</em> weights words by their overall frequency and how exclusive they are to the topic. <em>Lift</em> weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics. Similar to Lift, <em>Score</em> divides the log frequency of the word in the topic by the log frequency of the word in other topics <a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">(Roberts et al. 2013)</a>. <a href="https://icml.cc/2012/papers/113.pdf">Bischof and Airoldi (2012)</a> show the value of using <strong>FREX</strong> over the other measures.</p>
<p>You can use the <code>plot()</code> function to show the topics.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb287-1" tabindex="-1"></a><span class="fu">plot</span>(stm_us_pres, <span class="at">type =</span> <span class="st">&quot;summary&quot;</span>, <span class="at">labeltype =</span> <span class="st">&quot;frex&quot;</span>) <span class="co"># or prob, lift score</span></span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<p>Topic 5 seems to be about the economy: revenue, tariffs, etc. Topic 3 about slavery adn the Civil War. If you want to see a sample of a specific topic:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb288-1" tabindex="-1"></a><span class="fu">findThoughts</span>(stm_us_pres, <span class="at">texts =</span> <span class="fu">as.character</span>(corpus_us_pres)[<span class="fu">docnames</span>(dfm_us_pres)], <span class="at">topics =</span> <span class="dv">3</span>)  </span></code></pre></div>
<p>That is a long speech.</p>
<p>We can (should/must) run some diagnostics. There are two qualities that were are looking for in our model: semantic coherence and exclusivity. Exclusivity is based on the FREX labeling matrix. Semantic coherence is a criterion developed by Mimno et al. (2011) and it maximizes when the most probable words in a given topic frequently co-occur together. Mimno et al. (2011) show that the metric correlates well with human judgement of topic quality. Yet, it is fairly easy to obtain high semantic coherence so it is important to see it in tandem with exclusivity. Let’s see how exclusive are the words in each topic:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb289-1" tabindex="-1"></a><span class="fu">dotchart</span>(<span class="fu">exclusivity</span>(stm_us_pres), <span class="at">labels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<p>We can also see the semantic coherence of our topics –words a topic generates should co-occur often in the same document–:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb290-1" tabindex="-1"></a><span class="fu">dotchart</span>(<span class="fu">semanticCoherence</span>(stm_us_pres,dfm_us_pres), <span class="at">labels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>We can also see the overall quality of our topic model:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb291-1" tabindex="-1"></a><span class="fu">topicQuality</span>(stm_us_pres,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -5.287875  -7.035510 -12.913601  -3.154439  -8.562729 -11.770514
##  [7]  -4.095783  -5.495206  -5.782951  -4.794982
##  [1] 8.975188 9.296784 8.794789 8.229003 7.886663 9.119321 8.616780 7.907198
##  [9] 8.689432 8.813805</code></pre>
<p><img src="main_files/figure-html/Quality-1.png" width="672" /></p>
<p>On their own, both metrics are not really useful (what do those numbers even mean?). They are useful when we are looking for the “optimal” number of topics.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb293-1" tabindex="-1"></a>stm_us_pres_10_15_20 <span class="ot">&lt;-</span> <span class="fu">manyTopics</span>(dfm_us_pres,</span>
<span id="cb293-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb293-2" tabindex="-1"></a>                       <span class="at">prevalence =</span> <span class="sc">~</span> party,</span>
<span id="cb293-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb293-3" tabindex="-1"></a>                       <span class="at">K =</span> <span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>), <span class="at">runs=</span><span class="dv">2</span>,</span>
<span id="cb293-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb293-4" tabindex="-1"></a>                       <span class="co"># max.em.its = 100, </span></span>
<span id="cb293-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb293-5" tabindex="-1"></a>                       <span class="at">init.type =</span> <span class="st">&quot;Spectral&quot;</span>) <span class="co"># It takes around 250 iterations for the model to converge. Depending on your computer, this can take a while.</span></span></code></pre></div>
<p>We can now compare the performance of each model based on their semantic coherence and exclusivity. We are looking for high exclusivity and high coherence (top-right corner):</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb294-1" tabindex="-1"></a>k_10 <span class="ot">&lt;-</span> stm_us_pres_10_15_20<span class="sc">$</span>out[[<span class="dv">1</span>]] <span class="co"># k_10 is an stm object which can be explored and used like any other topic model. </span></span>
<span id="cb294-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb294-2" tabindex="-1"></a>k_15 <span class="ot">&lt;-</span> stm_us_pres_10_15_20<span class="sc">$</span>out[[<span class="dv">2</span>]]</span>
<span id="cb294-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb294-3" tabindex="-1"></a>k_20 <span class="ot">&lt;-</span> stm_us_pres_10_15_20<span class="sc">$</span>out[[<span class="dv">3</span>]]</span>
<span id="cb294-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb294-4" tabindex="-1"></a></span>
<span id="cb294-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb294-5" tabindex="-1"></a><span class="co"># I will just graph the &#39;quality&#39; of each model:</span></span>
<span id="cb294-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb294-6" tabindex="-1"></a><span class="fu">topicQuality</span>(k_10,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -5.287875  -7.035510 -12.913601  -3.154439  -8.562729 -11.770514
##  [7]  -4.095783  -5.495206  -5.782951  -4.794982
##  [1] 8.975188 9.296784 8.794789 8.229003 7.886663 9.119321 8.616780 7.907198
##  [9] 8.689432 8.813805</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb296-1" tabindex="-1"></a><span class="fu">topicQuality</span>(k_15,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -8.282551 -10.661122  -9.146329  -6.243444 -10.002100 -11.315179
##  [7]  -3.107797  -4.907182  -5.059424  -4.905652  -7.864316 -13.149897
## [13]  -6.834348 -11.917696  -4.182962
##  [1] 9.225426 9.359212 9.252890 9.186251 9.037698 9.150213 8.615448 8.497762
##  [9] 8.545416 9.139213 8.183189 9.136856 8.467946 9.642172 8.453394</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-95-2.png" width="672" /></p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb298-1" tabindex="-1"></a><span class="fu">topicQuality</span>(k_20,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -8.136428 -22.245476 -21.390006  -6.602534 -11.543624 -10.272049
##  [7]  -3.923380  -5.506620  -7.188791 -12.486262 -10.086060 -13.443443
## [13] -15.978725 -12.256070 -10.137597 -11.231218  -6.177453  -4.358259
## [19]  -5.246579  -2.209688
##  [1] 9.489401 9.872037 9.761442 9.184861 9.370893 9.330946 9.019413 8.522081
##  [9] 8.633818 9.649097 8.307636 9.125968 8.957859 9.644505 9.533281 8.824664
## [17] 9.488938 9.220756 8.596374 8.727104</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-95-3.png" width="672" /></p>
<p>Maybe we have some theory about the difference in topic prevalence across parties. We can see the topic proportions in our topic model object:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb300-1" tabindex="-1"></a><span class="fu">head</span>(stm_us_pres<span class="sc">$</span>theta)</span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]         [,4]         [,5]
## [1,] 0.0001965139 8.910433e-05 8.224181e-05 1.366525e-04 0.0003038924
## [2,] 0.0004943115 6.694931e-05 9.823350e-01 1.233605e-04 0.0162390724
## [3,] 0.0002929573 4.935711e-05 9.988060e-01 1.794408e-05 0.0005358752
## [4,] 0.1143189826 9.475844e-04 8.765515e-01 2.637466e-04 0.0030686131
## [5,] 0.0117265212 2.101959e-04 6.658082e-03 1.163485e-03 0.9768804322
## [6,] 0.0254848264 3.421690e-04 4.654306e-03 1.751075e-03 0.9609666414
##              [,6]         [,7]         [,8]         [,9]        [,10]
## [1,] 2.109828e-04 1.271428e-04 9.985770e-01 1.726828e-04 1.037666e-04
## [2,] 1.673617e-04 1.837545e-04 2.369797e-04 9.412580e-05 5.907198e-05
## [3,] 6.424073e-05 5.105122e-05 6.933715e-05 6.752421e-05 4.572869e-05
## [4,] 7.946389e-04 8.965244e-04 7.116956e-04 1.080859e-03 1.365866e-03
## [5,] 5.210418e-04 4.736609e-04 8.391293e-04 8.456329e-04 6.818197e-04
## [6,] 7.226817e-04 6.610786e-04 1.740575e-03 1.314907e-03 2.361741e-03</code></pre>
<p>Note that the prevalence terms <span class="math inline">\(\theta\)</span> will add to 1 within a document. That is, the term tells us the proportion of (words associated with) topics for each document:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb302-1" tabindex="-1"></a><span class="fu">sum</span>(stm_us_pres<span class="sc">$</span>theta[<span class="dv">1</span>,])</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb304-1" tabindex="-1"></a><span class="fu">sum</span>(stm_us_pres<span class="sc">$</span>theta[<span class="dv">2</span>,])</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>What about connecting this info to our dfm and seeing if there are differences in the proportion topic 5 (economy) is addressed by each side.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb306-1" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb306-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb306-2" tabindex="-1"></a><span class="fu">library</span>(sjPlot)</span></code></pre></div>
<pre><code>## #refugeeswelcome</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb308-1" tabindex="-1"></a>us_pres_prev <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">topic5 =</span> stm_us_pres<span class="sc">$</span>theta[,<span class="dv">5</span>], <span class="fu">docvars</span>(dfm_us_pres))</span>
<span id="cb308-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb308-2" tabindex="-1"></a>feols_topic5 <span class="ot">&lt;-</span> <span class="fu">feols</span>(topic5 <span class="sc">~</span> party , <span class="at">data =</span> us_pres_prev)</span>
<span id="cb308-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb308-3" tabindex="-1"></a><span class="fu">plot_model</span>(feols_topic5, <span class="at">type =</span> <span class="st">&quot;pred&quot;</span>, <span class="at">term =</span> <span class="st">&quot;party&quot;</span>) <span class="sc">+</span></span>
<span id="cb308-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb308-4" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb308-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb308-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">&quot;Stat. Sig. at p&lt;0.1&quot;</span>, <span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y=</span><span class="st">&quot;Topic Prevalence&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
<p>Seems that Republican presidents address more the economy in their speeches. Let’s plot the proportion of by president:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-1" tabindex="-1"></a>us_pres_prev <span class="sc">%&gt;%</span></span>
<span id="cb309-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-2" tabindex="-1"></a>  <span class="co"># Going to log the prev of topic 5 because is quite skewed but you should probably leave as is if you want to explore how topics are addressed. </span></span>
<span id="cb309-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(topic5), <span class="at">y =</span> <span class="fu">reorder</span>(President,topic5), <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb309-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb309-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;log(Theta)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb309-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-6" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb309-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb309-7" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() </span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<p>We can do something similar with the <code>stm</code> function directly. We just need to specify the functional form and add the document variables.</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-1" tabindex="-1"></a>topics_us_pres <span class="ot">&lt;-</span> <span class="fu">estimateEffect</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>) <span class="sc">~</span> party, stm_us_pres, <span class="fu">docvars</span>(dfm_us_pres)) <span class="co"># You can compare other topics by changing c(6,9). </span></span>
<span id="cb310-2"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-2" tabindex="-1"></a><span class="fu">plot</span>(topics_us_pres, <span class="st">&quot;party&quot;</span>, <span class="at">method =</span> <span class="st">&quot;difference&quot;</span>,</span>
<span id="cb310-3"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-3" tabindex="-1"></a>     <span class="at">cov.value1 =</span> <span class="st">&quot;Democrat&quot;</span>, </span>
<span id="cb310-4"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-4" tabindex="-1"></a>     <span class="at">cov.value2 =</span> <span class="st">&quot;Republican&quot;</span>,</span>
<span id="cb310-5"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-5" tabindex="-1"></a>     <span class="at">labeltype =</span> <span class="st">&quot;custom&quot;</span>,</span>
<span id="cb310-6"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-6" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">75</span>,.<span class="dv">25</span>),</span>
<span id="cb310-7"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-7" tabindex="-1"></a>     <span class="at">custom.labels =</span> <span class="fu">c</span>(<span class="st">&#39;Topic 3: Slavery&#39;</span>, <span class="st">&#39;Topic 5: Economy&#39;</span>),</span>
<span id="cb310-8"><a href="week-5-scaling-techniques-and-topic-modeling.html#cb310-8" tabindex="-1"></a>     <span class="at">model =</span> stm_us_pres)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>Same results, Republicans mention more Topic 5: Economy.</p>
</div>
<div id="exercise-2" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Exercise 2:</h2>
<ol style="list-style-type: decimal">
<li>We had a hard time scaling our text. Why looked at some possible problems. What are possible solutions if we want to position U.S. presidents in a ideological scale using text?</li>
<li>Use the <code>data/candidate-tweets.csv</code> data to run a STM. Decide what your covariates are going. Decide whether you will use all the data or a sample of the data. Decide if you are going to aggregate/divide in some way the text (i.e., decide your unit of analysis). Decide the number of topics you will look for (try more than one option). What can you tell me about the topics tweeted out by the 2015 U.S. primaries candidates?</li>
<li>Choose three topics. Can you place the candidates in an ideological scale within each topic (determine the <span class="math inline">\(theta\)</span> threshold for when you can say that a tweet is <em>mostly</em> about a topic)? Does it make sense? Why or why not?</li>
</ol>

</div>
</div>
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>PS9594A: Computational Text Analysis</strong>" was written by Dr. Sebastián Vallejo Vera | Western University. It was last built on 2024-01-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
