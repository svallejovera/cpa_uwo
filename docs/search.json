[{"path":"index.html","id":"computational-text-analysis","chapter":"“Computational Text Analysis”","heading":"“Computational Text Analysis”","text":" Welcome site course PS9594A: “Computational Text Analysis” Western University, taught Sebastián Vallejo Vera. week, find code, exercises, slides corresponding topic.start, check required software packages . Also, don’t forget read Syllabus check Perusall readings course. site corrected/updated throughout semester.","code":""},{"path":"index.html","id":"software-and-packages","chapter":"“Computational Text Analysis”","heading":"0.1 Software and Packages","text":"first part course (Weeks 1 - 5), mainly using R. second part course (Weeks 6 - 11), use combination R Python. assume familiar R language, RStudio, R packages. , please come office hours can help out1. R, main packages need installed:tidyverse (piping)tidylog (helps keep track pipins)tidytext (great working text)quanteda (stands “Quantitative Analysis Textual Data”)\nquanteda.textstats (obtain stats dfm)\nquanteda.textplots (obtain plots dfm stats)\nquanteda.dictionaries (use dictionaries quanteda)\nquanteda.textstats (obtain stats dfm)quanteda.textplots (obtain plots dfm stats)quanteda.dictionaries (use dictionaries quanteda)gutenbergr (download texts Project Gutenberg)wesanderson (make things pretty)stm (run Structural Topic Models)","code":""},{"path":"index.html","id":"datasets","chapter":"“Computational Text Analysis”","heading":"0.2 Datasets","text":"Throughout class, using number sample datasets. Access datasets provided directly code. Final Essay, can use one following datasets (, even better, can use ):Data Ventura et al. (2021) - “Connective Effervescence Streaming Chat Political Debates”: Link replication materialData Project Gutenberg: gutenbergrOpen-ended question 2021 Canadian Election Study survey: Link replication materialOpen-ended questions ANES surveys: Link ANES homepage","code":""},{"path":"index.html","id":"acknowledgments","chapter":"“Computational Text Analysis”","heading":"0.3 Acknowledgments","text":"organization first part course (Weeks 1 - 5) format assignments borrowed Christopher Barrie’s excellent course “Computational Text Analysis”, syllabus prolific Tiago Ventura, Grimmer, Roberts, Stewart’s excellent book, “Text data: new framework machine learning social sciences”. code used throughout course patchwork code, code borrows heavily internet (’s true code). try best give credit original authors code (possible).","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"week-1-a-primer-on-using-text-as-data","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1 Week 1: A Primer on Using Text as Data","text":"","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"slides","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"Slides","text":"1 Introduction CTA (link Perusall)2 Computational Text Analysis (link Perusall)","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"setup","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.1 Setup","text":"first example, replicate (extend) Mendenhall’s (1887) Mendenhall’s (1901) studies word-length distribution.\nFigure 1.1: Mendenhall (1987) - Characteristic Curves Composition.\nFirst load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(wesanderson) # to prettify\nlibrary(gutenbergr) # to get some books\nlibrary(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"get-data","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.2 Get Data","text":"Mendenhall (1887) argued “every writer makes use vocabulary peculiar , character materially change year \nyear productive,” one characteristics length words. Mendenhall (1901) takes , suggests , given assumption, Shakespeare Bacon person2.Let’s get corpus–collection documents–can analyze. can search Gutenberg repository create corpus selected work.","code":"\ngutenberg_metadata %>%\n  filter(author == \"Wilde, Oscar\")## # A tibble: 66 × 8\n##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n##           <int> <chr>    <chr>                <int> <chr>    <chr>              \n##  1          174 The Pic… Wilde…                 111 en       \"Gothic Fiction/Mo…\n##  2          301 The Bal… Wilde…                 111 en       \"\"                 \n##  3          773 Lord Ar… Wilde…                 111 en       \"Contemporary Revi…\n##  4          774 Essays … Wilde…                 111 en       \"\"                 \n##  5          790 Lady Wi… Wilde…                 111 en       \"\"                 \n##  6          844 The Imp… Wilde…                 111 en       \"Plays\"            \n##  7          854 A Woman… Wilde…                 111 en       \"Plays\"            \n##  8          873 A House… Wilde…                 111 en       \"Opera\"            \n##  9          875 The Duc… Wilde…                 111 en       \"\"                 \n## 10          885 An Idea… Wilde…                 111 en       \"Plays\"            \n## # ℹ 56 more rows\n## # ℹ 2 more variables: rights <chr>, has_text <lgl>"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"word-length-in-wildes-corpus","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.3 Word Length in Wilde’s Corpus","text":"’s lot Wilde! Let’s focus four plays: “Importance Earnest”, “Woman Importance”, “Lady Windermere’s Fan”, “Ideal Husband”. can download plays using ID number:unit analysis something like line. interested word—also known token—lengths play. clean unwanted text—text add noise analysis—count number words.Now, can change unit analysis token:’s lot words! now create column word length, count number words length (play!).Let’s see distribution play:problem. ?solution (proposed Mendenhall):look . Mendenhall something: author mark terms word length distribution. Wilde, observable change across time (play published different years). , happens compare Wilde’s mark Shakespeare’s? Let’s choose four plays (random) Shakespeare: Midsummer Night’s Dream, Merchant Venice, Much Ado Nothing, Tempest.","code":"\nwilde <- gutenberg_download(c(790,844, 854, 885), \n                            meta_fields = c(\"title\",\"author\"))\nprint(n=25,wilde[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text                                        title         author\n##           <int> <chr>                                       <chr>         <chr> \n##  1          790 \"\"                                          Lady Winderm… Wilde…\n##  2          790 \"\"                                          Lady Winderm… Wilde…\n##  3          790 \"THE PERSONS OF THE PLAY\"                   Lady Winderm… Wilde…\n##  4          790 \"\"                                          Lady Winderm… Wilde…\n##  5          790 \"\"                                          Lady Winderm… Wilde…\n##  6          790 \"Lord Windermere\"                           Lady Winderm… Wilde…\n##  7          790 \"\"                                          Lady Winderm… Wilde…\n##  8          790 \"Lord Darlington\"                           Lady Winderm… Wilde…\n##  9          790 \"\"                                          Lady Winderm… Wilde…\n## 10          790 \"Lord Augustus Lorton\"                      Lady Winderm… Wilde…\n## 11          790 \"\"                                          Lady Winderm… Wilde…\n## 12          790 \"Mr. Dumby\"                                 Lady Winderm… Wilde…\n## 13          790 \"\"                                          Lady Winderm… Wilde…\n## 14          790 \"Mr. Cecil Graham\"                          Lady Winderm… Wilde…\n## 15          790 \"\"                                          Lady Winderm… Wilde…\n## 16          790 \"Mr. Hopper\"                                Lady Winderm… Wilde…\n## 17          790 \"\"                                          Lady Winderm… Wilde…\n## 18          790 \"Parker, Butler\"                            Lady Winderm… Wilde…\n## 19          790 \"\"                                          Lady Winderm… Wilde…\n## 20          790 \"                                * * * * *\" Lady Winderm… Wilde…\n## 21          790 \"\"                                          Lady Winderm… Wilde…\n## 22          790 \"Lady Windermere\"                           Lady Winderm… Wilde…\n## 23          790 \"\"                                          Lady Winderm… Wilde…\n## 24          790 \"The Duchess of Berwick\"                    Lady Winderm… Wilde…\n## 25          790 \"\"                                          Lady Winderm… Wilde…\nwilde <- wilde %>%\n  # Some housekeeping \n  mutate(title = ifelse(str_detect(title,\"Importance of Being\"),\"The Importance of Being Earnest\", title)) %>%\n  # Filter out all empty rows\n  filter(text != \"\") %>%\n  # This is a play. The name of each character before they speak \n  filter(str_detect(text,\"[A-Z]{3,}\")==FALSE)## mutate: changed 3,884 values (27%) of 'title' (0 new NA)## filter: removed 4,232 rows (29%), 10,303 rows remaining## filter: removed 4,208 rows (41%), 6,095 rows remaining\nprint(n=25,wilde[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text                                                title author\n##           <int> <chr>                                               <chr> <chr> \n##  1          790 \"tea-table L._  _Window opening on to terrace L._ … Lady… Wilde…\n##  2          790 \"home to any one who calls.\"                        Lady… Wilde…\n##  3          790 \"                                                 … Lady… Wilde…\n##  4          790 \"he’s come.\"                                        Lady… Wilde…\n##  5          790 \"hands with you.  My hands are all wet with these … Lady… Wilde…\n##  6          790 \"lovely?  They came up from Selby this morning.\"    Lady… Wilde…\n##  7          790 \"table_.]  And what a wonderful fan!  May I look a… Lady… Wilde…\n##  8          790 \"everything.  I have only just seen it myself.  It… Lady… Wilde…\n##  9          790 \"present to me.  You know to-day is my birthday?\"   Lady… Wilde…\n## 10          790 \"life, isn’t it?  That is why I am giving this par… Lady… Wilde…\n## 11          790 \"down.  [_Still arranging flowers_.]\"               Lady… Wilde…\n## 12          790 \"birthday, Lady Windermere.  I would have covered … Lady… Wilde…\n## 13          790 \"front of your house with flowers for you to walk … Lady… Wilde…\n## 14          790 \"you.\"                                              Lady… Wilde…\n## 15          790 \"                                                 … Lady… Wilde…\n## 16          790 \"Foreign Office.  I am afraid you are going to ann… Lady… Wilde…\n## 17          790 \"with her pocket-handkerchief_, _goes to tea-table… Lady… Wilde…\n## 18          790 \"Won’t you come over, Lord Darlington?\"             Lady… Wilde…\n## 19          790 \"miserable, Lady Windermere.  You must tell me wha… Lady… Wilde…\n## 20          790 \"table L._]\"                                        Lady… Wilde…\n## 21          790 \"whole evening.\"                                    Lady… Wilde…\n## 22          790 \"that the only pleasant things to pay _are_ compli… Lady… Wilde…\n## 23          790 \"things we _can_ pay.\"                              Lady… Wilde…\n## 24          790 \"You mustn’t laugh, I am quite serious.  I don’t l… Lady… Wilde…\n## 25          790 \"don’t see why a man should think he is pleasing a… Lady… Wilde…\nwilde_words <- wilde %>%\n  # take the column text and divide it by words\n  unnest_tokens(word, text) %>%\n  # Remove some characters that add noise\n  mutate(word = str_remove_all(word, \"\\\\_\")) ## mutate: changed 1,225 values (2%) of 'word' (0 new NA)\nwilde_words## # A tibble: 60,465 × 4\n##    gutenberg_id title                 author       word     \n##           <int> <chr>                 <chr>        <chr>    \n##  1          790 Lady Windermere's Fan Wilde, Oscar by       \n##  2          790 Lady Windermere's Fan Wilde, Oscar sixteenth\n##  3          790 Lady Windermere's Fan Wilde, Oscar edition  \n##  4          790 Lady Windermere's Fan Wilde, Oscar first    \n##  5          790 Lady Windermere's Fan Wilde, Oscar published\n##  6          790 Lady Windermere's Fan Wilde, Oscar 1893     \n##  7          790 Lady Windermere's Fan Wilde, Oscar first    \n##  8          790 Lady Windermere's Fan Wilde, Oscar issued   \n##  9          790 Lady Windermere's Fan Wilde, Oscar by       \n## 10          790 Lady Windermere's Fan Wilde, Oscar methuen  \n## # ℹ 60,455 more rows\nwilde_words_ct <- wilde_words %>%\n  # Length of each word\n  mutate(word_length = str_length(word)) %>%\n  # Group by word_length and count how many \n  group_by(word_length,title) %>%\n  mutate(total_word_length = n()) %>%\n  # Keep relevant\n  distinct(word_length,title,.keep_all=T) %>%\n  select(word_length,title,author,total_word_length)## mutate: new variable 'word_length' (integer) with 17 unique values and 0% NA## group_by: 2 grouping variables (word_length, title)## mutate (grouped): new variable 'total_word_length' (integer) with 58 unique values and 0% NA## distinct (grouped): removed 60,403 rows (>99%), 62 rows remaining## select: dropped 2 variables (gutenberg_id, word)\nwilde_words_ct %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\")\nwilde_words %>%\n  group_by(title) %>%\n  slice_sample(n=10000) %>%\n  mutate(word_length = str_length(word),\n         median_word_length = median(word_length)) %>%\n  group_by(word_length,title) %>%\n  mutate(total_word_length = n()) %>%\n  distinct(word_length,title,.keep_all=T) %>%\n  select(word_length,title,author,total_word_length,median_word_length) %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  geom_vline(aes(xintercept = median_word_length,color=title,linetype = title))+\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\", linetype = \"\",\n       caption = \"Note: Line type shows median word length.\")## group_by: one grouping variable (title)## slice_sample (grouped): removed 20,465 rows (34%), 40,000 rows remaining## mutate (grouped): new variable 'word_length' (integer) with 17 unique values and 0% NA##                   new variable 'median_word_length' (double) with one unique value and 0% NA## group_by: 2 grouping variables (word_length, title)## mutate (grouped): new variable 'total_word_length' (integer) with 55 unique values and 0% NA## distinct (grouped): removed 39,938 rows (>99%), 62 rows remaining## select: dropped 2 variables (gutenberg_id, word)"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"comparing-shakespeare-and-wilde","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.4 Comparing Shakespeare and Wilde","text":"text cleaner Wilde’s corpus, leave . Also, harder systematically remove name person speaking. problem? ? ?can put together corpora see differences distributions word length.differences? can conclude evidence? limitations approach? alternative approaches study Mendenhall getting ?","code":"\nshakes <- gutenberg_download(c(1520,2242,2243,2235),\n                             meta_fields = c(\"title\",\"author\"))\nprint(n=25,shakes[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text                                                title author\n##           <int> <chr>                                               <chr> <chr> \n##  1         1520 \"Leon.\"                                             Much… Shake…\n##  2         1520 \"How many gentlemen have you lost in this action?\"  Much… Shake…\n##  3         1520 \"\"                                                  Much… Shake…\n##  4         1520 \"Mess.\"                                             Much… Shake…\n##  5         1520 \"But few of any sort, and none of name.\"            Much… Shake…\n##  6         1520 \"\"                                                  Much… Shake…\n##  7         1520 \"Leon.\"                                             Much… Shake…\n##  8         1520 \"A victory is twice itself when the achiever bring… Much… Shake…\n##  9         1520 \"numbers.  I find here that Don Pedro hath bestowe… Much… Shake…\n## 10         1520 \"a young Florentine, called Claudio.\"               Much… Shake…\n## 11         1520 \"\"                                                  Much… Shake…\n## 12         1520 \"Mess.\"                                             Much… Shake…\n## 13         1520 \"Much deserved on his part, and equally remembered… Much… Shake…\n## 14         1520 \"He hath borne himself beyond the promise of his a… Much… Shake…\n## 15         1520 \"in the figure of a lamb, the feats of a lion: he … Much… Shake…\n## 16         1520 \"better bettered expectation than you must expect … Much… Shake…\n## 17         1520 \"you how.\"                                          Much… Shake…\n## 18         1520 \"\"                                                  Much… Shake…\n## 19         1520 \"Leon.\"                                             Much… Shake…\n## 20         1520 \"He hath an uncle here in Messina will be very muc… Much… Shake…\n## 21         1520 \"\"                                                  Much… Shake…\n## 22         1520 \"Mess.\"                                             Much… Shake…\n## 23         1520 \"I have already delivered him letters, and there a… Much… Shake…\n## 24         1520 \"joy in him; even so much that joy could not show … Much… Shake…\n## 25         1520 \"enough without a badge of bitterness.\"             Much… Shake…\nshakes_words <- shakes %>%\n  # Filter out all empty rows\n  filter(text != \"\") %>%\n  # This is a play. The name of each character before they speak \n  filter(str_detect(text,\"[A-Z]{3,}\")==FALSE) %>%\n  # take the column text and divide it by words\n  unnest_tokens(word, text) ## filter: removed 3,088 rows (22%), 11,135 rows remaining## filter: removed 31 rows (<1%), 11,104 rows remaining\n# Bind both word dfs\nwords <- rbind.data.frame(shakes_words,wilde_words)\n\n# Count words etc.\nwords %>%\n  group_by(title,author) %>%\n  slice_sample(n=10000) %>%\n  mutate(word_length = str_length(word),\n         median_word_length = median(word_length)) %>%\n  group_by(word_length,title,author) %>%\n  mutate(total_word_length = n()) %>%\n  distinct(word_length,title,.keep_all=T) %>%\n  select(word_length,title,author,total_word_length,median_word_length) %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=author,group=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  # facet_wrap(~author, ncol = 2)+\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\", linetype = \"\",\n       caption = \"Note: Median word length is 4 for both authors.\")## group_by: 2 grouping variables (title, author)## slice_sample (grouped): removed 61,480 rows (43%), 80,000 rows remaining## mutate (grouped): new variable 'word_length' (integer) with 17 unique values and 0% NA##                   new variable 'median_word_length' (double) with one unique value and 0% NA## group_by: 3 grouping variables (word_length, title, author)## mutate (grouped): new variable 'total_word_length' (integer) with 101 unique values and 0% NA## distinct (grouped): removed 79,881 rows (>99%), 119 rows remaining## select: dropped 2 variables (gutenberg_id, word)"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"exercise-optional","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.5 Exercise (Optional)","text":"Extend current analysis authors works author.better ways compare distribution word length? changes across time? differences different types works (e.g., fiction vs. non-fiction, prose vs. poetry)?","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"final-words","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.6 Final Words","text":"often case, won’t able cover every single feature different packages offer, objects create look like, else can . advise go home explore code detail. Try applying code different corpus come next class questions (just show able ).","code":""},{"path":"week-2-tokenization-and-word-frequency.html","id":"week-2-tokenization-and-word-frequency","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2 Week 2: Tokenization and Word Frequency","text":"","code":""},{"path":"week-2-tokenization-and-word-frequency.html","id":"slides-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"Slides","text":"3 Tokenization Word Frequency (link Perusall)","code":""},{"path":"week-2-tokenization-and-word-frequency.html","id":"setup-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(wesanderson) # to prettify\nlibrary(readxl) # to read excel\nlibrary(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week-2-tokenization-and-word-frequency.html","id":"get-data-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.2 Get Data:","text":"example, using small corpus song lyrics.Ok, different artists, different genres year…lyrics following form:","code":"\nsample_lyrics <- read_excel(\"data/lyrics_sample.xlsx\")\nhead(sample_lyrics)## # A tibble: 6 × 5\n##   artist                   album                      year song           lyrics\n##   <chr>                    <chr>                     <dbl> <chr>          <chr> \n## 1 Rage Against the Machine Evil Empire                1996 Bulls on Para… \"Come…\n## 2 Rage Against the Machine Rage Against the Machine   1992 Killing in th… \"Kill…\n## 3 Rage Against the Machine Renegades                  2000 Renegades of … \"No m…\n## 4 Rage Against the Machine The Battle of Los Angeles  1999 Sleep Now in … \"Yeah…\n## 5 Rage Against the Machine The Battle of Los Angeles  1999 Guerrilla Rad… \"Tran…\n## 6 Rage Against the Machine The Battle of Los Angeles  1999 Testify        \"Uh!\\…## \n##      Megan Thee Stallion Rage Against the Machine         System of a Down \n##                        5                        6                        5 \n##             Taylor Swift \n##                        5## [1] \"Yeah\\r\\n\\r\\nThe world is my expense\\r\\nIt’s the cost of my desire\\r\\nJesus blessed me with its future\\r\\nAnd I protect it with fire\\r\\n\\r\\nSo raise your fists and march around\\r\\nJust don’t take what you need\\r\\nI’ll jail and bury those committed\\r\\nAnd smother the rest in greed\\r\\n\\r\\nCrawl with me into tomorrow\\r\\nOr I’ll drag you to your grave\\r\\nI’m deep inside your children\\r\\nThey’ll betray you in my name\\r\\n\\r\\nHey, hey, sleep now in the fire\\r\\nHey, hey, sleep now in the fire\\r\\n\\r\\nThe lie is my expense\\r\\nThe scope of my desire\\r\\nThe party blessed me with its future\\r\\nAnd I protect it with fire\\r\\n\\r\\nI am the Niña, the Pinta, the Santa María\\r\\nThe noose and the rapist, the fields overseer\\r\\nThe Agents of Orange, the Priests of Hiroshima\\r\\nThe cost of my desire, sleep now in the fire\\r\\n\\r\\nHey, hey, sleep now in the fire\\r\\nHey, hey, sleep now in the fire\\r\\n\\r\\nFor it’s the end of history\\r\\nIt’s caged and frozen still\\r\\nThere is no other pill to take\\r\\nSo swallow the one that made you ill\\r\\n\\r\\nThe Niña, the Pinta, the Santa María\\r\\nThe noose and the rapist, the fields overseer\\r\\nThe Agents of Orange, the Priests of Hiroshima\\r\\nThe cost of my desire to sleep now in the fire\\r\\n\\r\\nYeah\\r\\n\\r\\nSleep now in the fire\\r\\nSleep now in the fire\\r\\nSleep now in the fire\\r\\nSleep now in the fire\""},{"path":"week-2-tokenization-and-word-frequency.html","id":"cleaning-the-text","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.3 Cleaning the Text","text":"Much like music, text comes different forms qualities. Regex workshop, might remember special characters can signal, example, new line (\\n), carriage return (\\r). example, can get rid 3. working text, always check state documents loaded program choice.","code":"\nsample_lyrics <- sample_lyrics %>%\n  mutate(lyrics_clean = str_replace_all(lyrics,\"\\\\n\", \"\\\\.\"),\n         lyrics_clean = str_replace_all(lyrics_clean,\"\\\\r\", \"\\\\.\")) %>%\n  select(-lyrics)## mutate: new variable 'lyrics_clean' (character) with 21 unique values and 0% NA## select: dropped one variable (lyrics)\nsample_lyrics$lyrics_clean[4]## [1] \"Yeah....The world is my expense..It’s the cost of my desire..Jesus blessed me with its future..And I protect it with fire....So raise your fists and march around..Just don’t take what you need..I’ll jail and bury those committed..And smother the rest in greed....Crawl with me into tomorrow..Or I’ll drag you to your grave..I’m deep inside your children..They’ll betray you in my name....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....The lie is my expense..The scope of my desire..The party blessed me with its future..And I protect it with fire....I am the Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire, sleep now in the fire....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....For it’s the end of history..It’s caged and frozen still..There is no other pill to take..So swallow the one that made you ill....The Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire to sleep now in the fire....Yeah....Sleep now in the fire..Sleep now in the fire..Sleep now in the fire..Sleep now in the fire\""},{"path":"week-2-tokenization-and-word-frequency.html","id":"tokenization","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.4 Tokenization","text":"goal create document-feature matrix, later extract information word frequency. , start crating corpus object, quanteda package.Looks good. Now can tokenize corpus (reduce complexity). One benefit creating corpus object first maintain metadata every document tokenize. come handy future.got rid punctuation. Now let’s remove stop words, high low frequency words, stem remaining tokens. cheating, though. know high low frequency words checked dfm (see next code chunk).can compare stemmed output non-stemmed output. ‘future’ become ‘futur’? assuming , purposes, ‘future=futuristic’. researcher decide. finally, can create document-feature matrix (dfm).Note create dfm object, tokens become lowercase. Now can check 15 frequent tokens.tell us much, used previous code check low-information tokens might want remove analysis. can also see many tokens appear :interesting text analysis count words time/space. case, ‘space’ can artist.Interesting. lot overlap (apart token Megan Thee Stallion Rage Machine). However, great measure importance word relative much appears across documents (one way denominate). Enter TF-IDF: “Term-Frequency / Inverse-Document-frequency”. TF-IDF weighting -weights relatively rare words appear documents. Using term frequency inverse document frequency allows us find words characteristic one document within collection documents.building dictionary, example, might want include words high TF-IDF values. Another way think TF-IDF terms predictive power. Words common documents predictive power receive TD-IDF value 0. Words appear, relatively document, greater predictive power receive TD-IDF > 0. rare words also penalized, since might provide specific information one document (.e., high prediction one document information rest). read Chapter 6/7 Grimmer et al., idea find right balance.Another useful tool (concept) keyness. Keyness two--two association score features occur differentially across different categories. can estimate features associated one category (case, one artist), compared . Let’s compare Megan Thee Stallion Taylor Swift.Similar implied TF-IDF graphs. Notice stemming always works expected. Taylor Swift sings “shake, shake, shake” Megan Thee Stallion sings “shaking”. However, words appear distinct features artists.","code":"\ncorpus_lyrics <- corpus(sample_lyrics,\n                     text_field = \"lyrics_clean\",\n                     unique_docnames = TRUE)\n\nsummary(corpus_lyrics)## Corpus consisting of 21 documents, showing 21 documents:\n## \n##    Text Types Tokens Sentences                   artist\n##   text1   119    375        35 Rage Against the Machine\n##   text2    52    853        83 Rage Against the Machine\n##   text3   188    835        91 Rage Against the Machine\n##   text4    97    352        38 Rage Against the Machine\n##   text5   160    440        50 Rage Against the Machine\n##   text6   133    535        67 Rage Against the Machine\n##   text7   104    559        53         System of a Down\n##   text8    67    365        40         System of a Down\n##   text9    68    298        33         System of a Down\n##  text10    65    258        32         System of a Down\n##  text11   137    558        68         System of a Down\n##  text12   131    876        70             Taylor Swift\n##  text13   159    465        41             Taylor Swift\n##  text14   162    544        62             Taylor Swift\n##  text15   196    738        84             Taylor Swift\n##  text16   169    549        50             Taylor Swift\n##  text17   229    867        55      Megan Thee Stallion\n##  text18   193    664        61      Megan Thee Stallion\n##  text19   310   1190        87      Megan Thee Stallion\n##  text20   198    656        48      Megan Thee Stallion\n##  text21   256   1050        73      Megan Thee Stallion\n##                       album year                  song\n##                 Evil Empire 1996       Bulls on Parade\n##    Rage Against the Machine 1992   Killing in the Name\n##                   Renegades 2000     Renegades of Funk\n##   The Battle of Los Angeles 1999 Sleep Now in the Fire\n##   The Battle of Los Angeles 1999       Guerrilla Radio\n##   The Battle of Los Angeles 1999               Testify\n##                   Mezmerize 2005               B.Y.O.B\n##                    Toxicity 2001            Chop Suey!\n##                    Toxicity 2001               Aerials\n##                    Toxicity 2001               Toxicty\n##                    Toxicity 2001                 Sugar\n##                        1989 2014          Shake it Off\n##                   Midnights 2022             Anti-Hero\n##                    Fearless 2008    You Belong With Me\n##                        1989 2014           Blank Space\n##                    Fearless 2008            Love Story\n##                  Traumazine 2022                Plan B\n##                        Suga 2020                Savage\n##  Something for Thee Hotties 2021             Thot Shit\n##                  Traumazine 2022                   Her\n##                  Traumazine 2022            Ungrateful\nlyrics_toks <- tokens(corpus_lyrics,\n                   remove_numbers = TRUE, # Thinks about this\n                   remove_punct = TRUE, # Remove punctuation!\n                   remove_url = TRUE) # Might be helpful\nlyrics_toks[c(4,14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"The\"     \"world\"   \"is\"      \"my\"      \"expense\" \"It’s\"   \n##  [8] \"the\"     \"cost\"    \"of\"      \"my\"      \"desire\" \n## [ ... and 227 more ]\n## \n## text14 :\n##  [1] \"You're\"     \"on\"         \"the\"        \"phone\"      \"with\"      \n##  [6] \"your\"       \"girlfriend\" \"she's\"      \"upset\"      \"She's\"     \n## [11] \"going\"      \"off\"       \n## [ ... and 385 more ]\nlyrics_toks <- tokens_remove(lyrics_toks,\n                          # you can change or add stopwords depending on the \n                          # language(s) of the documents\n                          c(stopwords(language = \"en\"),\n                            # Now is high frequency... there are many low\n                            # frequency tokens. We will check these later\n                            \"now\"),\n                          padding = F)\n\nlyrics_toks_stem <- tokens_wordstem(lyrics_toks, language = \"en\")\n\nlyrics_toks[c(4,14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"world\"   \"expense\" \"It’s\"    \"cost\"    \"desire\"  \"Jesus\"  \n##  [8] \"blessed\" \"future\"  \"protect\" \"fire\"    \"raise\"  \n## [ ... and 105 more ]\n## \n## text14 :\n##  [1] \"phone\"      \"girlfriend\" \"upset\"      \"going\"      \"something\" \n##  [6] \"said\"       \"Cause\"      \"get\"        \"humor\"      \"like\"      \n## [11] \"room\"       \"typical\"   \n## [ ... and 133 more ]\nlyrics_toks_stem[c(4,14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"world\"   \"expens\"  \"It’s\"    \"cost\"    \"desir\"   \"Jesus\"  \n##  [8] \"bless\"   \"futur\"   \"protect\" \"fire\"    \"rais\"   \n## [ ... and 105 more ]\n## \n## text14 :\n##  [1] \"phone\"      \"girlfriend\" \"upset\"      \"go\"         \"someth\"    \n##  [6] \"said\"       \"Caus\"       \"get\"        \"humor\"      \"like\"      \n## [11] \"room\"       \"typic\"     \n## [ ... and 133 more ]\nlyrics_dfm <- dfm(lyrics_toks)\nlyrics_dfm_stem <- dfm(lyrics_toks_stem)\n\nhead(lyrics_dfm_stem)## Document-feature matrix of: 6 documents, 1,165 features (93.12% sparse) and 4 docvars.\n##        features\n## docs    come wit microphon explod shatter mold either drop hit like\n##   text1    4   4         1      1       1    1      1    3   1    1\n##   text2    2   0         0      0       0    0      0    0   0    0\n##   text3    0   0         0      0       0    0      0    0   0    4\n##   text4    0   0         0      0       0    0      0    0   0    0\n##   text5    0   0         0      0       0    0      0    0   0    1\n##   text6    0   4         0      0       0    0      0    0   0    0\n## [ reached max_nfeat ... 1,155 more features ]\nlyrics_dfm_stem %>%\n  textstat_frequency(n=30) %>%\n  ggplot(aes(x = reorder(feature,frequency),y=frequency,fill = (frequency), color = (frequency))) +\n  geom_col(alpha=0.5) +\n  coord_flip() +\n  scale_x_reordered() +\n  scale_color_distiller(palette = \"PuOr\") +\n  scale_fill_distiller(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x=\"\",y=\"Frequency\",color = \"\", fill = \"\") +\n  theme(legend.position=\"none\") \nonly_once <- lyrics_dfm_stem %>%\n  textstat_frequency() %>%\n  filter(frequency == 1)## filter: removed 566 rows (49%), 599 rows remaining\nlength(only_once$feature)## [1] 599\nlyrics_dfm_stem %>%\n  textstat_frequency(n=15, groups = c(artist)) %>%\n  ggplot(aes(x = reorder_within(feature,frequency,group),y=frequency,fill = group, color = group)) +\n  geom_col(alpha = 0.5) +\n  coord_flip() +\n  facet_wrap(~group, scales = \"free\") +\n  scale_x_reordered() +\n  scale_color_brewer(palette = \"PuOr\") +\n  scale_fill_brewer(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x=\"\",y=\"\",color = \"\", fill = \"\") +\n  theme(legend.position=\"none\") \nlyrics_dfm_tfidf <- dfm_tfidf(lyrics_dfm_stem)\n\nlyrics_dfm_tfidf %>%\n  textstat_frequency(n=15, groups = c(artist), force=T) %>%\n  ggplot(aes(x = reorder_within(feature,frequency,group),y=frequency,fill = group, color = group)) +\n  geom_col(alpha = 0.5) +\n  coord_flip() +\n  facet_wrap(~group, scales = \"free\") +\n  scale_x_reordered() +\n  scale_color_brewer(palette = \"PuOr\") +\n  scale_fill_brewer(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x=\"\",y=\"TF-IDF\",color = \"\", fill = \"\") +\n  theme(legend.position=\"none\") \nlyrics_dfm_ts_mts <- dfm_subset(lyrics_dfm_stem, year > 2006)\n\nlyrics_key <- textstat_keyness(lyrics_dfm_ts_mts, \n                              target = lyrics_dfm_ts_mts$artist == \"Taylor Swift\")\ntextplot_keyness(lyrics_key)"},{"path":"week-2-tokenization-and-word-frequency.html","id":"word-frequency-across-artist","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.5 Word Frequency Across Artist","text":"can something similar last week look word frequencies. Rather creating dfm, can use dataset , get information. example, average number tokens artist.Alternatively, can estimate frequency specific token song.can now join two data frames together left_join() function join “song” column. can pipe joined data plot.","code":"\nsample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  group_by(song) %>%\n  mutate(total_tk_song = n()) %>%\n  distinct(song,.keep_all=T) %>% \n  group_by(artist) %>%\n  mutate(mean_tokens = mean(total_tk_song)) %>%\n  ggplot(aes(x=song,y=total_tk_song,fill=artist,color=artist)) +\n  geom_col(alpha=.8) +\n  geom_hline(aes(yintercept = mean_tokens, color = artist), linetype = \"dashed\")+\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  scale_fill_manual(values = wes_palette(\"Royal2\")) +\n  facet_wrap(~artist, scales = \"free_x\", nrow = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 90, size = 5,vjust = 0.5, hjust=1)) +\n  labs(x=\"\", y = \"Total Tokens\", color = \"\", fill = \"\",\n       caption = \"Note: Mean token length in dashed line.\")## group_by: one grouping variable (song)## mutate (grouped): new variable 'total_tk_song' (integer) with 20 unique values and 0% NA## distinct (grouped): removed 8,958 rows (>99%), 21 rows remaining## group_by: one grouping variable (artist)## mutate (grouped): new variable 'mean_tokens' (double) with 4 unique values and 0% NA\nlyrics_totals <- sample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  group_by(song) %>%\n  mutate(total_tk_song = n()) %>%\n  distinct(song,.keep_all=T) ## group_by: one grouping variable (song)## mutate (grouped): new variable 'total_tk_song' (integer) with 20 unique values and 0% NA## distinct (grouped): removed 8,958 rows (>99%), 21 rows remaining\n# let's look for \"like\"\nlyrics_like <- sample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  filter(word==\"like\") %>%\n  group_by(song) %>%\n  mutate(total_like_song = n()) %>%\n  distinct(song,total_like_song) ## filter: removed 8,934 rows (99%), 45 rows remaining## group_by: one grouping variable (song)## mutate (grouped): new variable 'total_like_song' (integer) with 7 unique values and 0% NA## distinct (grouped): removed 33 rows (73%), 12 rows remaining\nlyrics_totals %>%\n  left_join(lyrics_like, by = \"song\") %>%\n  ungroup() %>%\n  mutate(like_prop = total_like_song/total_tk_song) %>%\n  ggplot(aes(x=song,y=like_prop,fill=artist,color=artist)) +\n  geom_col(alpha=.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  scale_fill_manual(values = wes_palette(\"Royal2\")) +\n  facet_wrap(~artist, scales = \"free_x\", nrow = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 90, size = 5,vjust = 0.5, hjust=1)) +\n  labs(x=\"\", y = \"Prop. of \\'Like\\'\", color = \"\", fill = \"\")## left_join: added one column (total_like_song)##            > rows only in x    9##            > rows only in y  ( 0)##            > matched rows     12##            >                 ====##            > rows total       21## ungroup: no grouping variables## mutate: new variable 'like_prop' (double) with 13 unique values and 43% NA"},{"path":"week-2-tokenization-and-word-frequency.html","id":"final-words-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.6 Final Words","text":"often case, won’t able cover every single feature different packages offer, objects create look like, else can . advise go home explore code detail. Try applying code different corpus come next class questions (just show able ).","code":""},{"path":"week-3-dictionary-based-approaches.html","id":"week-3-dictionary-based-approaches","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3 Week 3: Dictionary-Based Approaches","text":"","code":""},{"path":"week-3-dictionary-based-approaches.html","id":"slides-2","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"Slides","text":"4 Dictionary-Based Approaches (link Perusall)","code":""},{"path":"week-3-dictionary-based-approaches.html","id":"setup-2","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(textdata) # text datasets\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\nlibrary(quanteda.dictionaries)\nlibrary(wesanderson) # to prettify\nlibrary(knitr) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week-3-dictionary-based-approaches.html","id":"get-data-2","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.2 Get Data:","text":"example, using data Ventura et al. (2021) - Connective effervescence streaming chat political debates.","code":"\nload(\"data/ventura_etal_df.Rdata\")\nhead(ventura_etal_df)##   text_id\n## 1       1\n## 2       2\n## 3       3\n## 4       4\n## 5       5\n## 6       6\n##                                                                                                                                                                                                                                       comments\n## 1 MORE:\\n The coronavirus pandemic's impact on the race will be on display as the\\n two candidates won't partake in a handshake, customary at the top of \\nsuch events. The size of the audience will also be limited. https://abcn.ws/3kVyl16\n## 2                                                                                                                                                           God please bless all Trump supporters. They need it for they know not what they do\n## 3                                                                                                                               Trump  is  a  living  disaster!    What  an embarrassment  to  all  human  beings!    The  man  is  dangerous!\n## 4                                                                                                                                                   This debate is why other counties laugh at us. School yard class president debate at best.\n## 5                                                                    OMG\\n ... shut up tRump ... so rude and out of control.  Obviously freaking \\nout.  This is a debate NOT a convention or a speech or your platform.  \\nLearn some manners\n## 6                                                                                                      It’s\\n hard to see what this country has become.  The Presidency is no longer a\\n respected position it has lost all of it’s integrity.\n##                      id likes                  debate\n## 1              ABC News   100 abc_first_debate_manual\n## 2            Anita Hill    61 abc_first_debate_manual\n## 3          Dave Garland    99 abc_first_debate_manual\n## 4              Carl Roy    47 abc_first_debate_manual\n## 5 Lynda Martin-Chambers   154 abc_first_debate_manual\n## 6         Nica Merchant   171 abc_first_debate_manual"},{"path":"week-3-dictionary-based-approaches.html","id":"tokenization-etc.","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.3 Tokenization etc.","text":"comments mostly clean, can check () require cleaning. previous code, showed lower, remove stopwords, etc., using quanteda. can also using tidytext 4:","code":"\ntidy_ventura <- ventura_etal_df %>% \n  # to lower:\n  mutate(comments = tolower(comments)) %>%\n  # tokenize\n  unnest_tokens(word, comments) %>%\n  # keep only words (check regex)\n  filter(str_detect(word, \"[a-z]\")) %>%\n  # remove stop words\n  filter(!word %in% stop_words$word)## mutate: changed 29,261 values (99%) of 'comments' (0 new NA)## filter: removed 3,374 rows (1%), 494,341 rows remaining## filter: removed 296,793 rows (60%), 197,548 rows remaining\nhead(tidy_ventura, 20)##    text_id         id likes                  debate        word\n## 1        1   ABC News   100 abc_first_debate_manual coronavirus\n## 2        1   ABC News   100 abc_first_debate_manual  pandemic's\n## 3        1   ABC News   100 abc_first_debate_manual      impact\n## 4        1   ABC News   100 abc_first_debate_manual        race\n## 5        1   ABC News   100 abc_first_debate_manual     display\n## 6        1   ABC News   100 abc_first_debate_manual  candidates\n## 7        1   ABC News   100 abc_first_debate_manual     partake\n## 8        1   ABC News   100 abc_first_debate_manual   handshake\n## 9        1   ABC News   100 abc_first_debate_manual   customary\n## 10       1   ABC News   100 abc_first_debate_manual         top\n## 11       1   ABC News   100 abc_first_debate_manual      events\n## 12       1   ABC News   100 abc_first_debate_manual        size\n## 13       1   ABC News   100 abc_first_debate_manual    audience\n## 14       1   ABC News   100 abc_first_debate_manual     limited\n## 15       1   ABC News   100 abc_first_debate_manual       https\n## 16       1   ABC News   100 abc_first_debate_manual     abcn.ws\n## 17       1   ABC News   100 abc_first_debate_manual     3kvyl16\n## 18       2 Anita Hill    61 abc_first_debate_manual         god\n## 19       2 Anita Hill    61 abc_first_debate_manual       bless\n## 20       2 Anita Hill    61 abc_first_debate_manual       trump"},{"path":"week-3-dictionary-based-approaches.html","id":"keywords","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.4 Keywords","text":"can detect occurrence words trump biden comment (text_id).Rather replicating results Figure 3 Ventura et al. (2021), estimate median number likes comment mentioning Trump, Biden, , None get:can also see differences across news media:Similar Young Soroka (2012), can also explore keywords interest context. good way validate proposed measure (e.g., mentioning trump reflection interest? relevance?).can also look one word time:Alternatively, can see common words happen together. called collocations (similar concept n-grams). want see common names mentioned (first last name).(\\(\\lambda\\) score something like likelihood , example, chris wallace happening one next . complete explanation, can read paper.)can also discover collocations longer two words. example identify collocations consisting three words.","code":"\ntrump_biden <- tidy_ventura %>%\n  # create a dummy\n  mutate(trump_token = ifelse(word==\"trump\", 1, 0),\n         biden_token = ifelse(word==\"biden\", 1, 0)) %>%\n  # see which comments have the word trump / biden\n  group_by(text_id) %>%\n  mutate(trump_cmmnt = ifelse(sum(trump_token)>0, 1, 0),\n         biden_cmmnt = ifelse(sum(biden_token)>0, 1, 0)) %>%\n  # reduce to our unit of analysis (comment) \n  distinct(text_id, .keep_all = T) %>%\n  select(text_id,trump_cmmnt,biden_cmmnt,likes,debate)## mutate: new variable 'trump_token' (double) with 2 unique values and 0% NA##         new variable 'biden_token' (double) with 2 unique values and 0% NA## group_by: one grouping variable (text_id)## mutate (grouped): new variable 'trump_cmmnt' (double) with 2 unique values and 0% NA##                   new variable 'biden_cmmnt' (double) with 2 unique values and 0% NA## distinct (grouped): removed 168,013 rows (85%), 29,535 rows remaining## select: dropped 4 variables (id, word, trump_token, biden_token)\nhead(trump_biden, 20)## # A tibble: 20 × 5\n## # Groups:   text_id [20]\n##    text_id trump_cmmnt biden_cmmnt likes debate                 \n##      <int>       <dbl>       <dbl> <int> <chr>                  \n##  1       1           0           0   100 abc_first_debate_manual\n##  2       2           1           0    61 abc_first_debate_manual\n##  3       3           1           0    99 abc_first_debate_manual\n##  4       4           0           0    47 abc_first_debate_manual\n##  5       5           1           0   154 abc_first_debate_manual\n##  6       6           0           0   171 abc_first_debate_manual\n##  7       7           0           0    79 abc_first_debate_manual\n##  8       8           0           0    39 abc_first_debate_manual\n##  9       9           0           0    53 abc_first_debate_manual\n## 10      10           0           0    36 abc_first_debate_manual\n## 11      11           1           0    41 abc_first_debate_manual\n## 12      12           0           0    28 abc_first_debate_manual\n## 13      13           1           0    54 abc_first_debate_manual\n## 14      14           0           0    30 abc_first_debate_manual\n## 15      15           1           0    27 abc_first_debate_manual\n## 16      16           1           1    31 abc_first_debate_manual\n## 17      17           1           0    35 abc_first_debate_manual\n## 18      18           1           1    32 abc_first_debate_manual\n## 19      19           0           0    34 abc_first_debate_manual\n## 20      20           1           0    37 abc_first_debate_manual\ntrump_biden %>%\n  # Create categories\n  mutate(mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==0, \"1. None\", NA),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==0, \"2. Trump\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==1, \"3. Biden\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==1, \"4. Both\", mentions_cat)) %>%\n  group_by(mentions_cat) %>%\n  mutate(likes_mean = median(likes, na.rm = T)) %>%\n  ungroup() %>%\n  # Remove the ones people like too much\n  filter(likes < 26) %>%\n  # Plot\n  ggplot(aes(x=likes,fill = mentions_cat, color = mentions_cat)) +\n  geom_density(alpha = 0.3) +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~mentions_cat, ncol = 1) + \n  theme_minimal() +\n  geom_vline(aes(xintercept = likes_mean, color = mentions_cat), linetype = \"dashed\")+\n  theme(legend.position=\"none\") +\n  labs(x=\"\", y = \"Density\", color = \"\", fill = \"\",\n       caption = \"Note: Median likes in dashed lines.\")## mutate (grouped): new variable 'mentions_cat' (character) with 4 unique values and 0% NA## group_by: one grouping variable (mentions_cat)## mutate (grouped): new variable 'likes_mean' (double) with 4 unique values and 0% NA## ungroup: no grouping variables## filter: removed 8,136 rows (28%), 21,399 rows remaining\ntrump_biden %>%\n  # Create categories\n  mutate(mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==0, \"1. None\", NA),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==0, \"2. Trump\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==0 & biden_cmmnt==1, \"3. Biden\", mentions_cat),\n         mentions_cat = ifelse(trump_cmmnt==1 & biden_cmmnt==1, \"4. Both\", mentions_cat),\n         media = ifelse(str_detect(debate, \"abc\"), \"ABC\", NA),\n         media = ifelse(str_detect(debate, \"nbc\"), \"NBC\", media),\n         media = ifelse(str_detect(debate, \"fox\"), \"FOX\", media)) %>%\n  group_by(mentions_cat,media) %>%\n  mutate(median_like = median(likes,na.rm = T)) %>%\n  ungroup() %>%\n  # Remove the ones people like too much\n  filter(likes < 26) %>%\n  # Plot\n  ggplot(aes(x=likes,fill = mentions_cat, color = mentions_cat)) +\n  geom_density(alpha = 0.3) +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~media, ncol = 1) + \n  geom_vline(aes(xintercept = median_like, color = mentions_cat), linetype = \"dashed\")+\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(x=\"\", y = \"Density\", color = \"\", fill = \"\",\n       caption = \"Note: Median likes in dashed lines.\")## mutate (grouped): new variable 'mentions_cat' (character) with 4 unique values and 0% NA##                   new variable 'media' (character) with 3 unique values and 0% NA## group_by: 2 grouping variables (mentions_cat, media)## mutate (grouped): new variable 'median_like' (double) with 6 unique values and 0% NA## ungroup: no grouping variables## filter: removed 8,136 rows (28%), 21,399 rows remaining\ncorpus_ventura <- corpus(ventura_etal_df,\n                     text_field = \"comments\",\n                     unique_docnames = TRUE)\n\ntoks_ventura <- tokens(corpus_ventura)\nkw_trump <- kwic(toks_ventura, pattern = \"Trump\")\n\n## The number determines the size of the window: how many tokens before and after\nhead(kw_trump, 20)## Keyword-in-context with 20 matches.                                                 \n##    [text2, 5]      God please bless all | Trump |\n##    [text3, 1]                           | Trump |\n##    [text5, 7]               ... shut up | tRump |\n##  [text11, 11] a bad opiate problem then | trump |\n##   [text13, 4]                 This is a | TRUMP |\n##   [text15, 1]                           | Trump |\n##   [text16, 8]  this SO much better than | Trump |\n##   [text17, 3]                    I love | Trump |\n##   [text18, 4]            Biden is right | Trump |\n##   [text20, 1]                           | Trump |\n##  [text22, 12]     being a decent human. | Trump |\n##   [text23, 1]                           | Trump |\n##  [text27, 11]          for once, i wish | trump |\n##  [text28, 10]             it America... | Trump |\n##   [text30, 1]                           | Trump |\n##   [text31, 1]                           | Trump |\n##   [text32, 1]                           | Trump |\n##  [text32, 15]    People open your eyes. | Trump |\n##   [text34, 1]                           | Trump |\n##   [text36, 1]                           | Trump |\n##                                 \n##  supporters. They need it       \n##  is a living disaster!          \n##  ... so rude                    \n##  brings up about bidens son     \n##  all about ME debate and        \n##  is looking pretty flushed right\n##  and I wasn’t even going        \n##  ! He is the best               \n##  doesn’t have a plan for        \n##  worse president EVER 😡 thank  \n##  doesn't know the meaning of    \n##  such a hateful person he       \n##  would shut his trap for        \n##  IS NOT smarter than a          \n##  has improved our economy and   \n##  has done so much harm          \n##  is a clown and after           \n##  is evil.                       \n##  is so broke that is            \n##  is literally making this debate\nkw_best <- kwic(toks_ventura, pattern = c(\"best\",\"worst\"))\nhead(kw_best, 20)## Keyword-in-context with 20 matches.                                                       \n##    [text4, 17] yard class president debate at | best  |\n##    [text10, 1]                                | Worst |\n##    [text17, 8]               Trump! He is the | best  |\n##    [text43, 6]           This is gonna be the | best  |\n##   [text81, 31]  an incompetent President, the | worst |\n##   [text81, 33]          President, the worst, | worst |\n##   [text81, 35]              the worst, worst, | worst |\n##   [text82, 11]         was totally one sided! | Worst |\n##    [text86, 8]           right - Trump is the | worst |\n##   [text100, 9]             !! BRAVO BRAVO THE | BEST  |\n##   [text102, 4]                  Obama was the | worst |\n##  [text119, 10]            he said he would do | Best  |\n##  [text138, 13]               think. He is the | worst |\n##  [text141, 22]           puppet could be? The | worst |\n##   [text143, 6]           Trump may not be the | best  |\n##  [text158, 15]              This man is a the | worst |\n##   [text167, 3]                         He the | worst |\n##  [text221, 34]           by far have been the | worst |\n##  [text221, 36]           have been the worst, | WORST |\n##  [text221, 38]              the worst, WORST, | WORST |\n##                                     \n##  .                                  \n##  debate I’ve ever seen!             \n##  president ever! Thank you          \n##  show on TV in 4                    \n##  , worst, worst in                  \n##  , worst in history.                \n##  in history.                        \n##  ever! Our president kept           \n##  president America ever had!        \n##  PRESIDENT OF THE WORLD.            \n##  president ever!!!                  \n##  President ever Crybabies don't like\n##  president ever                     \n##  president in our time ever         \n##  choice but I will choose           \n##  thing that has ever happened       \n##  president we had in the            \n##  , WORST, WORST PRESIDENT           \n##  , WORST PRESIDENT!!                \n##  PRESIDENT!!!\ntoks_ventura <- tokens(corpus_ventura, remove_punct = TRUE)\ncol_ventura <- tokens_select(toks_ventura, \n                                # Keep only tokens that start with a capital letter\n                                pattern = \"^[A-Z]\", \n                                valuetype = \"regex\", \n                                case_insensitive = FALSE, \n                                padding = TRUE) %>% \n                  textstat_collocations(min_count = 20) # Minimum number of collocations to be taken into account.\nhead(col_ventura, 20)##          collocation count count_nested length    lambda         z\n## 1      chris wallace  1693            0      2  6.753757 128.18781\n## 2    president trump   831            0      2  3.752001  84.18127\n## 3          joe biden   430            0      2  3.387851  59.35890\n## 4           fox news   267            0      2  8.946604  53.79136\n## 5       mr president   152            0      2  4.991810  45.90814\n## 6      united states   144            0      2 12.106625  36.13436\n## 7       donald trump   141            0      2  4.737341  35.49434\n## 8         mike pence    40            0      2  8.952702  34.74382\n## 9       jo jorgensen    78            0      2 10.969527  34.45630\n## 10             HE IS    43            0      2  6.211846  34.14875\n## 11    vice president   343            0      2  8.415032  33.26922\n## 12  democratic party    38            0      2  9.093730  31.88339\n## 13     CHRIS WALLACE    38            0      2  9.634206  31.78885\n## 14   PRESIDENT TRUMP    37            0      2  5.852576  30.58102\n## 15          TRUMP IS    42            0      2  5.197507  30.15495\n## 16       white house    46            0      2 11.318748  29.41979\n## 17 african americans    35            0      2  7.749976  29.38678\n## 18         JOE BIDEN    25            0      2  7.541467  28.86445\n## 19           YOU ARE    27            0      2  6.656140  28.82971\n## 20            IS NOT    34            0      2  5.512521  28.74916\ncol_ventura <- tokens_select(toks_ventura, \n                                case_insensitive = FALSE, \n                                padding = TRUE) %>% \n              textstat_collocations(min_count = 100, size = 3)\nhead(col_ventura, 20)##            collocation count count_nested length      lambda            z\n## 1          know how to   115            0      3 3.098900337 11.327580933\n## 2  the american people   220            0      3 2.601543689 10.158047857\n## 3          this is the   158            0      3 1.393091382  9.012926334\n## 4           to do with   108            0      3 4.011182538  7.217176889\n## 5       this debate is   167            0      3 0.997383245  6.159091461\n## 6             is not a   139            0      3 0.796582289  6.084664757\n## 7     wallace needs to   172            0      3 1.634217570  4.628866454\n## 8         is the worst   110            0      3 1.840591657  3.639602823\n## 9         trump is the   153            0      3 0.283527984  2.554551024\n## 10           is such a   107            0      3 0.776224121  2.541279959\n## 11           is a joke   247            0      3 2.091736055  2.524522767\n## 12      trump has done   105            0      3 0.646275113  2.285231341\n## 13          trump is a   322            0      3 0.202976986  2.002649763\n## 14         this is not   119            0      3 0.446372828  1.986517242\n## 15      trump needs to   131            0      3 0.580848241  1.941689788\n## 16         what a joke   141            0      3 2.379466544  1.672336835\n## 17   the united states   132            0      3 0.738367705  1.431647428\n## 18         going to be   122            0      3 1.914497779  1.348587450\n## 19         is going to   210            0      3 0.101463083  0.603531369\n## 20          biden is a   164            0      3 0.001198663  0.009724797"},{"path":"week-3-dictionary-based-approaches.html","id":"dictionary-approaches","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5 Dictionary Approaches","text":"can extend previous analysis using dictionaries. can create , can use previously validates dictionaries, can use previously validates dictionaries already included tidytext quanteda (sentiment analysis).","code":""},{"path":"week-3-dictionary-based-approaches.html","id":"sentiment-analysis","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5.1 Sentiment Analysis","text":"Let’s look pre-loaded sentiment dictionaries tidytext:AFFIN: measures sentiment numeric score -5 5, validated paper.bing: sentiment words found online forums. information .nrc: underpaid workers Amazon mechanical Turk coded emotional valence long list terms, validated paper.dictionary classifies quantifies words different way. Let’s use nrc sentiment dictionary analyze comments dataset. nrc classifies words whether positive negative sentiment.dictionary classifies quantifies words different way. Let’s use nrc sentiment dictionary analyze comments dataset. nrc classifies words whether reflecting:focus solely positive negative sentiment:Let’s check top positive words top negative words:make sense: ‘love’ positive, ‘bully’ negative. , much: ‘talk’ positive? ‘joke’ negative? context: ‘vice’ negative, ‘vice’-president (especially since presidente considered ‘positive’, … really?). ‘vote’ positive negative … ? Let’s turn blind eye now (, , go back Grimmer et al. Ch. 15 best practices).people watching different news media using different language? Let’s see data tells us. always, check unit analysis dataset. case, observation word, grouping variable comment (text_id), can count many positive negative words per comment. calculate net sentiment score subtracting number negative words positive word (comment).Ok, now can plot differences:","code":"\nget_sentiments(\"afinn\")## # A tibble: 2,477 × 2\n##    word       value\n##    <chr>      <dbl>\n##  1 abandon       -2\n##  2 abandoned     -2\n##  3 abandons      -2\n##  4 abducted      -2\n##  5 abduction     -2\n##  6 abductions    -2\n##  7 abhor         -3\n##  8 abhorred      -3\n##  9 abhorrent     -3\n## 10 abhors        -3\n## # ℹ 2,467 more rows\nget_sentiments(\"bing\")## # A tibble: 6,786 × 2\n##    word        sentiment\n##    <chr>       <chr>    \n##  1 2-faces     negative \n##  2 abnormal    negative \n##  3 abolish     negative \n##  4 abominable  negative \n##  5 abominably  negative \n##  6 abominate   negative \n##  7 abomination negative \n##  8 abort       negative \n##  9 aborted     negative \n## 10 aborts      negative \n## # ℹ 6,776 more rows\nget_sentiments(\"nrc\")## # A tibble: 13,872 × 2\n##    word        sentiment\n##    <chr>       <chr>    \n##  1 abacus      trust    \n##  2 abandon     fear     \n##  3 abandon     negative \n##  4 abandon     sadness  \n##  5 abandoned   anger    \n##  6 abandoned   fear     \n##  7 abandoned   negative \n##  8 abandoned   sadness  \n##  9 abandonment anger    \n## 10 abandonment fear     \n## # ℹ 13,862 more rows\nnrc <- get_sentiments(\"nrc\") \ntable(nrc$sentiment)## \n##        anger anticipation      disgust         fear          joy     negative \n##         1245          837         1056         1474          687         3316 \n##     positive      sadness     surprise        trust \n##         2308         1187          532         1230\nnrc_pos_neg <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"positive\" | sentiment == \"negative\")## filter: removed 8,248 rows (59%), 5,624 rows remaining\nventura_pos_neg <- tidy_ventura %>%\n  left_join(nrc_pos_neg)## Joining with `by = join_by(word)`## left_join: added one column (sentiment)##            > rows only in x   147,204##            > rows only in y  (  3,402)##            > matched rows      52,059    (includes duplicates)##            >                 =========##            > rows total       199,263\nventura_pos_neg %>%\n  group_by(sentiment) %>%\n  count(word, sort = TRUE)## group_by: one grouping variable (sentiment)## count: now 14,242 rows and 3 columns, one group variable remaining (sentiment)## # A tibble: 14,242 × 3\n## # Groups:   sentiment [3]\n##    sentiment word          n\n##    <chr>     <chr>     <int>\n##  1 <NA>      trump     11676\n##  2 <NA>      biden      7847\n##  3 positive  president  4920\n##  4 <NA>      wallace    4188\n##  5 positive  debate     2693\n##  6 <NA>      people     2591\n##  7 <NA>      chris      2559\n##  8 <NA>      joe        2380\n##  9 <NA>      country    1589\n## 10 <NA>      time       1226\n## # ℹ 14,232 more rows\ncomment_pos_neg <- ventura_pos_neg %>%\n  # Create dummies of pos and neg for counting\n  mutate(pos_dum = ifelse(sentiment == \"positive\", 1, 0),\n         neg_dum = ifelse(sentiment == \"negative\", 1, 0)) %>%\n  # Estimate total number of tokens per comment, pos , and negs\n  group_by(text_id) %>%\n  mutate(total_words = n(),\n         total_pos = sum(pos_dum, na.rm = T),\n         total_neg = sum(neg_dum, na.rm = T)) %>%\n  # These values are aggregated at the text_id level so we can eliminate repeated text_id\n  distinct(text_id,.keep_all=TRUE) %>%\n  # Now we estimate the net sentiment score. You can change this and get a different way to measure the ratio of positive to negative\n  mutate(net_sent = total_pos - total_neg) %>%\n  ungroup() ## mutate: new variable 'pos_dum' (double) with 3 unique values and 74% NA##         new variable 'neg_dum' (double) with 3 unique values and 74% NA## group_by: one grouping variable (text_id)## mutate (grouped): new variable 'total_words' (integer) with 25 unique values and 0% NA##                   new variable 'total_pos' (double) with 14 unique values and 0% NA##                   new variable 'total_neg' (double) with 10 unique values and 0% NA## distinct (grouped): removed 169,728 rows (85%), 29,535 rows remaining## mutate (grouped): new variable 'net_sent' (double) with 21 unique values and 0% NA## ungroup: no grouping variables\n# Note that the `word` and `sentiment` columns are meaningless now\nhead(comment_pos_neg, 10)## # A tibble: 10 × 12\n##    text_id id           likes debate word  sentiment pos_dum neg_dum total_words\n##      <int> <chr>        <int> <chr>  <chr> <chr>       <dbl>   <dbl>       <int>\n##  1       1 ABC News       100 abc_f… coro… <NA>           NA      NA          17\n##  2       2 Anita Hill      61 abc_f… god   positive        1       0           4\n##  3       3 Dave Garland    99 abc_f… trump <NA>           NA      NA           6\n##  4       4 Carl Roy        47 abc_f… deba… positive        1       0           8\n##  5       5 Lynda Marti…   154 abc_f… omg   <NA>           NA      NA          12\n##  6       6 Nica Mercha…   171 abc_f… it’s  <NA>           NA      NA           9\n##  7       7 Connie Sage     79 abc_f… happ… <NA>           NA      NA           7\n##  8       8 Tammy Eisen     39 abc_f… expe… <NA>           NA      NA           4\n##  9       9 Susan Weyant    53 abc_f… smart <NA>           NA      NA          13\n## 10      10 Dana Spencer    36 abc_f… worst <NA>           NA      NA          15\n## # ℹ 3 more variables: total_pos <dbl>, total_neg <dbl>, net_sent <dbl>\ncomment_pos_neg %>%\n    # Create categories\n  mutate(media = ifelse(str_detect(debate, \"abc\"), \"ABC\", NA),\n         media = ifelse(str_detect(debate, \"nbc\"), \"NBC\", media),\n         media = ifelse(str_detect(debate, \"fox\"), \"FOX\", media)) %>%\n  group_by(media) %>%\n  mutate(median_sent = mean(net_sent)) %>%\n  ggplot(aes(x=net_sent,color=media,fill=media)) +\n  geom_histogram(alpha = 0.4,\n                 binwidth = 1) +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~media, ncol = 1) + \n  geom_vline(aes(xintercept = median_sent, color = media), linetype = \"dashed\")+\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  coord_cartesian(xlim = c(-5,5)) +\n  labs(x=\"\", y = \"Count\", color = \"\", fill = \"\",\n       caption = \"Note: Mean net sentiment in dashed lines.\")## mutate: new variable 'media' (character) with 3 unique values and 0% NA## group_by: one grouping variable (media)## mutate (grouped): new variable 'median_sent' (double) with 3 unique values and 0% NA"},{"path":"week-3-dictionary-based-approaches.html","id":"domain-specific-dictionaries","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5.2 Domain-Specific Dictionaries","text":"Sentiment dictionaries common. can make dictionary whatever concept interested . , long can create lexicon (validate ), can conduct analysis similar one just carried . time, rather using --shelf (sentiment) dictionary, create . Let’s try dictionary two topics: economy migration.long dictionary shape nrc_pos_neg object, can follow process followed sentiment dictionaries.Let’s see find words comments:many. Note stem lemmatized corpus, order get ‘job’ ‘jobs’ must dictionary. means pre-processing step carry corpus, must also carry dictionary.bit versed R language, notice dictionaries actually lists. quanteda understand dictionaries lists can actually build use function likcalike() find words text. added benefit can use glob find variations word (e.g., job* match ‘job’ ‘jobs’ ‘jobless’).output provides interesting information. First, economy immigration gives us percentage words text economy immigration. general, expect many words sentence mention, example, ‘jobs’ argue sentences talks economy. , number 0% can counted mentioning economy (unless theoretical grounds 3% words mentioning economy > 2% words mentioning economy). rest variables:WPS: Words per sentence.WC: Word count.Sixltr: Six-letter words (%).Dic: % words dictionary.Allpunct: % punctuation marks.Period OtherP: % specific punctuation marks.information obtained, can find users focused topic:\nTable 3.1: % mentions topic media outlet.\n","code":"\n# First, we define the economy and migration as a concept, and then find words that signal that concept:\neconomy <- cbind.data.frame(c(\"economy\",\"taxes\",\"inflation\",\"debt\",\"employment\",\"jobs\"),\"economy\")\ncolnames(economy) <- c(\"word\",\"topic\")\nmigration <- cbind.data.frame(c(\"immigrants\",\"border\",\"wall\",\"alien\",\"migrant\",\"visa\",\"daca\",\"dreamer\"),\"migration\") \ncolnames(migration) <- c(\"word\",\"topic\")\n\ndict <- rbind.data.frame(economy,migration)\ndict##          word     topic\n## 1     economy   economy\n## 2       taxes   economy\n## 3   inflation   economy\n## 4        debt   economy\n## 5  employment   economy\n## 6        jobs   economy\n## 7  immigrants migration\n## 8      border migration\n## 9        wall migration\n## 10      alien migration\n## 11    migrant migration\n## 12       visa migration\n## 13       daca migration\n## 14    dreamer migration\nventura_topic <- tidy_ventura %>%\n  left_join(dict)## Joining with `by = join_by(word)`\n## left_join: added one column (topic)\n## > rows only in x 196,175\n## > rows only in y ( 3)\n## > matched rows 1,373\n## > =========\n## > rows total 197,548\nventura_topic %>%\n  filter(!is.na(topic)) %>%\n  group_by(topic) %>%\n  count(word, sort = TRUE)## filter: removed 196,175 rows (99%), 1,373 rows remaining\n## group_by: one grouping variable (topic)\n## count: now 11 rows and 3 columns, one group variable remaining (topic)## # A tibble: 11 × 3\n## # Groups:   topic [2]\n##    topic     word           n\n##    <chr>     <chr>      <int>\n##  1 economy   taxes        680\n##  2 economy   economy      328\n##  3 economy   jobs         273\n##  4 migration wall          34\n##  5 economy   debt          32\n##  6 migration immigrants    12\n##  7 migration border         7\n##  8 economy   employment     3\n##  9 migration alien          2\n## 10 migration daca           1\n## 11 migration visa           1\ndict <- dictionary(list(economy = c(\"econom*\",\"tax*\",\"inflation\",\"debt*\",\"employ*\",\"job*\"),\n                        immigration = c(\"immigrant*\",\"border\",\"wall\",\"alien\",\"migrant*\",\"visa*\",\"daca\",\"dreamer*\"))) \n\n# liwcalike lowercases input text\nventura_topics <- liwcalike(ventura_etal_df$comments,\n                               dictionary = dict)\n\n# liwcalike keeps the order so we can cbind them directly\ntopics <- cbind.data.frame(ventura_etal_df,ventura_topics) \n\n# Look only at the comments that mention the economy and immigration\nhead(topics[topics$economy>0 & topics$immigration>0,])##       text_id\n## 4998     4999\n## 6475     6477\n## 8098     8113\n## 12331   32211\n## 14345   34225\n## 19889   62164\n##                                                                                                                                              comments\n## 4998                           Trump is going to create jobs to finish that wall,  hows that working for ya?  I don’t see Mexico paying for it either\n## 6475                           Trump is trash illegal immigrants pay more taxes than this man and you guys support this broke failure con billionaire\n## 8098                                  $750.00 in taxes in two years?????   BUT HE'S ALL OVER THE PLACE INSULTING IMMIGRANTS WHO PAID MORE IN TAXES!!!\n## 12331    Ask\\n Biden how much he will raise taxes to pay for all the things he says he\\n is going to provide everyone - including illegal immigrants!\n## 14345 Trump has been living the life and does not care for the hard working American...His taxes are not the only rip off...Investigate Wall Money...\n## 19889                                                               Vote trump out. He needs to pay taxes too ... immigrants pay more than that thief\n##                        id likes                  debate   docname Segment\n## 4998         Ellen Lustic    NA abc_first_debate_manual  text4998    4998\n## 6475      Kevin G Vazquez     1 abc_first_debate_manual  text6475    6475\n## 8098      Prince M Dorbor     1 abc_first_debate_manual  text8098    8098\n## 12331 Lynne Basista Shine     6 fox_first_debate_manual text12331   12331\n## 14345          RJ Jimenez     4 fox_first_debate_manual text14345   14345\n## 19889      Nicole Brennan    13 nbc_first_debate_manual text19889   19889\n##            WPS WC Sixltr   Dic economy immigration AllPunc Period Comma Colon\n## 4998  12.50000 25   4.00  8.00    4.00        4.00   12.00   0.00     4     0\n## 6475  20.00000 20  25.00 10.00    5.00        5.00    0.00   0.00     0     0\n## 8098  14.00000 28   7.14 10.71    7.14        3.57   35.71   3.57     0     0\n## 12331 27.00000 27  18.52  7.41    3.70        3.70    7.41   0.00     0     0\n## 14345 11.66667 35   8.57  5.71    2.86        2.86   25.71  25.71     0     0\n## 19889  9.50000 19   5.26 10.53    5.26        5.26   21.05  21.05     0     0\n##       SemiC QMark Exclam Dash Quote Apostro Parenth OtherP\n## 4998      0  4.00   0.00  0.0  4.00    4.00       0   8.00\n## 6475      0  0.00   0.00  0.0  0.00    0.00       0   0.00\n## 8098      0 17.86  10.71  0.0  3.57    3.57       0  35.71\n## 12331     0  0.00   3.70  3.7  0.00    0.00       0   3.70\n## 14345     0  0.00   0.00  0.0  0.00    0.00       0  25.71\n## 19889     0  0.00   0.00  0.0  0.00    0.00       0  21.05## mutate: new variable 'media' (character) with 3 unique values and 0% NA##         new variable 'economy_dum' (double) with 2 unique values and 0% NA##         new variable 'immigration_dum' (double) with 2 unique values and 0% NA## group_by: one grouping variable (media)## mutate (grouped): new variable 'pct_econ' (double) with 3 unique values and 0% NA##                   new variable 'pct_migr' (double) with 3 unique values and 0% NA## distinct (grouped): removed 29,544 rows (>99%), 3 rows remaining"},{"path":"week-3-dictionary-based-approaches.html","id":"using-pre-built-dictionaries","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.5.3 Using Pre-Built Dictionaries","text":"far seen apply pre-loaded dictionaries (e.g., sentiment) dictionaries. pre-built dictionary want apply corpus? long pre-built dictionary correct shape, can use techniques applied far. also means need data-wrangling pre-built dictionaries come different shapes.Let’s use NRC Affect Intensity Lexicon (created people made pre-loaded nrc sentiment dictionary). NRC Affect Intensity Lexicon measure intensity emotion scale 0 (low) 1 (high). example, ‘defiance’ anger intensity 0.51 ‘hate’ anger intensity 0.83.dictionary, best use include intensity word obtain variation analysis text (e.g., rather showing anger-anger, can analyze degree anger). use tidytext approach analyze degrees ‘joy’ corpus.Now, can see relationship likes joy:Finally, sake showing process, write code load dictionary using quanteda, note approach loses intensity information.","code":"\nintense_lex <- read.table(file = \"data/NRC-AffectIntensity-Lexicon.txt\", fill = TRUE,\n                          header = TRUE)\nhead(intense_lex)##         term score AffectDimension\n## 1   outraged 0.964           anger\n## 2  brutality 0.959           anger\n## 3     hatred 0.953           anger\n## 4    hateful 0.940           anger\n## 5  terrorize 0.939           anger\n## 6 infuriated 0.938           anger\njoy_lex <- intense_lex %>%\n  filter(AffectDimension==\"joy\") %>%\n  mutate(word=term) %>%\n  select(word,AffectDimension,score)## filter: removed 4,546 rows (78%), 1,268 rows remaining## mutate: new variable 'word' (character) with 1,268 unique values and 0% NA## select: dropped one variable (term)\nventura_joy <- tidy_ventura %>%\n  left_join(joy_lex) %>%\n  ## Most of the comments have no joy words so we will change these NAs to 0 but this is an ad-hoc decision. This decision must be theoretically motivated and justified\n  mutate(score = ifelse(is.na(score),0,score))## Joining with `by = join_by(word)`## left_join: added 2 columns (AffectDimension, score)##            > rows only in x   184,943##            > rows only in y  (    769)##            > matched rows      12,605##            >                 =========##            > rows total       197,548## mutate: changed 184,943 values (94%) of 'score' (184943 fewer NA)\nhead(ventura_joy[ventura_joy$score>0,])##    text_id           id likes                  debate       word\n## 18       2   Anita Hill    61 abc_first_debate_manual        god\n## 19       2   Anita Hill    61 abc_first_debate_manual      bless\n## 23       3 Dave Garland    99 abc_first_debate_manual     living\n## 30       4     Carl Roy    47 abc_first_debate_manual      laugh\n## 64       8  Tammy Eisen    39 abc_first_debate_manual experience\n## 65       8  Tammy Eisen    39 abc_first_debate_manual      share\n##    AffectDimension score\n## 18             joy 0.545\n## 19             joy 0.561\n## 23             joy 0.312\n## 30             joy 0.891\n## 64             joy 0.375\n## 65             joy 0.438\nlibrary(MASS) # To add the negative binomial fitted line## \n## Attaching package: 'MASS'## The following object is masked from 'package:tidylog':\n## \n##     select## The following object is masked from 'package:dplyr':\n## \n##     select\nventura_joy %>%\n  mutate(media = ifelse(str_detect(debate, \"abc\"), \"ABC\", NA),\n         media = ifelse(str_detect(debate, \"nbc\"), \"NBC\", media),\n         media = ifelse(str_detect(debate, \"fox\"), \"FOX\", media)) %>%\n  # Calculate mean joy in each comment\n  group_by(text_id) %>%\n  mutate(mean_joy = mean(score)) %>%\n  distinct(text_id,mean_joy,likes,media) %>%\n  ungroup() %>%\n  # Let's only look at comments that had SOME joy in them\n  filter(mean_joy > 0) %>%\n  # Remove the ones people like too much\n  filter(likes < 26) %>%\n  # Plot\n  ggplot(aes(x=mean_joy,y=likes,color=media,fill=media)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"glm.nb\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  facet_wrap(~media, ncol = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\") +\n  labs(x=\"Mean Joy\", y = \"Likes\", color = \"\", fill = \"\")## mutate: new variable 'media' (character) with 3 unique values and 0% NA## group_by: one grouping variable (text_id)## mutate (grouped): new variable 'mean_joy' (double) with 3,118 unique values and 0% NA## distinct (grouped): removed 168,013 rows (85%), 29,535 rows remaining## ungroup: no grouping variables## filter: removed 20,355 rows (69%), 9,180 rows remaining## filter: removed 2,518 rows (27%), 6,662 rows remaining## `geom_smooth()` using formula = 'y ~ x'\naffect_dict <- dictionary(list(anger = intense_lex$term[intense_lex$AffectDimension==\"anger\"],\n                        fear = intense_lex$term[intense_lex$AffectDimension==\"fear\"],\n                        joy = intense_lex$term[intense_lex$AffectDimension==\"joy\"],\n                        sadness = intense_lex$term[intense_lex$AffectDimension==\"sadness\"])) \n\nventura_affect <- liwcalike(ventura_etal_df$comments,\n                               dictionary = affect_dict)\n\n# liwcalike keeps the order so we can cbind them directly\naffect <- cbind.data.frame(ventura_etal_df,ventura_affect) \n\n# Look only at the comments that have anger and fear\nhead(affect[affect$anger>0 & affect$fear>0,])##    text_id\n## 3        3\n## 7        7\n## 9        9\n## 11      11\n## 12      12\n## 23      23\n##                                                                                                                                                                                       comments\n## 3                                                                               Trump  is  a  living  disaster!    What  an embarrassment  to  all  human  beings!    The  man  is  dangerous!\n## 7                                                                                  What happened to the days when it was a debate not a bully session! I am so ashamed of this administration!\n## 9  ......\\n a smart president?  A thief, a con man, and a liar that has taken tax \\npayers money to his own properties.  A liar that knew the magnitude of \\nthe virus and did not address it.\n## 11                             with\\n the usa having such a bad opiate problem then trump brings up about \\nbidens son is the most disgraceful thing any human being could do...vote\\n him out\n## 12   Trump’s\\n only recourse in the debate is to demean his opponent and talk about \\nwhat a great man he, himself is. Turn his mic off when it’s not his turn\\n to speak. Nothing but babble!\n## 23                                                                                           Trump such a hateful person he has no moral or respect in a debate he blames everyone except him.\n##              id likes                  debate docname Segment       WPS WC\n## 3  Dave Garland    99 abc_first_debate_manual   text3       3  6.333333 19\n## 7   Connie Sage    79 abc_first_debate_manual   text7       7 11.500000 23\n## 9  Susan Weyant    53 abc_first_debate_manual   text9       9 15.333333 46\n## 11  Lynn Kohler    41 abc_first_debate_manual  text11      11 32.000000 32\n## 12     Jim Lape    28 abc_first_debate_manual  text12      12 13.000000 39\n## 23   Joe Sonera    65 abc_first_debate_manual  text23      23 20.000000 20\n##    Sixltr   Dic anger  fear  joy sadness AllPunc Period Comma Colon SemiC QMark\n## 3   15.79 36.84  5.26 15.79 5.26   10.53   15.79   0.00  0.00     0     0  0.00\n## 7   17.39 17.39  4.35  4.35 0.00    8.70    8.70   0.00  0.00     0     0  0.00\n## 9    8.70 13.04  4.35  2.17 2.17    4.35   23.91  17.39  4.35     0     0  2.17\n## 11   6.25 28.12  9.38  6.25 3.12    9.38    9.38   9.38  0.00     0     0  0.00\n## 12  12.82  5.13  2.56  2.56 0.00    0.00   15.38   5.13  2.56     0     0  0.00\n## 23  15.00 25.00 10.00  5.00 5.00    5.00    5.00   5.00  0.00     0     0  0.00\n##    Exclam Dash Quote Apostro Parenth OtherP\n## 3   15.79    0  0.00    0.00       0  15.79\n## 7    8.70    0  0.00    0.00       0   8.70\n## 9    0.00    0  0.00    0.00       0  23.91\n## 11   0.00    0  0.00    0.00       0   9.38\n## 12   2.56    0  5.13    5.13       0  10.26\n## 23   0.00    0  0.00    0.00       0   5.00"},{"path":"week-3-dictionary-based-approaches.html","id":"homework","chapter":"3 Week 3: Dictionary-Based Approaches","heading":"3.6 Homework","text":"Replicate results Figure 3 Ventura et al. (2021)Look keywords context Biden ventura_etal_df dataset, compare results data, pre-processed (.e., lower-case, remove stopwords, etc.). provides information context Biden appears comments?different collocation approach ventura_etal_df dataset, pre-process data (.e., lower-case, remove stopwords, etc.). approach (pre-processed pre-processed) provides better picture corpus collocations found?Compare positive sentiments comments mentioning trump comments mentioning biden obtained using bing afinn. Note afinn gives numeric value, need choose threshold determine positive sentiment.Using bing, compare sentiment comments mentioning trump comments mentioning biden using different metrics (e.g., Young Soroka 2012, Martins Baumard 2020, Ventura et al. 2021).Create domain-specific dictionary apply ventura_etal_df dataset. Show limitation dictionary (e.g., false positives) comment much problem wanted conduct analysis corpus.","code":""},{"path":"week-4-complexity-and-similarity.html","id":"week-4-complexity-and-similarity","chapter":"4 Week 4: Complexity and Similarity","heading":"4 Week 4: Complexity and Similarity","text":"","code":""},{"path":"week-4-complexity-and-similarity.html","id":"slides-3","chapter":"4 Week 4: Complexity and Similarity","heading":"Slides","text":"5 Complexity Similarity (link Perusall)","code":""},{"path":"week-4-complexity-and-similarity.html","id":"setup-3","chapter":"4 Week 4: Complexity and Similarity","heading":"4.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(textdata) # text datasets\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(wesanderson) # to prettify\nlibrary(stringdist) # measure string distance\nlibrary(reshape2)"},{"path":"week-4-complexity-and-similarity.html","id":"replicating-the-lecture","chapter":"4 Week 4: Complexity and Similarity","heading":"4.2 Replicating the Lecture","text":"weeks lecture, learned similarity complexity measures word- document-level. follow order lecture slides.","code":""},{"path":"week-4-complexity-and-similarity.html","id":"comparing-text","chapter":"4 Week 4: Complexity and Similarity","heading":"4.3 Comparing Text","text":"different ways compare text, depending unit analysis:\n- Character-level comparisons\n- Token-level comparison","code":""},{"path":"week-4-complexity-and-similarity.html","id":"character-level-comparisons","chapter":"4 Week 4: Complexity and Similarity","heading":"4.3.1 Character-Level Comparisons:","text":"Let’s start using character-level comparisons tools evaluate two documents (case, two statements made given Ontario winter day):?stringdist, know “longest common substring distance defined number unpaired characters. distance equivalent edit distance allowing deletions insertions, weight one.” also learned Levenshtein distance Jaro distance. can easily implement using stringdist function:distance provides slightly different information relation documents. distances stringdist function can compute. something interests , information measure paper.ever used measure work? Actually, yes. combining corpus legislative speeches Ecuadorian Congress data set Ecuadorian legislators, matched names data set using fuzzy matching matching names closely related (even perfect match). example code:saved lot time. still needed validate matches manually match unmatched names.","code":"\ndoc_1 <- \"By the professor’s standards, the weather in Ontario during the Winter term is miserable.\"\ndoc_2 <- \"By the professor’s high standards, the weather in London during the Winter term is depressive.\"\nstringdist(doc_1,doc_2,method = \"lcs\")## [1] 27\nstringdist(doc_1,doc_2,method = \"lv\")## [1] 20\nstringdist(doc_1,doc_2,method = \"jw\")## [1] 0.1768849\n# I have a dataframe df_a and df_b. I want to match names from b to a. I run a loop that goes through all the names b and gives a Jaro distance score for a name in a. I assume that the names are a match when the Jaro distance score is highest AND it is above a threshold (0.4).\nfor(i in 1:length(df_a$CANDIDATO_to_MATCH)){\n  score_temp <- stringdist(df_a$CANDIDATO_to_MATCH[i],df_b$CANDIDATO_MERGE,method = \"jw\") \n  if(max(score_temp)>.4 & length(which(score_temp == max(score_temp)))<2){\n    df_a$CANDIDATO_MERGE[i] <- df_b$CANDIDATO_MERGE[which(score_temp == max(score_temp))]}\n  else{\n    df_a$CANDIDATO_MERGE[i] <- NA}\n}"},{"path":"week-4-complexity-and-similarity.html","id":"token-level-comparisons","chapter":"4 Week 4: Complexity and Similarity","heading":"4.3.2 Token-Level Comparisons:","text":"compare documents token level (.e., many often token coincide), can think document row/column matrix word row/column matrix. call matrices, document-feature matrices dfm. using quanteda first need tokenize corpus:Now ready create dfm:Just matrix (really sparse matrix becomes even sparse corpus grows). Now can measure similarity distance two text. straightforward way just correlate occurrence 1s 0s across texts. intuitive way see transposing dfm presenting shape familiar :Ok, now just use simple correlation test:can see text1 highly correlated text2 text 3. Alternatively, can use built-functions quanteda obtain similar results without transform dfm:can use textstat_simil whole bunch similarity/distance methods:can also present matrices nice plots:Noise!","code":"\ndoc_3 <- \"The professor has strong evidence that the weather in London (Ontario) is miserable and depressive.\"\n\ndocs_toks <- tokens(rbind(doc_1,doc_2,doc_3),\n                            remove_punct = T)\ndocs_toks <- tokens_remove(docs_toks,\n                           stopwords(language = \"en\"))\ndocs_toks## Tokens consisting of 3 documents.\n## text1 :\n## [1] \"professor’s\" \"standards\"   \"weather\"     \"Ontario\"     \"Winter\"     \n## [6] \"term\"        \"miserable\"  \n## \n## text2 :\n## [1] \"professor’s\" \"high\"        \"standards\"   \"weather\"     \"London\"     \n## [6] \"Winter\"      \"term\"        \"depressive\" \n## \n## text3 :\n## [1] \"professor\"  \"strong\"     \"evidence\"   \"weather\"    \"London\"    \n## [6] \"Ontario\"    \"miserable\"  \"depressive\"\ndocs_dmf <- dfm(docs_toks)\ndocs_dmf## Document-feature matrix of: 3 documents, 13 features (41.03% sparse) and 0 docvars.\n##        features\n## docs    professor’s standards weather ontario winter term miserable high london\n##   text1           1         1       1       1      1    1         1    0      0\n##   text2           1         1       1       0      1    1         0    1      1\n##   text3           0         0       1       1      0    0         1    0      1\n##        features\n## docs    depressive\n##   text1          0\n##   text2          1\n##   text3          1\n## [ reached max_nfeat ... 3 more features ]\ndfm_df <- convert(docs_dmf, to = \"matrix\")\ndfm_df_t <- t(dfm_df)\ndfm_df_t##              docs\n## features      text1 text2 text3\n##   professor’s     1     1     0\n##   standards       1     1     0\n##   weather         1     1     1\n##   ontario         1     0     1\n##   winter          1     1     0\n##   term            1     1     0\n##   miserable       1     0     1\n##   high            0     1     0\n##   london          0     1     1\n##   depressive      0     1     1\n##   professor       0     0     1\n##   strong          0     0     1\n##   evidence        0     0     1\ncor(dfm_df_t[,c(1:3)])##            text1      text2      text3\n## text1  1.0000000  0.2195775 -0.4147575\n## text2  0.2195775  1.0000000 -0.6250000\n## text3 -0.4147575 -0.6250000  1.0000000\ntextstat_simil(docs_dmf, margin = \"documents\", method = \"correlation\")## textstat_simil object; method = \"correlation\"\n##        text1  text2  text3\n## text1  1.000  0.220 -0.415\n## text2  0.220  1.000 -0.625\n## text3 -0.415 -0.625  1.000\ntextstat_simil(docs_dmf, margin = \"documents\", method = \"cosine\")## textstat_simil object; method = \"cosine\"\n##       text1 text2 text3\n## text1 1.000 0.668 0.401\n## text2 0.668 1.000 0.375\n## text3 0.401 0.375 1.000\ntextstat_simil(docs_dmf, margin = \"documents\", method = \"jaccard\")## textstat_simil object; method = \"jaccard\"\n##       text1 text2 text3\n## text1  1.00 0.500 0.250\n## text2  0.50 1.000 0.231\n## text3  0.25 0.231 1.000\ntextstat_dist(docs_dmf, margin = \"documents\", method = \"euclidean\")## textstat_dist object; method = \"euclidean\"\n##       text1 text2 text3\n## text1     0  2.24  3.00\n## text2  2.24     0  3.16\n## text3  3.00  3.16     0\ntextstat_dist(docs_dmf, margin = \"documents\", method = \"manhattan\")## textstat_dist object; method = \"manhattan\"\n##       text1 text2 text3\n## text1     0     5     9\n## text2     5     0    10\n## text3     9    10     0\ncos_sim_doc <- textstat_simil(docs_dmf, margin = \"documents\", method = \"cosine\")\ncos_sim_doc <- as.matrix(cos_sim_doc)\n  \n# We do this to use ggplot\ncos_sim_doc_df <- as.data.frame(cos_sim_doc)\ncos_sim_doc_df %>%\n    rownames_to_column() %>%\n  # ggplot prefers \n    melt() %>%\n    ggplot(aes(x = as.character(variable),y = as.character(rowname), col = value)) +\n    geom_tile(col=\"black\", fill=\"white\") + \n    # coord_fixed() +\n    labs(x=\"\",y=\"\",col = \"Cosine Sim\", fill=\"\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(\n      angle = 90,\n      vjust = 1,\n      hjust = 1)) +\n    geom_point(aes(size = value)) + \n    scale_size(guide = 'none') +\n    scale_color_gradient2(mid=\"#A63446\",low= \"#A63446\",high=\"#0C6291\") +\n    scale_x_discrete(expand=c(0,0)) +\n    scale_y_discrete(expand=c(0,0))## Using rowname as id variables"},{"path":"week-4-complexity-and-similarity.html","id":"complexity","chapter":"4 Week 4: Complexity and Similarity","heading":"4.4 Complexity","text":"week’s lecture (one readings) know another way analyzing text computing complexity. Schoonvelde et al. (2019) - Liberals lecture, conservatives communicate: Analyzing complexity ideology 381,609 political speeches, authors use Flesch’s Reading Ease Score measure “complexity” readability text (see ??textstat_readability formula readability measures). Flesch’s Reading Ease Score ranges 0 100, higher values suggest less complex/readable text (e.g., score 90 100 text can understood 5th grade; score 0 30 text can understood college graduate professional). obtains score taking consideration average length sentence, number words, number syllables.Let’s apply readability score open-ended questions 2020 ANES survey, see correlate characteristics respondents.open-ended survey questions ask respondents like dislike Democratic (Joe Biden) Republican (Donald Trump) 2020 US presidential candidates election. Note survey respondent opt question given NA.Let’s check:Makes sense: third row quite easy ready, fourth row bit complex, eleventh row impossible read Spanish.Look … degree makes speak complicated.","code":"\nload(\"data/anes_sample.Rdata\")\nhead(open_srvy)## # A tibble: 6 × 9\n##   V200001 like_dem_pres   dislike_dem_pres like_rep_pres dislike_rep_pres income\n##     <dbl> <chr>           <chr>            <chr>         <chr>             <int>\n## 1  200015 <NA>            nothing about s… belife in a … <NA>                 21\n## 2  200022 <NA>            He wants to tak… <NA>          <NA>                 13\n## 3  200039 He is not Dona… <NA>             <NA>          He is a racist,…     17\n## 4  200046 he look honest… <NA>             <NA>          racism, equalit…      7\n## 5  200053 <NA>            Open borders, l… No war, No o… Ridiculous Covi…     22\n## 6  200060 he is NOT Dona… <NA>             <NA>          He is a crimina…      3\n## # ℹ 3 more variables: pid <int>, edu <int>, age <int>\nread_like_dem_pres <- textstat_readability(open_srvy$like_dem_pres,measure = \"Flesch\")## Warning: NA is replaced by empty string\nopen_srvy$read_like_dem_pres <- read_like_dem_pres$Flesch\nhead(open_srvy[,c(2,10)],15)## # A tibble: 15 × 2\n##    like_dem_pres                                              read_like_dem_pres\n##    <chr>                                                                   <dbl>\n##  1 <NA>                                                                     NA  \n##  2 <NA>                                                                     NA  \n##  3 He is not Donald Trump.                                                 100. \n##  4 he look honest and his politics history.                                 54.7\n##  5 <NA>                                                                     NA  \n##  6 he is NOT Donald Trump !!!!!!                                           100. \n##  7 he has been in gov for almost 50 yrs and was vice for 8 a…               89.6\n##  8 <NA>                                                                     NA  \n##  9 he is wanting to do things to help the people of the US.                 96.0\n## 10 <NA>                                                                     NA  \n## 11 Candidato adecuado para liderar un pais.                                -10.8\n## 12 <NA>                                                                     NA  \n## 13 <NA>                                                                     NA  \n## 14 Everything he stands for.                                                75.9\n## 15 He is very intuned with his voters and their point of view               81.9\nopen_srvy %>%\n  # Remove people who did not answer\n  filter(edu>0) %>%\n  # Remove negative scores\n  filter(read_like_dem_pres>0) %>%\n  ggplot(aes(x=as.factor(edu),y=read_like_dem_pres)) +\n  geom_boxplot(alpha=0.6) +\n  # scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  # scale_fill_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  theme(legend.position=\"bottom\") +\n  labs(x=\"Education\", y = \"Flesch Score\", \n       caption = \"Note: Education goes from 1 - Less the high school credentials to 5 - Graduate Degree\")## filter: removed 131 rows (2%), 8,149 rows remaining## filter: removed 4,413 rows (54%), 3,736 rows remaining"},{"path":"week-4-complexity-and-similarity.html","id":"exercise-optional-1","chapter":"4 Week 4: Complexity and Similarity","heading":"4.5 Exercise (Optional)","text":"Extend analysis ANES data using readiability scores /variables.wanted use similarity/distance measure explore ANES data, go ? able compare using data provided?","code":""},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"week-5-scaling-techniques-and-topic-modeling","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"5 Week 5: Scaling Techniques and Topic Modeling","text":"","code":""},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"slides-4","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"Slides","text":"6 Scaling Techniques Topic Modeling (link Perusall)","code":""},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"setup-4","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"5.1 Setup","text":"always, first load packages ’ll using:","code":"\n# devtools::install_github(\"conjugateprior/austin\")\nlibrary(austin) # just for those sweet wordscores\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textmodels)\nlibrary(stm) # run structural topic models\nlibrary(wesanderson) # to prettify"},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"wordscores","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"5.2 Wordscores","text":"Laver et al. (2003) propose supervised scaling technique called wordscores. learned intuition weeks lecture. now replicate Table 1 Laver Benoit (2003) using austin package. package includes sample data using:Let’s keep reference documents:matrix Figure 1, count count word (case, letters) reference document (documents previously labelled). can give scores A_scores reference text place ideological scale (whatever scale want). estimate wordscores word.Now get virgin text predict textscore estimating average weighted wordscores virgin document:Cool.","code":"\ndata(lbg)\nref <- getdocs(lbg, 1:5)\nref##      docs\n## words  R1  R2  R3  R4  R5\n##    A    2   0   0   0   0\n##    B    3   0   0   0   0\n##    C   10   0   0   0   0\n##    D   22   0   0   0   0\n##    E   45   0   0   0   0\n##    F   78   2   0   0   0\n##    G  115   3   0   0   0\n##    H  146  10   0   0   0\n##    I  158  22   0   0   0\n##    J  146  45   0   0   0\n##    K  115  78   2   0   0\n##    L   78 115   3   0   0\n##    M   45 146  10   0   0\n##    N   22 158  22   0   0\n##    O   10 146  45   0   0\n##    P    3 115  78   2   0\n##    Q    2  78 115   3   0\n##    R    0  45 146  10   0\n##    S    0  22 158  22   0\n##    T    0  10 146  45   0\n##    U    0   3 115  78   2\n##    V    0   2  78 115   3\n##    W    0   0  45 146  10\n##    X    0   0  22 158  22\n##    Y    0   0  10 146  45\n##    Z    0   0   3 115  78\n##    ZA   0   0   2  78 115\n##    ZB   0   0   0  45 146\n##    ZC   0   0   0  22 158\n##    ZD   0   0   0  10 146\n##    ZE   0   0   0   3 115\n##    ZF   0   0   0   2  78\n##    ZG   0   0   0   0  45\n##    ZH   0   0   0   0  22\n##    ZI   0   0   0   0  10\n##    ZJ   0   0   0   0   3\n##    ZK   0   0   0   0   2\n# We do this in the order of the reference texts:\nA_score <- c(-1.5,-0.75,0,0.75,1.5)\nws <- classic.wordscores(ref, scores=A_score)\nws$pi##         Score\n## A  -1.5000000\n## B  -1.5000000\n## C  -1.5000000\n## D  -1.5000000\n## E  -1.5000000\n## F  -1.4812500\n## G  -1.4809322\n## H  -1.4519231\n## I  -1.4083333\n## J  -1.3232984\n## K  -1.1846154\n## L  -1.0369898\n## M  -0.8805970\n## N  -0.7500000\n## O  -0.6194030\n## P  -0.4507576\n## Q  -0.2992424\n## R  -0.1305970\n## S   0.0000000\n## T   0.1305970\n## U   0.2992424\n## V   0.4507576\n## W   0.6194030\n## X   0.7500000\n## Y   0.8805970\n## Z   1.0369898\n## ZA  1.1846154\n## ZB  1.3232984\n## ZC  1.4083333\n## ZD  1.4519231\n## ZE  1.4809322\n## ZF  1.4812500\n## ZG  1.5000000\n## ZH  1.5000000\n## ZI  1.5000000\n## ZJ  1.5000000\n## ZK  1.5000000\nvir <- getdocs(lbg, 'V1')\nvir##      docs\n## words  V1\n##    A    0\n##    B    0\n##    C    0\n##    D    0\n##    E    0\n##    F    0\n##    G    0\n##    H    2\n##    I    3\n##    J   10\n##    K   22\n##    L   45\n##    M   78\n##    N  115\n##    O  146\n##    P  158\n##    Q  146\n##    R  115\n##    S   78\n##    T   45\n##    U   22\n##    V   10\n##    W    3\n##    X    2\n##    Y    0\n##    Z    0\n##    ZA   0\n##    ZB   0\n##    ZC   0\n##    ZD   0\n##    ZE   0\n##    ZF   0\n##    ZG   0\n##    ZH   0\n##    ZI   0\n##    ZJ   0\n##    ZK   0\n# predict textscores for the virgin documents\npredict(ws, newdata=vir)## 37 of 37 words (100%) are scorable\n## \n##     Score Std. Err. Rescaled  Lower  Upper\n## V1 -0.448    0.0119   -0.448 -0.459 -0.437"},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"wordfish","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"5.3 Wordfish","text":"Slapin Proksch (2008) propose unsupervised scaling model places texts one-dimensional scale. underlying assumption \\[w_{ik} ∼ Poisson(\\lambda _{ik})\\]\n\\[\\lambda_{ik} = exp(α_i +ψ_k +β_k ×θ_i)\\]\\(\\lambda_{ik}\\) generated \\(α_i\\) (“loquaciousness” politician \\(\\) document fixed-effects), \\(ψ_k\\) (frequency word k), \\(β_k\\) (discrimination parameter word \\(k\\)) , importantly, \\(θ_i\\) (politician’s ideological position). Let’s believe moment peer-review system works use textmodel_wordfish() function estimate ideological positions U.S. Presidents using inaugural speeches.text pretty clean, can change corpus object dfm apply textmodel_wordfish():Let’s see made sense. Since party president, see Republican cluster together apart Democrats (something):Two things note. First, direction scale theoretically-based decision researcher make (algorithm). case, based results, say positive values left-leaning negative values right-leaning. can switch (visualization purposes) just multiplying -1:\nSecond, seems mismatch theoretical expectations empirical observations. assume Republicans (Democrats) talk similarly Republicans (Democrats) different Democrats (Republicans). However, case. happening?One answer language changes time issues change time meant Democrat Republican changed time, change picked model:seems one possible explanation. pre-processing steps substantively modified texts (see Denny Spirling 2018). can estimate model using different pre-processed text:time main predictor, maybe need think periods time comparable parties (e.g., Civil Rights Act).","code":"\nus_pres <- readxl::read_xlsx(path = \"data/inaugTexts.xlsx\")\nhead(us_pres)## # A tibble: 6 × 4\n##   inaugSpeech                                               Year President party\n##   <chr>                                                    <dbl> <chr>     <chr>\n## 1 \"My Countrymen, It a relief to feel that no heart but m…  1853 Pierce    Demo…\n## 2 \"Fellow citizens, I appear before you this day to take …  1857 Buchanan  Demo…\n## 3 \"Fellow-Citizens of the United States: In compliance wi…  1861 Lincoln   Repu…\n## 4 \"Fellow-Countrymen:\\r\\n\\r\\nAt this second appearing to …  1865 Lincoln   Repu…\n## 5 \"Citizens of the United States:\\r\\n\\r\\nYour suffrages h…  1869 Grant     Repu…\n## 6 \"Fellow-Citizens:\\r\\n\\r\\nUnder Providence I have been c…  1873 Grant     Repu…\ncorpus_us_pres <- corpus(us_pres,\n                     text_field = \"inaugSpeech\",\n                     unique_docnames = TRUE)\n\nsummary(corpus_us_pres)## Corpus consisting of 41 documents, showing 41 documents:\n## \n##    Text Types Tokens Sentences Year    President      party\n##   text1  1164   3631       104 1853       Pierce   Democrat\n##   text2   944   3080        89 1857     Buchanan   Democrat\n##   text3  1074   3992       135 1861      Lincoln Republican\n##   text4   359    774        26 1865      Lincoln Republican\n##   text5   484   1223        40 1869        Grant Republican\n##   text6   551   1469        43 1873        Grant Republican\n##   text7   830   2698        59 1877        Hayes Republican\n##   text8  1020   3206       111 1881     Garfield Republican\n##   text9   675   1812        44 1885    Cleveland   Democrat\n##  text10  1351   4720       157 1889     Harrison Republican\n##  text11   821   2125        58 1893    Cleveland   Democrat\n##  text12  1231   4345       130 1897     McKinley Republican\n##  text13   854   2437       100 1901     McKinley Republican\n##  text14   404   1079        33 1905  T Roosevelt Republican\n##  text15  1437   5821       158 1909         Taft Republican\n##  text16   658   1882        68 1913       Wilson   Democrat\n##  text17   548   1648        59 1917       Wilson   Democrat\n##  text18  1168   3717       148 1921      Harding Republican\n##  text19  1220   4440       196 1925     Coolidge Republican\n##  text20  1089   3855       158 1929       Hoover Republican\n##  text21   742   2052        85 1933 FD Roosevelt   Democrat\n##  text22   724   1981        96 1937 FD Roosevelt   Democrat\n##  text23   525   1494        68 1941 FD Roosevelt   Democrat\n##  text24   274    619        27 1945 FD Roosevelt   Democrat\n##  text25   780   2495       116 1949       Truman   Democrat\n##  text26   899   2729       119 1953   Eisenhower Republican\n##  text27   620   1883        92 1957   Eisenhower Republican\n##  text28   565   1516        52 1961      Kennedy   Democrat\n##  text29   567   1697        93 1965      Johnson   Democrat\n##  text30   742   2395       103 1969        Nixon Republican\n##  text31   543   1978        68 1973        Nixon Republican\n##  text32   527   1363        52 1977       Carter   Democrat\n##  text33   901   2771       129 1981       Reagan Republican\n##  text34   924   2897       124 1985       Reagan Republican\n##  text35   794   2666       141 1989         Bush Republican\n##  text36   642   1833        81 1993      Clinton   Democrat\n##  text37   772   2423       111 1997      Clinton   Democrat\n##  text38   620   1804        97 2001         Bush Republican\n##  text39   773   2321       100 2005         Bush Republican\n##  text40   937   2667       110 2009        Obama   Democrat\n##  text41   814   2317        88 2013        Obama   Democrat\n# We do the whole tokenization sequence\ntoks_us_pres <- tokens(corpus_us_pres,\n                   remove_numbers = TRUE, # Thinks about this\n                   remove_punct = TRUE, # Remove punctuation!\n                   remove_url = TRUE) # Might be helpful\n\ntoks_us_pres <- tokens_remove(toks_us_pres,\n                              # Should we though? See Denny and Spirling (2018)\n                              c(stopwords(language = \"en\")),\n                              padding = F)\n\ntoks_us_pres <- tokens_wordstem(toks_us_pres, language = \"en\")\n\ndfm_us_pres <- dfm(toks_us_pres)\n\nwfish_us_pres <- textmodel_wordfish(dfm_us_pres, dir = c(28,30)) #Does not really matter what the starting values are, they just serve as anchors for the relative position of the rest of the texts. In this case, I chose Kennedy and Nixon.  \nsummary(wfish_us_pres)## \n## Call:\n## textmodel_wordfish.dfm(x = dfm_us_pres, dir = c(28, 30))\n## \n## Estimated Document Positions:\n##           theta      se\n## text1  -0.95629 0.03619\n## text2  -1.27077 0.03413\n## text3  -1.40878 0.02863\n## text4  -0.37189 0.08907\n## text5  -1.19374 0.05617\n## text6  -0.98770 0.05747\n## text7  -1.25058 0.03681\n## text8  -1.15827 0.03506\n## text9  -1.06959 0.04866\n## text10 -1.37028 0.02601\n## text11 -1.09563 0.04336\n## text12 -1.36432 0.02716\n## text13 -0.96921 0.04392\n## text14  0.14969 0.07992\n## text15 -1.67350 0.01837\n## text16  0.04362 0.05968\n## text17 -0.14872 0.06481\n## text18 -0.23053 0.04050\n## text19 -0.64321 0.03619\n## text20 -0.81689 0.03636\n## text21 -0.26532 0.05470\n## text22  0.26579 0.05564\n## text23  0.56628 0.06543\n## text24  0.82820 0.09574\n## text25  0.09676 0.04999\n## text26  0.37480 0.04777\n## text27  0.60678 0.05655\n## text28  0.92039 0.05651\n## text29  0.95967 0.05604\n## text30  1.42421 0.03773\n## text31  0.93162 0.05098\n## text32  0.87564 0.06215\n## text33  1.13335 0.04084\n## text34  1.19832 0.03745\n## text35  1.26112 0.03960\n## text36  1.38125 0.04374\n## text37  1.37962 0.03680\n## text38  0.86932 0.05416\n## text39  0.74023 0.04862\n## text40  1.18811 0.03946\n## text41  1.05044 0.04446\n## \n## Estimated Feature Scores:\n##      countrymen  relief    feel   heart    can   know  person  regret   bitter\n## beta    -0.5490 -0.9578 -0.4909 0.80654 0.1395 0.9422 -0.1117 -0.2531  0.00357\n## psi     -0.5455 -1.8438 -0.3950 0.08677 2.1155 0.6955 -0.1544 -2.8008 -2.01944\n##       sorrow    born   posit suitabl  other    rather   desir circumst   call\n## beta  0.5176  0.7626 -0.9823  -4.120 0.3938  0.005296 -0.6213  -0.6355 0.3744\n## psi  -2.0333 -0.8259 -1.2129  -6.134 0.1718 -0.591942 -0.5404  -1.3620 0.5610\n##         limit    period presid destini   republ    fill profound    sens\n## beta -0.07365 -0.004602 0.4897  0.2596 -0.27095  0.6832 -0.05067 -0.1156\n## psi  -0.02462 -1.040467 0.4726 -0.2598  0.08116 -1.6845 -1.52213 -0.2337\n##      respons     noth    like  shrink\n## beta  0.2263 -0.08437 0.01591  0.1689\n## psi   0.8392 -0.25100 0.19881 -1.7082\n# Get predictions:\nwfish_preds <- predict(wfish_us_pres, interval = \"confidence\")\n\n# Tidy everything up:\nposi_us_pres <- data.frame(docvars(corpus_us_pres),\n                      wfish_preds$fit) %>%\n  arrange(fit)\n\n# Plot\nposi_us_pres %>%\n  ggplot(aes(x = fit, y = reorder(President,fit), xmin = lwr, xmax = upr, color = party)) +\n  geom_point(alpha = 0.8) +\n  geom_errorbarh(height = 0) +\n  labs(x = \"Position\", y = \"\", color = \"Party\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  ggtitle(\"Estimated Positions\")\n# Plot inverse\nposi_us_pres %>%\n  ggplot(aes(x = -fit, y = reorder(President,fit), xmin = -lwr, xmax = -upr, color = party)) +\n  geom_point(alpha = 0.8) +\n  geom_errorbarh(height = 0) +\n  labs(x = \"Position\", y = \"\", color = \"Party\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  ggtitle(\"Estimated Positions\")\n# Plot time\nposi_us_pres %>%\n  ggplot(aes(y = -fit, x = Year, ymin = -lwr, ymax = -upr, color = party)) +\n  geom_point(alpha = 0.8) +\n  geom_errorbar() +\n  labs(x = \"Year\", y = \"Position\", color = \"Party\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  ggtitle(\"Estimated Positions\")\n# Tokenization only removing punctuation\ntoks_us_pres2 <- tokens(corpus_us_pres,\n                   remove_punct = TRUE) \n\ndfm_us_pres2 <- dfm(toks_us_pres2)\nwfish_us_pres <- textmodel_wordfish(dfm_us_pres2, dir = c(28,30))  \n\n# Get predictions:\nwfish_preds <- predict(wfish_us_pres, interval = \"confidence\")\n\n# Tidy everything up:\nposi_us_pres <- data.frame(docvars(corpus_us_pres),\n                      wfish_preds$fit) %>%\n  arrange(fit)\n\n# Plot\nposi_us_pres %>%\n  ggplot(aes(x = -fit, y = reorder(President,fit), xmin = -lwr, xmax = -upr, color = party)) +\n  geom_point(alpha = 0.8) +\n  geom_errorbarh(height = 0) +\n  labs(x = \"Position\", y = \"\", color = \"Party\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  ggtitle(\"Estimated Positions (No Pre-Processing\")\n# Plot time\nposi_us_pres %>%\n  ggplot(aes(y = -fit, x = Year, ymin = -lwr, ymax = -upr, color = party)) +\n  geom_point(alpha = 0.8) +\n  geom_errorbar() +\n  labs(x = \"Year\", y = \"Position\", color = \"Party\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() +\n  ggtitle(\"Estimated Positions\")"},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"structural-topic-models","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"5.4 Structural Topic Models","text":"STM provides two ways include contextual information “guide” estimation model. First, topic prevalence can vary metadata (e.g. Republicans talk military issues Democrats). Second, topic content can vary metadata (e.g. Republicans talk military issues differently Democrats).can run STM using stm package. stm package includes complete workflow (.e. raw text figures), planning use future highly encourage check . stm() takes dfm produces topics. specify prevalence terms, estimate LDA. Since Bayesian approach, recommended set seed value future replication. also need set \\(K\\) number topics. many topics right number topics? good number. many pre-specified topics categories might meaningless. , might piling together two topics. Note changes ) number topics, b) prevalence term, c) omitted words, d) seed value, can (greatly) change outcome. validation becomes crucial (review see Wilkerson Casas 2017).Using presidential speeches data, use stm estimate topics surrounding inaugural addresses. prevalence term, add party speaker. set number topics 10 (corpus big probably set ~30 work way ).nice thing stm() function allows us see “real-time” going within black box. can summarize process following way (similar collapsed Gibbs sampling, stm() function sort uses):Go document, randomly assign word document one topics \\(\\displaystyle t\\k\\).Go document, randomly assign word document one topics \\(\\displaystyle t\\k\\).Notice random assignment already gives topic representations documents word distributions topics (albeit good ones).Notice random assignment already gives topic representations documents word distributions topics (albeit good ones).improve , document \\(\\displaystyle W\\) following:improve , document \\(\\displaystyle W\\) following:3.1 Go word \\(\\displaystyle w\\) \\(\\displaystyle W\\)3.1.1 topic \\(\\displaystyle t\\), compute two things:3.1.1.1 \\(\\displaystyle p(t|W)\\) = proportion words document \\(\\displaystyle W\\) currently assigned topic \\(\\displaystyle t\\), and3.1.1.2 \\(\\displaystyle p(w|t)\\) = proportion assignments topic \\(\\displaystyle t\\) documents come word \\(\\displaystyle w\\). Reassign \\(\\displaystyle w\\) new topic, choose topic \\(\\displaystyle t\\) probability \\(\\displaystyle p(t|W)*p(w|t)\\). worth noting according generative model, essentially probability topic \\(\\displaystyle t\\) generated word \\(\\displaystyle w\\), makes sense resample current word’s topic probability. (Also, ’m glossing couple things , particular use priors/pseudocounts probabilities.)3.1.1.3 words, step, ’re assuming topic assignments except current word question correct, updating assignment current word using model documents generated.repeating previous step large number times, ’ll eventually reach roughly steady state assignments pretty good. use assignments estimate topic mixtures document (counting proportion words assigned topic within document) words associated topic (counting proportion words assigned topic overall).(explanation taken ). Let’s explore topics produced:FREX weights words overall frequency exclusive topic. Lift weights words dividing frequency topics, therefore giving higher weight words appear less frequently topics. Similar Lift, Score divides log frequency word topic log frequency word topics (Roberts et al. 2013). Bischof Airoldi (2012) show value using FREX measures.can use plot() function show topics.Topic 5 seems economy: revenue, tariffs, etc. Topic 3 slavery adn Civil War. want see sample specific topic:long speech.can (/must) run diagnostics. two qualities looking model: semantic coherence exclusivity. Exclusivity based FREX labeling matrix. Semantic coherence criterion developed Mimno et al. (2011) maximizes probable words given topic frequently co-occur together. Mimno et al. (2011) show metric correlates well human judgement topic quality. Yet, fairly easy obtain high semantic coherence important see tandem exclusivity. Let’s see exclusive words topic:can also see semantic coherence topics –words topic generates co-occur often document–:can also see overall quality topic model:, metrics really useful (numbers even mean?). useful looking “optimal” number topics.can now compare performance model based semantic coherence exclusivity. looking high exclusivity high coherence (top-right corner):Maybe theory difference topic prevalence across parties. can see topic proportions topic model object:Note prevalence terms \\(\\theta\\) add 1 within document. , term tells us proportion (words associated ) topics document:connecting info dfm seeing differences proportion topic 5 (economy) addressed side.Seems Republican presidents address economy speeches. Let’s plot proportion president:can something similar stm function directly. just need specify functional form add document variables.results, Republicans mention Topic 5: Economy.","code":"\nstm_us_pres <- stm(dfm_us_pres, K = 10, seed = 1984,\n                   prevalence = ~party,\n                   init.type = \"Spectral\")## Beginning Spectral Initialization \n##   Calculating the gram matrix...\n##   Finding anchor words...\n##      ..........\n##   Recovering initialization...\n##      ..............................................\n## Initialization complete.\n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 1 (approx. per word bound = -7.071) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 2 (approx. per word bound = -6.881, relative change = 2.689e-02) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 3 (approx. per word bound = -6.819, relative change = 8.961e-03) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 4 (approx. per word bound = -6.790, relative change = 4.253e-03) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 5 (approx. per word bound = -6.780, relative change = 1.521e-03) \n## Topic 1: us, new, world, nation, let \n##  Topic 2: new, can, us, nation, work \n##  Topic 3: constitut, state, union, can, law \n##  Topic 4: nation, must, us, peopl, can \n##  Topic 5: govern, peopl, upon, state, law \n##  Topic 6: nation, freedom, america, govern, peopl \n##  Topic 7: us, america, must, nation, american \n##  Topic 8: upon, nation, govern, peopl, shall \n##  Topic 9: world, nation, peopl, peac, can \n##  Topic 10: us, nation, govern, must, peopl \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 6 (approx. per word bound = -6.775, relative change = 6.929e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 7 (approx. per word bound = -6.771, relative change = 5.320e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 8 (approx. per word bound = -6.768, relative change = 5.267e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 9 (approx. per word bound = -6.765, relative change = 4.328e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 10 (approx. per word bound = -6.763, relative change = 3.022e-04) \n## Topic 1: us, new, world, let, nation \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, union, can, shall \n##  Topic 4: nation, must, peopl, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, peopl, govern \n##  Topic 7: us, america, must, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, world, peopl, peac, can \n##  Topic 10: us, govern, nation, peopl, must \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 11 (approx. per word bound = -6.762, relative change = 2.094e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 12 (approx. per word bound = -6.760, relative change = 1.745e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 13 (approx. per word bound = -6.759, relative change = 1.485e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 14 (approx. per word bound = -6.759, relative change = 1.147e-04) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 15 (approx. per word bound = -6.758, relative change = 1.018e-04) \n## Topic 1: us, new, let, world, nation \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, union, can, shall \n##  Topic 4: nation, must, peopl, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, peopl, govern \n##  Topic 7: us, america, must, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, must \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 16 (approx. per word bound = -6.757, relative change = 9.728e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 17 (approx. per word bound = -6.757, relative change = 8.328e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 18 (approx. per word bound = -6.756, relative change = 7.150e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 19 (approx. per word bound = -6.756, relative change = 5.364e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 20 (approx. per word bound = -6.756, relative change = 3.903e-05) \n## Topic 1: us, new, let, world, nation \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, govern, shall, union \n##  Topic 4: nation, must, peopl, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, peopl, govern \n##  Topic 7: us, america, must, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, must \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 21 (approx. per word bound = -6.755, relative change = 3.678e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 22 (approx. per word bound = -6.755, relative change = 3.378e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 23 (approx. per word bound = -6.755, relative change = 3.008e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 24 (approx. per word bound = -6.755, relative change = 3.311e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 25 (approx. per word bound = -6.754, relative change = 3.247e-05) \n## Topic 1: us, new, let, world, nation \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, govern, peopl, shall \n##  Topic 4: nation, must, peopl, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, govern, peopl \n##  Topic 7: us, must, america, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, must \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 26 (approx. per word bound = -6.754, relative change = 2.886e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 27 (approx. per word bound = -6.754, relative change = 2.778e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 28 (approx. per word bound = -6.754, relative change = 2.814e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 29 (approx. per word bound = -6.754, relative change = 4.342e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 30 (approx. per word bound = -6.753, relative change = 2.111e-05) \n## Topic 1: us, new, let, nation, world \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, govern, peopl, shall \n##  Topic 4: nation, peopl, must, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, govern, peopl \n##  Topic 7: us, must, america, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, world \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 31 (approx. per word bound = -6.753, relative change = 1.621e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 32 (approx. per word bound = -6.753, relative change = 1.678e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 33 (approx. per word bound = -6.753, relative change = 1.686e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 34 (approx. per word bound = -6.753, relative change = 1.705e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 35 (approx. per word bound = -6.753, relative change = 1.793e-05) \n## Topic 1: us, new, let, nation, world \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, govern, peopl, shall \n##  Topic 4: nation, peopl, must, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, govern, peopl \n##  Topic 7: us, must, america, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, world \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 36 (approx. per word bound = -6.753, relative change = 2.033e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 37 (approx. per word bound = -6.753, relative change = 2.216e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 38 (approx. per word bound = -6.752, relative change = 1.878e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 39 (approx. per word bound = -6.752, relative change = 1.591e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 40 (approx. per word bound = -6.752, relative change = 1.384e-05) \n## Topic 1: us, new, let, nation, world \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, govern, peopl, shall \n##  Topic 4: nation, peopl, must, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, govern, peopl \n##  Topic 7: us, must, america, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, world \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 41 (approx. per word bound = -6.752, relative change = 1.337e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 42 (approx. per word bound = -6.752, relative change = 1.381e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 43 (approx. per word bound = -6.752, relative change = 1.388e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 44 (approx. per word bound = -6.752, relative change = 1.720e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 45 (approx. per word bound = -6.752, relative change = 1.363e-05) \n## Topic 1: us, new, let, nation, world \n##  Topic 2: us, new, can, nation, work \n##  Topic 3: constitut, state, govern, peopl, shall \n##  Topic 4: nation, peopl, must, us, world \n##  Topic 5: govern, peopl, upon, law, state \n##  Topic 6: nation, freedom, america, peopl, govern \n##  Topic 7: us, must, america, nation, american \n##  Topic 8: upon, nation, govern, peopl, can \n##  Topic 9: nation, peopl, world, can, peac \n##  Topic 10: us, govern, nation, peopl, world \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 46 (approx. per word bound = -6.752, relative change = 1.725e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Completing Iteration 47 (approx. per word bound = -6.752, relative change = 1.410e-05) \n## .........................................\n## Completed E-Step (0 seconds). \n## Completed M-Step. \n## Model Converged\nlabelTopics(stm_us_pres)## Topic 1 Top Words:\n##       Highest Prob: us, new, let, nation, world, can, america \n##       FREX: let, centuri, togeth, dream, new, promis, weak \n##       Lift: 200th, 20th, micah, rhetor, 18th, 19th, accident \n##       Score: role, dream, abroad, third, explor, shape, proud \n## Topic 2 Top Words:\n##       Highest Prob: us, new, can, nation, work, world, great \n##       FREX: friend, mr, thing, breez, word, blow, fact \n##       Lift: breez, crucial, addict, alloc, assistanc, bacteria, bicentenni \n##       Score: breez, crucial, blow, page, manger, thank, sometim \n## Topic 3 Top Words:\n##       Highest Prob: constitut, state, govern, peopl, shall, can, law \n##       FREX: case, constitut, slave, union, territori, slaveri, minor \n##       Lift: alleg, anarchi, besid, compulsori, constru, cover, dissatisfi \n##       Score: case, slaveri, territori, slave, invas, provis, fli \n## Topic 4 Top Words:\n##       Highest Prob: nation, peopl, must, us, world, can, govern \n##       FREX: activ, republ, task, confid, industri, inspir, normal \n##       Lift: abnorm, acclaim, aright, changer, comiti, frugal, gaze \n##       Score: normal, activ, amid, readjust, self-reli, relationship, unshaken \n## Topic 5 Top Words:\n##       Highest Prob: govern, peopl, upon, law, state, countri, nation \n##       FREX: revenu, tariff, offic, appoint, busi, consider, proper \n##       Lift: ampli, antitrust, board, box, boycott, congression, dakota \n##       Score: revenu, legisl, enforc, polici, negro, interst, tariff \n## Topic 6 Top Words:\n##       Highest Prob: nation, freedom, america, peopl, govern, know, democraci \n##       FREX: democraci, ideal, million, liberti, freedom, came, seen \n##       Lift: paint, >, aught, autocrat, baffl, baggag, beli \n##       Score: democraci, paint, million, magna, excus, seen, encount \n## Topic 7 Top Words:\n##       Highest Prob: us, must, america, nation, american, world, peopl \n##       FREX: journey, stori, generat, storm, america, job, ideal \n##       Lift: afghanistan, aids, alongsid, anchor, anybodi, apathi, appalachia \n##       Score: stori, journey, job, capitol, storm, thank, drift \n## Topic 8 Top Words:\n##       Highest Prob: upon, nation, govern, peopl, can, shall, great \n##       FREX: enforc, counsel, organ, island, thought, upon, integr \n##       Lift: cuba, eighteenth, adapt, aspect, creation, cuban, dilig \n##       Score: enforc, island, cuba, counsel, organ, eighteenth, adapt \n## Topic 9 Top Words:\n##       Highest Prob: nation, peopl, world, can, peac, must, free \n##       FREX: resourc, contribut, repres, everywher, result, free, europ \n##       Lift: display, joint, likewis, mockeri, philosophi, array, barter \n##       Score: europ, philosophi, commun, contribut, precept, tax, program \n## Topic 10 Top Words:\n##       Highest Prob: us, govern, nation, peopl, world, must, american \n##       FREX: weapon, tax, believ, hero, man, reduc, dream \n##       Lift: 50th, absent, adam, alamo, anger, ant, artilleri \n##       Score: weapon, hero, monument, nuclear, spend, tax, soviet\nplot(stm_us_pres, type = \"summary\", labeltype = \"frex\") # or prob, lift score\nfindThoughts(stm_us_pres, texts = as.character(corpus_us_pres)[docnames(dfm_us_pres)], topics = 3)  \ndotchart(exclusivity(stm_us_pres), labels = 1:10)\ndotchart(semanticCoherence(stm_us_pres,dfm_us_pres), labels = 1:10)\ntopicQuality(stm_us_pres,dfm_us_pres)##  [1]  -5.287875  -7.035510 -12.913601  -3.154439  -8.562729 -11.770514\n##  [7]  -4.095783  -5.495206  -5.782951  -4.794982\n##  [1] 8.975188 9.296784 8.794789 8.229003 7.886663 9.119321 8.616780 7.907198\n##  [9] 8.689432 8.813805\nstm_us_pres_10_15_20 <- manyTopics(dfm_us_pres,\n                       prevalence = ~ party,\n                       K = c(10,15,20), runs=2,\n                       # max.em.its = 100, \n                       init.type = \"Spectral\") # It takes around 250 iterations for the model to converge. Depending on your computer, this can take a while.\nk_10 <- stm_us_pres_10_15_20$out[[1]] # k_10 is an stm object which can be explored and used like any other topic model. \nk_15 <- stm_us_pres_10_15_20$out[[2]]\nk_20 <- stm_us_pres_10_15_20$out[[3]]\n\n# I will just graph the 'quality' of each model:\ntopicQuality(k_10,dfm_us_pres)##  [1]  -5.287875  -7.035510 -12.913601  -3.154439  -8.562729 -11.770514\n##  [7]  -4.095783  -5.495206  -5.782951  -4.794982\n##  [1] 8.975188 9.296784 8.794789 8.229003 7.886663 9.119321 8.616780 7.907198\n##  [9] 8.689432 8.813805\ntopicQuality(k_15,dfm_us_pres)##  [1]  -8.282551 -10.661122  -9.146329  -6.243444 -10.002100 -11.315179\n##  [7]  -3.107797  -4.907182  -5.059424  -4.905652  -7.864316 -13.149897\n## [13]  -6.834348 -11.917696  -4.182962\n##  [1] 9.225426 9.359212 9.252890 9.186251 9.037698 9.150213 8.615448 8.497762\n##  [9] 8.545416 9.139213 8.183189 9.136856 8.467946 9.642172 8.453394\ntopicQuality(k_20,dfm_us_pres)##  [1]  -8.136428 -22.245476 -21.390006  -6.602534 -11.543624 -10.272049\n##  [7]  -3.923380  -5.506620  -7.188791 -12.486262 -10.086060 -13.443443\n## [13] -15.978725 -12.256070 -10.137597 -11.231218  -6.177453  -4.358259\n## [19]  -5.246579  -2.209688\n##  [1] 9.489401 9.872037 9.761442 9.184861 9.370893 9.330946 9.019413 8.522081\n##  [9] 8.633818 9.649097 8.307636 9.125968 8.957859 9.644505 9.533281 8.824664\n## [17] 9.488938 9.220756 8.596374 8.727104\nhead(stm_us_pres$theta)##              [,1]         [,2]         [,3]         [,4]         [,5]\n## [1,] 0.0001965139 8.910433e-05 8.224181e-05 1.366525e-04 0.0003038924\n## [2,] 0.0004943115 6.694931e-05 9.823350e-01 1.233605e-04 0.0162390724\n## [3,] 0.0002929573 4.935711e-05 9.988060e-01 1.794408e-05 0.0005358752\n## [4,] 0.1143189826 9.475844e-04 8.765515e-01 2.637466e-04 0.0030686131\n## [5,] 0.0117265212 2.101959e-04 6.658082e-03 1.163485e-03 0.9768804322\n## [6,] 0.0254848264 3.421690e-04 4.654306e-03 1.751075e-03 0.9609666414\n##              [,6]         [,7]         [,8]         [,9]        [,10]\n## [1,] 2.109828e-04 1.271428e-04 9.985770e-01 1.726828e-04 1.037666e-04\n## [2,] 1.673617e-04 1.837545e-04 2.369797e-04 9.412580e-05 5.907198e-05\n## [3,] 6.424073e-05 5.105122e-05 6.933715e-05 6.752421e-05 4.572869e-05\n## [4,] 7.946389e-04 8.965244e-04 7.116956e-04 1.080859e-03 1.365866e-03\n## [5,] 5.210418e-04 4.736609e-04 8.391293e-04 8.456329e-04 6.818197e-04\n## [6,] 7.226817e-04 6.610786e-04 1.740575e-03 1.314907e-03 2.361741e-03\nsum(stm_us_pres$theta[1,])## [1] 1\nsum(stm_us_pres$theta[2,])## [1] 1\nlibrary(fixest)\nlibrary(sjPlot)## #refugeeswelcome\nus_pres_prev <- data.frame(topic5 = stm_us_pres$theta[,5], docvars(dfm_us_pres))\nfeols_topic5 <- feols(topic5 ~ party , data = us_pres_prev)\nplot_model(feols_topic5, type = \"pred\", term = \"party\") +\n  theme_minimal() +\n  labs(caption = \"Stat. Sig. at p<0.1\", x=\"\", y=\"Topic Prevalence\")\nus_pres_prev %>%\n  # Going to log the prev of topic 5 because is quite skewed but you should probably leave as is if you want to explore how topics are addressed. \n  ggplot(aes(x = log(topic5), y = reorder(President,topic5), color = party)) +\n  geom_point(alpha = 0.8) +\n  labs(x = \"log(Theta)\", y = \"\", color = \"Party\") +\n  scale_color_manual(values = wes_palette(\"BottleRocket2\")) +\n  theme_minimal() \ntopics_us_pres <- estimateEffect(c(3,5) ~ party, stm_us_pres, docvars(dfm_us_pres)) # You can compare other topics by changing c(6,9). \nplot(topics_us_pres, \"party\", method = \"difference\",\n     cov.value1 = \"Democrat\", \n     cov.value2 = \"Republican\",\n     labeltype = \"custom\",\n     xlim = c(-.75,.25),\n     custom.labels = c('Topic 3: Slavery', 'Topic 5: Economy'),\n     model = stm_us_pres)"},{"path":"week-5-scaling-techniques-and-topic-modeling.html","id":"exercise-2","chapter":"5 Week 5: Scaling Techniques and Topic Modeling","heading":"5.5 Exercise 2:","text":"hard time scaling text. looked possible problems. possible solutions want position U.S. presidents ideological scale using text?Use data/candidate-tweets.csv data run STM. Decide covariates going. Decide whether use data sample data. Decide going aggregate/divide way text (.e., decide unit analysis). Decide number topics look (try one option). can tell topics tweeted 2015 U.S. primaries candidates?Choose three topics. Can place candidates ideological scale within topic (determine \\(theta\\) threshold can say tweet mostly topic)? make sense? ?","code":""},{"path":"week-6-word-embeddings.html","id":"week-6-word-embeddings","chapter":"6 Week 6: Word Embeddings","heading":"6 Week 6: Word Embeddings","text":"","code":""},{"path":"week-6-word-embeddings.html","id":"slides-5","chapter":"6 Week 6: Word Embeddings","heading":"Slides","text":"7 Word Embeddings (link Perusall)","code":""},{"path":"week-6-word-embeddings.html","id":"setup-5","chapter":"6 Week 6: Word Embeddings","heading":"6.1 Setup","text":"Today work python. can access jupyter notebook .","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
