[{"path":"index.html","id":"computational-text-analysis","chapter":"“Computational Text Analysis”","heading":"“Computational Text Analysis”","text":" dedicated site course PS9594A: “Computational Text Analysis” Western University, taught Sebastián Vallejo Vera. week, find code, exercises, slides corresponding topic.start, check required software packages . Also, don’t forget read Syllabus Perusall readings course. site corrected/updated throughout semester.","code":""},{"path":"index.html","id":"software-and-packages","chapter":"“Computational Text Analysis”","heading":"0.1 Software and Packages","text":"first part course (Weeks 1 - 5), mainly using R. second part course (Weeks 6 - 11), use combination R Python. assume familiar R language, RStudio, R packages. , please come office hours can help out1. R, main packages need installed:tidyverse (piping)tidylog (helps keep track pipins)tidytext (great working text)quanteda (stands “Quantitative Analysis Textual Data”)\nquanteda.textstats (obtain stats dfm)\nquanteda.textstats (obtain stats dfm)gutenbergr (download texts Project Gutenberg)wesanderson (make things pretty)","code":""},{"path":"index.html","id":"datasets","chapter":"“Computational Text Analysis”","heading":"0.2 Datasets","text":"Throughout class, using number sample datasets. Access datasets provided directly code. Final Essay, can use one following datasets (, even better, can use ):data 1 + linkdata 2 + linkdata 3 + link","code":""},{"path":"index.html","id":"acknowledgments","chapter":"“Computational Text Analysis”","heading":"0.3 Acknowledgments","text":"organization first part course (Weeks 1 - 5) format assignments borrowed Christopher Barrie’s excellent course “Computational Text Analysis”, syllabus prolific Tiago Ventura, Grimmer, Roberts, Stewart’s excellent book, “Text data: new framework machine learning social sciences”. code used throughout course patchwork code, code borrows heavily internet (’s true code). try best give credit original authors code (possible).","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"week-1-a-primer-on-using-text-as-data","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1 Week 1: A Primer on Using Text as Data","text":"","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"slides","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"Slides","text":"1 Introduction CTA (link Perusall)2 Computational Text Analysis (link Perusall)","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"setup","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.1 Setup","text":"first example, replicate (extend) Mendenhall’s (1887) Mendenhall’s (1901) studies word-length distribution.\nFigure 1.1: Mendenhall (1987) - Characteristic Curves Composition.\nFirst load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(wesanderson) # to prettify\nlibrary(gutenbergr) # to get some books\nlibrary(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"get-data","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.2 Get Data","text":"Mendenhall (1887) argued “every writer makes use vocabulary peculiar , character materially change year \nyear productive,” one characteristics length words. Mendenhall (1901) takes , suggests , given assumption, Shakespeare Bacon person2.Let’s get corpus–collection documents–can analyze. can search Gutenberg repository create corpus selected work.","code":"\ngutenberg_metadata %>%\n  filter(author == \"Wilde, Oscar\")## # A tibble: 66 × 8\n##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf\n##           <int> <chr>    <chr>                <int> <chr>    <chr>              \n##  1          174 The Pic… Wilde…                 111 en       \"Gothic Fiction/Mo…\n##  2          301 The Bal… Wilde…                 111 en       \"\"                 \n##  3          773 Lord Ar… Wilde…                 111 en       \"Contemporary Revi…\n##  4          774 Essays … Wilde…                 111 en       \"\"                 \n##  5          790 Lady Wi… Wilde…                 111 en       \"\"                 \n##  6          844 The Imp… Wilde…                 111 en       \"Plays\"            \n##  7          854 A Woman… Wilde…                 111 en       \"Plays\"            \n##  8          873 A House… Wilde…                 111 en       \"Opera\"            \n##  9          875 The Duc… Wilde…                 111 en       \"\"                 \n## 10          885 An Idea… Wilde…                 111 en       \"Plays\"            \n## # ℹ 56 more rows\n## # ℹ 2 more variables: rights <chr>, has_text <lgl>"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"word-length-in-wildes-corpus","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.3 Word Length in Wilde’s Corpus","text":"’s lot Wilde! Let’s focus four plays: “Importance Earnest”, “Woman Importance”, “Lady Windermere’s Fan”, “Ideal Husband”. can download plays using ID number:unit analysis something like line. interested word—also known token—lengths play. clean unwanted text—text add noise analysis—count number words.Now, can change unit analysis token:’s lot words! now create column word length, count number words length (play!).Let’s see distribution play:problem. ?solution (proposed Mendenhall):look . Mendenhall something: author mark terms word length distribution. Wilde, observable change across time (play published different years). , happens compare Wilde’s mark Shakespeare’s? Let’s choose four plays (random) Shakespeare: Midsummer Night’s Dream, Merchant Venice, Much Ado Nothing, Tempest.","code":"\nwilde <- gutenberg_download(c(790,844, 854, 885), \n                            meta_fields = c(\"title\",\"author\"))\nprint(n=25,wilde[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text                                        title         author\n##           <int> <chr>                                       <chr>         <chr> \n##  1          790 \"\"                                          Lady Winderm… Wilde…\n##  2          790 \"\"                                          Lady Winderm… Wilde…\n##  3          790 \"THE PERSONS OF THE PLAY\"                   Lady Winderm… Wilde…\n##  4          790 \"\"                                          Lady Winderm… Wilde…\n##  5          790 \"\"                                          Lady Winderm… Wilde…\n##  6          790 \"Lord Windermere\"                           Lady Winderm… Wilde…\n##  7          790 \"\"                                          Lady Winderm… Wilde…\n##  8          790 \"Lord Darlington\"                           Lady Winderm… Wilde…\n##  9          790 \"\"                                          Lady Winderm… Wilde…\n## 10          790 \"Lord Augustus Lorton\"                      Lady Winderm… Wilde…\n## 11          790 \"\"                                          Lady Winderm… Wilde…\n## 12          790 \"Mr. Dumby\"                                 Lady Winderm… Wilde…\n## 13          790 \"\"                                          Lady Winderm… Wilde…\n## 14          790 \"Mr. Cecil Graham\"                          Lady Winderm… Wilde…\n## 15          790 \"\"                                          Lady Winderm… Wilde…\n## 16          790 \"Mr. Hopper\"                                Lady Winderm… Wilde…\n## 17          790 \"\"                                          Lady Winderm… Wilde…\n## 18          790 \"Parker, Butler\"                            Lady Winderm… Wilde…\n## 19          790 \"\"                                          Lady Winderm… Wilde…\n## 20          790 \"                                * * * * *\" Lady Winderm… Wilde…\n## 21          790 \"\"                                          Lady Winderm… Wilde…\n## 22          790 \"Lady Windermere\"                           Lady Winderm… Wilde…\n## 23          790 \"\"                                          Lady Winderm… Wilde…\n## 24          790 \"The Duchess of Berwick\"                    Lady Winderm… Wilde…\n## 25          790 \"\"                                          Lady Winderm… Wilde…\nwilde <- wilde %>%\n  # Some housekeeping \n  mutate(title = ifelse(str_detect(title,\"Importance of Being\"),\"The Importance of Being Earnest\", title)) %>%\n  # Filter out all empty rows\n  filter(text != \"\") %>%\n  # This is a play. The name of each character before they speak \n  filter(str_detect(text,\"[A-Z]{3,}\")==FALSE)## mutate: changed 3,884 values (27%) of 'title' (0 new NA)## filter: removed 4,232 rows (29%), 10,303 rows remaining## filter: removed 4,208 rows (41%), 6,095 rows remaining\nprint(n=25,wilde[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text                                                title author\n##           <int> <chr>                                               <chr> <chr> \n##  1          790 \"tea-table L._  _Window opening on to terrace L._ … Lady… Wilde…\n##  2          790 \"home to any one who calls.\"                        Lady… Wilde…\n##  3          790 \"                                                 … Lady… Wilde…\n##  4          790 \"he’s come.\"                                        Lady… Wilde…\n##  5          790 \"hands with you.  My hands are all wet with these … Lady… Wilde…\n##  6          790 \"lovely?  They came up from Selby this morning.\"    Lady… Wilde…\n##  7          790 \"table_.]  And what a wonderful fan!  May I look a… Lady… Wilde…\n##  8          790 \"everything.  I have only just seen it myself.  It… Lady… Wilde…\n##  9          790 \"present to me.  You know to-day is my birthday?\"   Lady… Wilde…\n## 10          790 \"life, isn’t it?  That is why I am giving this par… Lady… Wilde…\n## 11          790 \"down.  [_Still arranging flowers_.]\"               Lady… Wilde…\n## 12          790 \"birthday, Lady Windermere.  I would have covered … Lady… Wilde…\n## 13          790 \"front of your house with flowers for you to walk … Lady… Wilde…\n## 14          790 \"you.\"                                              Lady… Wilde…\n## 15          790 \"                                                 … Lady… Wilde…\n## 16          790 \"Foreign Office.  I am afraid you are going to ann… Lady… Wilde…\n## 17          790 \"with her pocket-handkerchief_, _goes to tea-table… Lady… Wilde…\n## 18          790 \"Won’t you come over, Lord Darlington?\"             Lady… Wilde…\n## 19          790 \"miserable, Lady Windermere.  You must tell me wha… Lady… Wilde…\n## 20          790 \"table L._]\"                                        Lady… Wilde…\n## 21          790 \"whole evening.\"                                    Lady… Wilde…\n## 22          790 \"that the only pleasant things to pay _are_ compli… Lady… Wilde…\n## 23          790 \"things we _can_ pay.\"                              Lady… Wilde…\n## 24          790 \"You mustn’t laugh, I am quite serious.  I don’t l… Lady… Wilde…\n## 25          790 \"don’t see why a man should think he is pleasing a… Lady… Wilde…\nwilde_words <- wilde %>%\n  # take the column text and divide it by words\n  unnest_tokens(word, text) %>%\n  # Remove some characters that add noise\n  mutate(word = str_remove_all(word, \"\\\\_\")) ## mutate: changed 1,225 values (2%) of 'word' (0 new NA)\nwilde_words## # A tibble: 60,465 × 4\n##    gutenberg_id title                 author       word     \n##           <int> <chr>                 <chr>        <chr>    \n##  1          790 Lady Windermere's Fan Wilde, Oscar by       \n##  2          790 Lady Windermere's Fan Wilde, Oscar sixteenth\n##  3          790 Lady Windermere's Fan Wilde, Oscar edition  \n##  4          790 Lady Windermere's Fan Wilde, Oscar first    \n##  5          790 Lady Windermere's Fan Wilde, Oscar published\n##  6          790 Lady Windermere's Fan Wilde, Oscar 1893     \n##  7          790 Lady Windermere's Fan Wilde, Oscar first    \n##  8          790 Lady Windermere's Fan Wilde, Oscar issued   \n##  9          790 Lady Windermere's Fan Wilde, Oscar by       \n## 10          790 Lady Windermere's Fan Wilde, Oscar methuen  \n## # ℹ 60,455 more rows\nwilde_words_ct <- wilde_words %>%\n  # Length of each word\n  mutate(word_length = str_length(word)) %>%\n  # Group by word_length and count how many \n  group_by(word_length,title) %>%\n  mutate(total_word_length = n()) %>%\n  # Keep relevant\n  distinct(word_length,title,.keep_all=T) %>%\n  select(word_length,title,author,total_word_length)## mutate: new variable 'word_length' (integer) with 17 unique values and 0% NA## group_by: 2 grouping variables (word_length, title)## mutate (grouped): new variable 'total_word_length' (integer) with 58 unique values and 0% NA## distinct (grouped): removed 60,403 rows (>99%), 62 rows remaining## select: dropped 2 variables (gutenberg_id, word)\nwilde_words_ct %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\")\nwilde_words %>%\n  group_by(title) %>%\n  slice_sample(n=10000) %>%\n  mutate(word_length = str_length(word),\n         median_word_length = median(word_length)) %>%\n  group_by(word_length,title) %>%\n  mutate(total_word_length = n()) %>%\n  distinct(word_length,title,.keep_all=T) %>%\n  select(word_length,title,author,total_word_length,median_word_length) %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  geom_vline(aes(xintercept = median_word_length,color=title,linetype = title))+\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\", linetype = \"\",\n       caption = \"Note: Line type shows median word length.\")## group_by: one grouping variable (title)## slice_sample (grouped): removed 20,465 rows (34%), 40,000 rows remaining## mutate (grouped): new variable 'word_length' (integer) with 17 unique values and 0% NA##                   new variable 'median_word_length' (double) with one unique value and 0% NA## group_by: 2 grouping variables (word_length, title)## mutate (grouped): new variable 'total_word_length' (integer) with 57 unique values and 0% NA## distinct (grouped): removed 39,939 rows (>99%), 61 rows remaining## select: dropped 2 variables (gutenberg_id, word)"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"comparing-shakespeare-and-wilde","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.4 Comparing Shakespeare and Wilde","text":"text cleaner Wilde’s corpus, leave . Also, harder systematically remove name person speaking. problem? ? ?can put together corpora see differences distributions word length.differences? can conclude evidence? limitations approach? alternative approaches study Mendenhall getting ?","code":"\nshakes <- gutenberg_download(c(1520,2242,2243,2235),\n                             meta_fields = c(\"title\",\"author\"))\nprint(n=25,shakes[c(51:75),])## # A tibble: 25 × 4\n##    gutenberg_id text                                                title author\n##           <int> <chr>                                               <chr> <chr> \n##  1         1520 \"Leon.\"                                             Much… Shake…\n##  2         1520 \"How many gentlemen have you lost in this action?\"  Much… Shake…\n##  3         1520 \"\"                                                  Much… Shake…\n##  4         1520 \"Mess.\"                                             Much… Shake…\n##  5         1520 \"But few of any sort, and none of name.\"            Much… Shake…\n##  6         1520 \"\"                                                  Much… Shake…\n##  7         1520 \"Leon.\"                                             Much… Shake…\n##  8         1520 \"A victory is twice itself when the achiever bring… Much… Shake…\n##  9         1520 \"numbers.  I find here that Don Pedro hath bestowe… Much… Shake…\n## 10         1520 \"a young Florentine, called Claudio.\"               Much… Shake…\n## 11         1520 \"\"                                                  Much… Shake…\n## 12         1520 \"Mess.\"                                             Much… Shake…\n## 13         1520 \"Much deserved on his part, and equally remembered… Much… Shake…\n## 14         1520 \"He hath borne himself beyond the promise of his a… Much… Shake…\n## 15         1520 \"in the figure of a lamb, the feats of a lion: he … Much… Shake…\n## 16         1520 \"better bettered expectation than you must expect … Much… Shake…\n## 17         1520 \"you how.\"                                          Much… Shake…\n## 18         1520 \"\"                                                  Much… Shake…\n## 19         1520 \"Leon.\"                                             Much… Shake…\n## 20         1520 \"He hath an uncle here in Messina will be very muc… Much… Shake…\n## 21         1520 \"\"                                                  Much… Shake…\n## 22         1520 \"Mess.\"                                             Much… Shake…\n## 23         1520 \"I have already delivered him letters, and there a… Much… Shake…\n## 24         1520 \"joy in him; even so much that joy could not show … Much… Shake…\n## 25         1520 \"enough without a badge of bitterness.\"             Much… Shake…\nshakes_words <- shakes %>%\n  # Filter out all empty rows\n  filter(text != \"\") %>%\n  # This is a play. The name of each character before they speak \n  filter(str_detect(text,\"[A-Z]{3,}\")==FALSE) %>%\n  # take the column text and divide it by words\n  unnest_tokens(word, text) ## filter: removed 3,088 rows (22%), 11,135 rows remaining## filter: removed 31 rows (<1%), 11,104 rows remaining\n# Bind both word dfs\nwords <- rbind.data.frame(shakes_words,wilde_words)\n\n# Count words etc.\nwords %>%\n  group_by(title,author) %>%\n  slice_sample(n=10000) %>%\n  mutate(word_length = str_length(word),\n         median_word_length = median(word_length)) %>%\n  group_by(word_length,title,author) %>%\n  mutate(total_word_length = n()) %>%\n  distinct(word_length,title,.keep_all=T) %>%\n  select(word_length,title,author,total_word_length,median_word_length) %>%\n  ggplot(aes(y=total_word_length,x=word_length,color=author,group=title)) +\n  geom_point(alpha=0.8) +\n  geom_line(alpha=0.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  # facet_wrap(~author, ncol = 2)+\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(x=\"Length\", y = \"Total Number of Words\", color = \"\", linetype = \"\",\n       caption = \"Note: Median word length is 4 for both authors.\")## group_by: 2 grouping variables (title, author)## slice_sample (grouped): removed 61,480 rows (43%), 80,000 rows remaining## mutate (grouped): new variable 'word_length' (integer) with 17 unique values and 0% NA##                   new variable 'median_word_length' (double) with one unique value and 0% NA## group_by: 3 grouping variables (word_length, title, author)## mutate (grouped): new variable 'total_word_length' (integer) with 102 unique values and 0% NA## distinct (grouped): removed 79,883 rows (>99%), 117 rows remaining## select: dropped 2 variables (gutenberg_id, word)"},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"exercise-optional","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.5 Exercise (Optional)","text":"Extend current analysis authors works author.better ways compare distribution word length? changes across time? differences different types works (e.g., fiction vs. non-fiction, prose vs. poetry)?","code":""},{"path":"week-1-a-primer-on-using-text-as-data.html","id":"final-words","chapter":"1 Week 1: A Primer on Using Text as Data","heading":"1.6 Final Words","text":"often case, won’t able cover every single feature different packages offer, objects create look like, else can . advise go home explore code detail. Try applying code different corpus come next class questions (just show able ).","code":""},{"path":"week-2-tokenization-and-word-frequency.html","id":"week-2-tokenization-and-word-frequency","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2 Week 2: Tokenization and Word Frequency","text":"","code":""},{"path":"week-2-tokenization-and-word-frequency.html","id":"slides-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"Slides","text":"3 Tokenization Word Frequency (link Perusall)","code":""},{"path":"week-2-tokenization-and-word-frequency.html","id":"setup-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.1 Setup","text":"always, first load packages ’ll using:","code":"\nlibrary(tidyverse) # for wrangling data\nlibrary(tidylog) # to know what we are wrangling\nlibrary(tidytext) # for 'tidy' manipulation of text data\nlibrary(quanteda) # tokenization power house\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(wesanderson) # to prettify\nlibrary(readxl) # to read excel\nlibrary(kableExtra) # for displaying data in html format (relevant for formatting this worksheet mainly)"},{"path":"week-2-tokenization-and-word-frequency.html","id":"get-data-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.2 Get Data:","text":"example, using small corpus song lyrics.Ok, different artists, different genres year…lyrics following form:","code":"\nsample_lyrics <- read_excel(\"data/lyrics_sample.xlsx\")\nhead(sample_lyrics)## # A tibble: 6 × 5\n##   artist                   album                      year song           lyrics\n##   <chr>                    <chr>                     <dbl> <chr>          <chr> \n## 1 Rage Against the Machine Evil Empire                1996 Bulls on Para… \"Come…\n## 2 Rage Against the Machine Rage Against the Machine   1992 Killing in th… \"Kill…\n## 3 Rage Against the Machine Renegades                  2000 Renegades of … \"No m…\n## 4 Rage Against the Machine The Battle of Los Angeles  1999 Sleep Now in … \"Yeah…\n## 5 Rage Against the Machine The Battle of Los Angeles  1999 Guerrilla Rad… \"Tran…\n## 6 Rage Against the Machine The Battle of Los Angeles  1999 Testify        \"Uh!\\…## \n##      Megan Thee Stallion Rage Against the Machine         System of a Down \n##                        5                        6                        5 \n##             Taylor Swift \n##                        5## [1] \"Yeah\\r\\n\\r\\nThe world is my expense\\r\\nIt’s the cost of my desire\\r\\nJesus blessed me with its future\\r\\nAnd I protect it with fire\\r\\n\\r\\nSo raise your fists and march around\\r\\nJust don’t take what you need\\r\\nI’ll jail and bury those committed\\r\\nAnd smother the rest in greed\\r\\n\\r\\nCrawl with me into tomorrow\\r\\nOr I’ll drag you to your grave\\r\\nI’m deep inside your children\\r\\nThey’ll betray you in my name\\r\\n\\r\\nHey, hey, sleep now in the fire\\r\\nHey, hey, sleep now in the fire\\r\\n\\r\\nThe lie is my expense\\r\\nThe scope of my desire\\r\\nThe party blessed me with its future\\r\\nAnd I protect it with fire\\r\\n\\r\\nI am the Niña, the Pinta, the Santa María\\r\\nThe noose and the rapist, the fields overseer\\r\\nThe Agents of Orange, the Priests of Hiroshima\\r\\nThe cost of my desire, sleep now in the fire\\r\\n\\r\\nHey, hey, sleep now in the fire\\r\\nHey, hey, sleep now in the fire\\r\\n\\r\\nFor it’s the end of history\\r\\nIt’s caged and frozen still\\r\\nThere is no other pill to take\\r\\nSo swallow the one that made you ill\\r\\n\\r\\nThe Niña, the Pinta, the Santa María\\r\\nThe noose and the rapist, the fields overseer\\r\\nThe Agents of Orange, the Priests of Hiroshima\\r\\nThe cost of my desire to sleep now in the fire\\r\\n\\r\\nYeah\\r\\n\\r\\nSleep now in the fire\\r\\nSleep now in the fire\\r\\nSleep now in the fire\\r\\nSleep now in the fire\""},{"path":"week-2-tokenization-and-word-frequency.html","id":"cleaning-the-text","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.3 Cleaning the Text","text":"Much like music, text comes different forms qualities. Regex workshop, might remember special characters can signal, example, new line (\\n), carriage return (\\r). example, can get rid 3. working text, always check state documents loaded program choice.","code":"\nsample_lyrics <- sample_lyrics %>%\n  mutate(lyrics_clean = str_replace_all(lyrics,\"\\\\n\", \"\\\\.\"),\n         lyrics_clean = str_replace_all(lyrics_clean,\"\\\\r\", \"\\\\.\")) %>%\n  select(-lyrics)## mutate: new variable 'lyrics_clean' (character) with 21 unique values and 0% NA## select: dropped one variable (lyrics)\nsample_lyrics$lyrics_clean[4]## [1] \"Yeah....The world is my expense..It’s the cost of my desire..Jesus blessed me with its future..And I protect it with fire....So raise your fists and march around..Just don’t take what you need..I’ll jail and bury those committed..And smother the rest in greed....Crawl with me into tomorrow..Or I’ll drag you to your grave..I’m deep inside your children..They’ll betray you in my name....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....The lie is my expense..The scope of my desire..The party blessed me with its future..And I protect it with fire....I am the Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire, sleep now in the fire....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....For it’s the end of history..It’s caged and frozen still..There is no other pill to take..So swallow the one that made you ill....The Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire to sleep now in the fire....Yeah....Sleep now in the fire..Sleep now in the fire..Sleep now in the fire..Sleep now in the fire\""},{"path":"week-2-tokenization-and-word-frequency.html","id":"tokenization","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.4 Tokenization","text":"goal create document-feature matrix, later extract information word frequency. , start crating corpus object, quanteda package.Looks good. Now can tokenize corpus (reduce complexity). One benefit creating corpus object first maintain metadata every document tokenize. come handy future.got rid punctuation. Now let’s remove stop words, high low frequency words, stem remaining tokens. cheating, though. know high low frequency words checked dfm (see next code chunk).can compare stemmed output non-stemmed output. ‘future’ become ‘futur’? assuming , purposes, ‘future=futuristic’. researcher decide. finally, can create document-feature matrix (dfm).Note create dfm object, tokens become lowercase. Now can check 15 frequent tokens.tell us much, used previous code check low-information tokens might want remove analysis. can also see many tokens appear :interesting text analysis count words time/space. case, ‘space’ can artist.Interesting. lot overlap (apart token Megan Thee Stallion Rage Machine). However, great measure importance word relative much appears across documents (one way denominate). Enter TF-IDF: “Term-Frequency / Inverse-Document-frequency”. TF-IDF weighting -weights relatively rare words appear documents. Using term frequency inverse document frequency allows us find words characteristic one document within collection documents.building dictionary, example, might want include words high TF-IDF values. Another way think TF-IDF terms predictive power. Words common documents predictive power receive TD-IDF value 0. Words appear, relatively document, greater predictive power receive TD-IDF > 0.Another useful tool (concept) keyness. Keyness two--two association score features occur differentially across different categories. can estimate features associated one category (case, one artist), compared . Let’s compare Megan Thee Stallion Taylor Swift.Similar implied TF-IDF graphs. Notice stemming always works expected. Taylor Swift sings “shake, shake, shake” Megan Thee Stallion sings “shaking”. However, words appear distinct features artists.","code":"\ncorpus_lyrics <- corpus(sample_lyrics,\n                     # docid_field = \"texto_id\",\n                     text_field = \"lyrics_clean\",\n                     unique_docnames = TRUE)\n\nsummary(corpus_lyrics)## Corpus consisting of 21 documents, showing 21 documents:\n## \n##    Text Types Tokens Sentences                   artist\n##   text1   119    375        35 Rage Against the Machine\n##   text2    52    853        83 Rage Against the Machine\n##   text3   188    835        91 Rage Against the Machine\n##   text4    97    352        38 Rage Against the Machine\n##   text5   160    440        50 Rage Against the Machine\n##   text6   133    535        67 Rage Against the Machine\n##   text7   104    559        53         System of a Down\n##   text8    67    365        40         System of a Down\n##   text9    68    298        33         System of a Down\n##  text10    65    258        32         System of a Down\n##  text11   137    558        68         System of a Down\n##  text12   131    876        70             Taylor Swift\n##  text13   159    465        41             Taylor Swift\n##  text14   162    544        62             Taylor Swift\n##  text15   196    738        84             Taylor Swift\n##  text16   169    549        50             Taylor Swift\n##  text17   229    867        55      Megan Thee Stallion\n##  text18   193    664        61      Megan Thee Stallion\n##  text19   310   1190        87      Megan Thee Stallion\n##  text20   198    656        48      Megan Thee Stallion\n##  text21   256   1050        73      Megan Thee Stallion\n##                       album year                  song\n##                 Evil Empire 1996       Bulls on Parade\n##    Rage Against the Machine 1992   Killing in the Name\n##                   Renegades 2000     Renegades of Funk\n##   The Battle of Los Angeles 1999 Sleep Now in the Fire\n##   The Battle of Los Angeles 1999       Guerrilla Radio\n##   The Battle of Los Angeles 1999               Testify\n##                   Mezmerize 2005               B.Y.O.B\n##                    Toxicity 2001            Chop Suey!\n##                    Toxicity 2001               Aerials\n##                    Toxicity 2001               Toxicty\n##                    Toxicity 2001                 Sugar\n##                        1989 2014          Shake it Off\n##                   Midnights 2022             Anti-Hero\n##                    Fearless 2008    You Belong With Me\n##                        1989 2014           Blank Space\n##                    Fearless 2008            Love Story\n##                  Traumazine 2022                Plan B\n##                        Suga 2020                Savage\n##  Something for Thee Hotties 2021             Thot Shit\n##                  Traumazine 2022                   Her\n##                  Traumazine 2022            Ungrateful\nlyrics_toks <- tokens(corpus_lyrics,\n                   remove_numbers = TRUE, # Thinks about this\n                   remove_punct = TRUE, # Remove punctuation!\n                   remove_url = TRUE) # Might be helpful\nlyrics_toks[c(4,14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"The\"     \"world\"   \"is\"      \"my\"      \"expense\" \"It’s\"   \n##  [8] \"the\"     \"cost\"    \"of\"      \"my\"      \"desire\" \n## [ ... and 227 more ]\n## \n## text14 :\n##  [1] \"You're\"     \"on\"         \"the\"        \"phone\"      \"with\"      \n##  [6] \"your\"       \"girlfriend\" \"she's\"      \"upset\"      \"She's\"     \n## [11] \"going\"      \"off\"       \n## [ ... and 385 more ]\nlyrics_toks <- tokens_remove(lyrics_toks,\n                          # you can change or add stopwords depending on the \n                          # language(s) of the documents\n                          c(stopwords(language = \"en\"),\n                            # Now is high frequency... there are many low\n                            # frequency tokens. We will check these later\n                            \"now\"),\n                          padding = F)\n\nlyrics_toks_stem <- tokens_wordstem(lyrics_toks, language = \"en\")\n\nlyrics_toks[c(4,14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"world\"   \"expense\" \"It’s\"    \"cost\"    \"desire\"  \"Jesus\"  \n##  [8] \"blessed\" \"future\"  \"protect\" \"fire\"    \"raise\"  \n## [ ... and 105 more ]\n## \n## text14 :\n##  [1] \"phone\"      \"girlfriend\" \"upset\"      \"going\"      \"something\" \n##  [6] \"said\"       \"Cause\"      \"get\"        \"humor\"      \"like\"      \n## [11] \"room\"       \"typical\"   \n## [ ... and 133 more ]\nlyrics_toks_stem[c(4,14)]## Tokens consisting of 2 documents and 4 docvars.\n## text4 :\n##  [1] \"Yeah\"    \"world\"   \"expens\"  \"It’s\"    \"cost\"    \"desir\"   \"Jesus\"  \n##  [8] \"bless\"   \"futur\"   \"protect\" \"fire\"    \"rais\"   \n## [ ... and 105 more ]\n## \n## text14 :\n##  [1] \"phone\"      \"girlfriend\" \"upset\"      \"go\"         \"someth\"    \n##  [6] \"said\"       \"Caus\"       \"get\"        \"humor\"      \"like\"      \n## [11] \"room\"       \"typic\"     \n## [ ... and 133 more ]\nlyrics_dfm <- dfm(lyrics_toks)\nlyrics_dfm_stem <- dfm(lyrics_toks_stem)\n\nhead(lyrics_dfm_stem)## Document-feature matrix of: 6 documents, 1,165 features (93.12% sparse) and 4 docvars.\n##        features\n## docs    come wit microphon explod shatter mold either drop hit like\n##   text1    4   4         1      1       1    1      1    3   1    1\n##   text2    2   0         0      0       0    0      0    0   0    0\n##   text3    0   0         0      0       0    0      0    0   0    4\n##   text4    0   0         0      0       0    0      0    0   0    0\n##   text5    0   0         0      0       0    0      0    0   0    1\n##   text6    0   4         0      0       0    0      0    0   0    0\n## [ reached max_nfeat ... 1,155 more features ]\nlyrics_dfm_stem %>%\n  textstat_frequency(n=30) %>%\n  ggplot(aes(x = reorder(feature,frequency),y=frequency,fill = (frequency), color = (frequency))) +\n  geom_col(alpha=0.5) +\n  coord_flip() +\n  scale_x_reordered() +\n  scale_color_distiller(palette = \"PuOr\") +\n  scale_fill_distiller(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x=\"\",y=\"Frequency\",color = \"\", fill = \"\") +\n  theme(legend.position=\"none\") \nonly_once <- lyrics_dfm_stem %>%\n  textstat_frequency() %>%\n  filter(frequency == 1)## filter: removed 566 rows (49%), 599 rows remaining\nlength(only_once$feature)## [1] 599\nlyrics_dfm_stem %>%\n  textstat_frequency(n=15, groups = c(artist)) %>%\n  ggplot(aes(x = reorder_within(feature,frequency,group),y=frequency,fill = group, color = group)) +\n  geom_col(alpha = 0.5) +\n  coord_flip() +\n  facet_wrap(~group, scales = \"free\") +\n  scale_x_reordered() +\n  scale_color_brewer(palette = \"PuOr\") +\n  scale_fill_brewer(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x=\"\",y=\"\",color = \"\", fill = \"\") +\n  theme(legend.position=\"none\") \nlyrics_dfm_tfidf <- dfm_tfidf(lyrics_dfm_stem)\n\nlyrics_dfm_tfidf %>%\n  textstat_frequency(n=15, groups = c(artist), force=T) %>%\n  ggplot(aes(x = reorder_within(feature,frequency,group),y=frequency,fill = group, color = group)) +\n  geom_col(alpha = 0.5) +\n  coord_flip() +\n  facet_wrap(~group, scales = \"free\") +\n  scale_x_reordered() +\n  scale_color_brewer(palette = \"PuOr\") +\n  scale_fill_brewer(palette = \"PuOr\") +\n  theme_minimal() + \n  labs(x=\"\",y=\"TF-IDF\",color = \"\", fill = \"\") +\n  theme(legend.position=\"none\") \nlyrics_dfm_ts_mts <- dfm_subset(lyrics_dfm_stem, year > 2006)\n\nlyrics_key <- textstat_keyness(lyrics_dfm_ts_mts, \n                              target = lyrics_dfm_ts_mts$artist == \"Taylor Swift\")\ntextplot_keyness(lyrics_key)"},{"path":"week-2-tokenization-and-word-frequency.html","id":"word-frequency-across-artist","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.5 Word Frequency Across Artist","text":"can something similar last week look word frequencies. Rather creating dfm, can use dataset , get information. example, average number tokens artist.Alternatively, can estimate frequency token song.can now join two data frames together left_join() function join “song” column. can pipe joined data plot.","code":"\nsample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  group_by(song) %>%\n  mutate(total_tk_song = n()) %>%\n  distinct(song,.keep_all=T) %>% \n  group_by(artist) %>%\n  mutate(mean_tokens = mean(total_tk_song)) %>%\n  ggplot(aes(x=song,y=total_tk_song,fill=artist,color=artist)) +\n  geom_col(alpha=.8) +\n  geom_hline(aes(yintercept = mean_tokens, color = artist), linetype = \"dashed\")+\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  scale_fill_manual(values = wes_palette(\"Royal2\")) +\n  facet_wrap(~artist, scales = \"free_x\", nrow = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 90, size = 5,vjust = 0.5, hjust=1)) +\n  labs(x=\"\", y = \"Total Tokens\", color = \"\", fill = \"\",\n       caption = \"Note: Mean token length in dashed line.\")## group_by: one grouping variable (song)## mutate (grouped): new variable 'total_tk_song' (integer) with 20 unique values and 0% NA## distinct (grouped): removed 8,958 rows (>99%), 21 rows remaining## group_by: one grouping variable (artist)## mutate (grouped): new variable 'mean_tokens' (double) with 4 unique values and 0% NA\nlyrics_totals <- sample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  group_by(song) %>%\n  mutate(total_tk_song = n()) %>%\n  distinct(song,.keep_all=T) ## group_by: one grouping variable (song)## mutate (grouped): new variable 'total_tk_song' (integer) with 20 unique values and 0% NA## distinct (grouped): removed 8,958 rows (>99%), 21 rows remaining\n# let's look for \"like\"\nlyrics_like <- sample_lyrics %>%\n  # take the column lyrics_clean and divide it by words\n  # this uses a similar tokenizer to quanteda\n  unnest_tokens(word, lyrics_clean) %>%\n  filter(word==\"like\") %>%\n  group_by(song) %>%\n  mutate(total_like_song = n()) %>%\n  distinct(song,total_like_song) ## filter: removed 8,934 rows (99%), 45 rows remaining## group_by: one grouping variable (song)## mutate (grouped): new variable 'total_like_song' (integer) with 7 unique values and 0% NA## distinct (grouped): removed 33 rows (73%), 12 rows remaining\nlyrics_totals %>%\n  left_join(lyrics_like, by = \"song\") %>%\n  ungroup() %>%\n  mutate(like_prop = total_like_song/total_tk_song) %>%\n  ggplot(aes(x=song,y=like_prop,fill=artist,color=artist)) +\n  geom_col(alpha=.8) +\n  scale_color_manual(values = wes_palette(\"Royal2\")) +\n  scale_fill_manual(values = wes_palette(\"Royal2\")) +\n  facet_wrap(~artist, scales = \"free_x\", nrow = 1) + \n  theme_minimal() +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 90, size = 5,vjust = 0.5, hjust=1)) +\n  labs(x=\"\", y = \"Prop. of \\'Like\\'\", color = \"\", fill = \"\")## left_join: added one column (total_like_song)##            > rows only in x    9##            > rows only in y  ( 0)##            > matched rows     12##            >                 ====##            > rows total       21## ungroup: no grouping variables## mutate: new variable 'like_prop' (double) with 13 unique values and 43% NA"},{"path":"week-2-tokenization-and-word-frequency.html","id":"final-words-1","chapter":"2 Week 2: Tokenization and Word Frequency","heading":"2.6 Final Words","text":"often case, won’t able cover every single feature different packages offer, objects create look like, else can . advise go home explore code detail. Try applying code different corpus come next class questions (just show able ).","code":""},{"path":"parts.html","id":"parts","chapter":"3 Parts","heading":"3 Parts","text":"can add parts organize one book chapters together. Parts can inserted top .Rmd file, first-level chapter heading file.Add numbered part: # (PART) Act one {-} (followed # chapter)Add unnumbered part: # (PART\\*) Act one {-} (followed # chapter)Add appendix special kind un-numbered part: # (APPENDIX) stuff {-} (followed # chapter). Chapters appendix prepended letters instead numbers.","code":""},{"path":"footnotes-and-citations.html","id":"footnotes-and-citations","chapter":"4 Footnotes and citations","heading":"4 Footnotes and citations","text":"","code":""},{"path":"footnotes-and-citations.html","id":"footnotes","chapter":"4 Footnotes and citations","heading":"4.1 Footnotes","text":"Footnotes put inside square brackets caret ^[]. Like one 4.","code":""},{"path":"footnotes-and-citations.html","id":"citations","chapter":"4 Footnotes and citations","heading":"4.2 Citations","text":"Reference items bibliography file(s) using @key.example, using bookdown package (Xie 2023) (check last code chunk index.Rmd see citation key added) sample book, built top R Markdown knitr (Xie 2015) (citation added manually external file book.bib).\nNote .bib files need listed index.Rmd YAML bibliography key.RStudio Visual Markdown Editor can also make easier insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations","code":""},{"path":"blocks.html","id":"blocks","chapter":"5 Blocks","heading":"5 Blocks","text":"","code":""},{"path":"blocks.html","id":"equations","chapter":"5 Blocks","heading":"5.1 Equations","text":"equation.\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  \\tag{5.1}\n\\end{equation}\\]may refer using \\@ref(eq:binom), like see Equation (5.1).","code":""},{"path":"blocks.html","id":"theorems-and-proofs","chapter":"5 Blocks","heading":"5.2 Theorems and proofs","text":"Labeled theorems can referenced text using \\@ref(thm:tri), example, check smart theorem 5.1.Theorem 5.1  right triangle, \\(c\\) denotes length hypotenuse\n\\(\\) \\(b\\) denote lengths two sides, \n\\[^2 + b^2 = c^2\\]Read https://bookdown.org/yihui/bookdown/markdown-extensions--bookdown.html.","code":""},{"path":"blocks.html","id":"callout-blocks","chapter":"5 Blocks","heading":"5.3 Callout blocks","text":"R Markdown Cookbook provides help use custom blocks design callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html","code":""},{"path":"sharing-your-book.html","id":"sharing-your-book","chapter":"6 Sharing your book","heading":"6 Sharing your book","text":"","code":""},{"path":"sharing-your-book.html","id":"publishing","chapter":"6 Sharing your book","heading":"6.1 Publishing","text":"HTML books can published online, see: https://bookdown.org/yihui/bookdown/publishing.html","code":""},{"path":"sharing-your-book.html","id":"pages","chapter":"6 Sharing your book","heading":"6.2 404 pages","text":"default, users directed 404 page try access webpage found. ’d like customize 404 page instead using default, may add either _404.Rmd _404.md file project root use code /Markdown syntax.","code":""},{"path":"sharing-your-book.html","id":"metadata-for-sharing","chapter":"6 Sharing your book","heading":"6.3 Metadata for sharing","text":"Bookdown HTML books provide HTML metadata social sharing platforms like Twitter, Facebook, LinkedIn, using information provide index.Rmd YAML. setup, set url book path cover-image file. book’s title description also used.gitbook uses social sharing data across chapters book- links shared look .Specify book’s source repository GitHub using edit key configuration options _output.yml file, allows users suggest edit linking chapter’s source file.Read features output format :https://pkgs.rstudio.com/bookdown/reference/gitbook.htmlOr use:","code":"\n?bookdown::gitbook"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
