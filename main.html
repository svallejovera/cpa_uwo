<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>PS9594A: Computational Text Analysis</title>

    <meta name="author" content="Dr. Sebastián Vallejo Vera | Western University" />
  
   <meta name="description" content="Code, exercises, and slides for PS9594A: Computational Text Analysis" />
   <meta name="generator" content="placeholder" />
  <meta property="og:title" content="PS9594A: Computational Text Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Code, exercises, and slides for PS9594A: Computational Text Analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="PS9594A: Computational Text Analysis" />
  
  <meta name="twitter:description" content="Code, exercises, and slides for PS9594A: Computational Text Analysis" />
  
  <!-- JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script>
    <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet" />
    <script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script>
    <script src="libs/bs3compat-0.5.0/transition.js"></script>
    <script src="libs/bs3compat-0.5.0/tabs.js"></script>
    <script src="libs/bs3compat-0.5.0/bs3compat.js"></script>
    <link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet" />
    <script src="libs/bs4_book-1.0.0/bs4_book.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script>

  <!-- CSS -->
  <style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
    
</head>

<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<!--bookdown:title:start-->
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book">
    <a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">PS9594A: Computational Text Analysis</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
      </form>

      <nav aria-label="Table of contents">
        <h2>Table of contents</h2>
        <div id="book-toc"></div>

        <div class="book-extra">
          <p><a id="book-repo" href="#">View book source <i class="fab fa-github"></i></a></li></p>
        </div>
      </nav>
    </div>
  </header>

  <main class="col-sm-12 col-md-9 col-lg-7" id="content">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="computational-text-analysis" class="section level1 unnumbered">
<h1 class="unnumbered">“Computational Text Analysis”</h1>
<p> </p>
<p>Welcome to the site for the course PS9594A: “Computational Text Analysis” at Western University, taught by Sebastián Vallejo Vera. In each week, you will find the code, exercises, and slides for the corresponding topic.</p>
<p>Before you start, check the required software and packages below. Also, don’t forget to read the <a href="https://svallejovera.github.io/files/9594A__Computational_Text_Analysis.pdf">Syllabus</a> and check Perusall for the readings for the course. This site will be corrected/updated throughout the semester.</p>
<div id="software-and-packages" class="section level2" number="0.1">
<h2><span class="header-section-number">0.1</span> Software and Packages</h2>
<p>For the first part of this course (Weeks 1 - 5), we will be mainly using R. For the second part of the this course (Weeks 6 - 11), we will use a combination of R and Python. I will assume that you are familiar with R language, RStudio, and R packages. If you are not, please come to office hours and I can help you out<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. In R, these are the main packages you will need to have installed:</p>
<ul>
<li>tidyverse (we will be piping)</li>
<li>tidylog (helps keep track of what your are pipins)</li>
<li>tidytext (great for working with text)</li>
<li>quanteda (stands for “Quantitative Analysis of Textual Data”)
<ul>
<li>quanteda.textstats (to obtain stats from our dfm)</li>
<li>quanteda.textplots (to obtain plots from our dfm stats)</li>
<li>quanteda.dictionaries (to use dictionaries with quanteda)</li>
</ul></li>
<li>gutenbergr (to download texts from <a href="https://www.gutenberg.org/">Project Gutenberg</a>)</li>
<li>wesanderson (to make things pretty)</li>
<li>stm (to run Structural Topic Models)</li>
</ul>
</div>
<div id="datasets" class="section level2" number="0.2">
<h2><span class="header-section-number">0.2</span> Datasets</h2>
<p>Throughout the class, we will be using a number of sample datasets. Access to these datasets will be provided directly on the code. For your Final Essay, you can use one of the following datasets (or, even better, you can use your own):</p>
<ul>
<li>Data from Ventura et al. (2021) - “Connective Effervescence and Streaming Chat During Political Debates”: <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/EWEJZN">Link to replication material</a></li>
<li>Data from Project Gutenberg: <code>gutenbergr</code></li>
<li>Open-ended question from the 2021 Canadian Election Study survey: <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/XBZHKC">Link to replication material</a></li>
<li>Open-ended questions from ANES surveys: <a href="https://electionstudies.org/">Link to ANES homepage</a></li>
</ul>
</div>
<div id="acknowledgments" class="section level2" number="0.3">
<h2><span class="header-section-number">0.3</span> Acknowledgments</h2>
<p>The organization of the first part of this course (Weeks 1 - 5) and the format of the assignments are borrowed from Christopher Barrie’s excellent course on <a href="https://cjbarrie.github.io/CTA-ED/">“Computational Text Analysis”</a>, a syllabus from the prolific <a href="https://www.venturatiago.com/">Tiago Ventura</a>, and Grimmer, Roberts, and Stewart’s excellent book, <a href="https://press.princeton.edu/books/paperback/9780691207551/text-as-data">“Text as data: A new framework for machine learning and the social sciences”</a>. The code used throughout the course is a patchwork of my own code, but my own code borrows heavily from the internet (but that’s true for all code). I try my best to give credit to the original authors of the code (when and if possible).</p>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="week-1-a-primer-on-using-text-as-data" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Week 1: A Primer on Using Text as Data</h1>
<div id="slides" class="section level2 unnumbered">
<h2 class="unnumbered">Slides</h2>
<ul>
<li>1 Introduction to CTA (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/1%20Introduction%20to%20CTA.pptx">link</a> or in Perusall)</li>
<li>2 Why Computational Text Analysis (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/2%20Why%20Computational%20Text%20Analysis.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Setup</h2>
<p>For this first example, we will replicate (and extend) Mendenhall’s (1887) and Mendenhall’s (1901) studies of word-length distribution.</p>
<div class="figure" style="text-align: center">
<img src="images/curve.png" alt="From Mendenhall (1987) - The Characteristic Curves of Composition." width="65%" />
<p class="caption">
(#fig:curve)From Mendenhall (1987) - The Characteristic Curves of Composition.
</p>
</div>
<p>First we load the packages that we’ll be using:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># for wrangling data</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidylog) <span class="co"># to know what we are wrangling</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(tidytext) <span class="co"># for &#39;tidy&#39; manipulation of text data</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(wesanderson) <span class="co"># to prettify</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="fu">library</span>(gutenbergr) <span class="co"># to get some books</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="fu">library</span>(kableExtra) <span class="co"># for displaying data in html format (relevant for formatting this worksheet mainly)</span></span></code></pre></div>
</div>
<div id="get-data" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Get Data</h2>
<p>Mendenhall (1887) argued that “every writer makes use of a vocabulary which is peculiar to himself, and the character of which does not materially change from year to
year during his productive,” and that one of these characteristics was the length of words. Mendenhall (1901) takes this further, and suggests that, given this assumption, Shakespeare and Bacon were <em>not</em> the same person<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>Let’s get a corpus–a collection of documents–that we can analyze. We can search the Gutenberg repository and create a corpus with some selected work.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>gutenberg_metadata <span class="sc">%&gt;%</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>  <span class="fu">filter</span>(author <span class="sc">==</span> <span class="st">&quot;Wilde, Oscar&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 66 × 8
##    gutenberg_id title    author gutenberg_author_id language gutenberg_bookshelf
##           &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;              
##  1          174 The Pic… Wilde…                 111 en       &quot;Gothic Fiction/Mo…
##  2          301 The Bal… Wilde…                 111 en       &quot;&quot;                 
##  3          773 Lord Ar… Wilde…                 111 en       &quot;Contemporary Revi…
##  4          774 Essays … Wilde…                 111 en       &quot;&quot;                 
##  5          790 Lady Wi… Wilde…                 111 en       &quot;&quot;                 
##  6          844 The Imp… Wilde…                 111 en       &quot;Plays&quot;            
##  7          854 A Woman… Wilde…                 111 en       &quot;Plays&quot;            
##  8          873 A House… Wilde…                 111 en       &quot;Opera&quot;            
##  9          875 The Duc… Wilde…                 111 en       &quot;&quot;                 
## 10          885 An Idea… Wilde…                 111 en       &quot;Plays&quot;            
## # ℹ 56 more rows
## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt;</code></pre>
</div>
<div id="word-length-in-wildes-corpus" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Word Length in Wilde’s Corpus</h2>
<p>That’s a lot of Wilde! Let’s focus on four plays: “The Importance of Being Earnest”, “A Woman of No Importance”, “Lady Windermere’s Fan”, and “An Ideal Husband”. We can download all of these plays using their ID number:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>wilde <span class="ot">&lt;-</span> <span class="fu">gutenberg_download</span>(<span class="fu">c</span>(<span class="dv">790</span>,<span class="dv">844</span>, <span class="dv">854</span>, <span class="dv">885</span>), </span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>                            <span class="at">meta_fields =</span> <span class="fu">c</span>(<span class="st">&quot;title&quot;</span>,<span class="st">&quot;author&quot;</span>))</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="fu">print</span>(<span class="at">n=</span><span class="dv">25</span>,wilde[<span class="fu">c</span>(<span class="dv">51</span><span class="sc">:</span><span class="dv">75</span>),])</span></code></pre></div>
<pre><code>## # A tibble: 25 × 4
##    gutenberg_id text                                        title         author
##           &lt;int&gt; &lt;chr&gt;                                       &lt;chr&gt;         &lt;chr&gt; 
##  1          790 &quot;&quot;                                          Lady Winderm… Wilde…
##  2          790 &quot;&quot;                                          Lady Winderm… Wilde…
##  3          790 &quot;THE PERSONS OF THE PLAY&quot;                   Lady Winderm… Wilde…
##  4          790 &quot;&quot;                                          Lady Winderm… Wilde…
##  5          790 &quot;&quot;                                          Lady Winderm… Wilde…
##  6          790 &quot;Lord Windermere&quot;                           Lady Winderm… Wilde…
##  7          790 &quot;&quot;                                          Lady Winderm… Wilde…
##  8          790 &quot;Lord Darlington&quot;                           Lady Winderm… Wilde…
##  9          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 10          790 &quot;Lord Augustus Lorton&quot;                      Lady Winderm… Wilde…
## 11          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 12          790 &quot;Mr. Dumby&quot;                                 Lady Winderm… Wilde…
## 13          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 14          790 &quot;Mr. Cecil Graham&quot;                          Lady Winderm… Wilde…
## 15          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 16          790 &quot;Mr. Hopper&quot;                                Lady Winderm… Wilde…
## 17          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 18          790 &quot;Parker, Butler&quot;                            Lady Winderm… Wilde…
## 19          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 20          790 &quot;                                * * * * *&quot; Lady Winderm… Wilde…
## 21          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 22          790 &quot;Lady Windermere&quot;                           Lady Winderm… Wilde…
## 23          790 &quot;&quot;                                          Lady Winderm… Wilde…
## 24          790 &quot;The Duchess of Berwick&quot;                    Lady Winderm… Wilde…
## 25          790 &quot;&quot;                                          Lady Winderm… Wilde…</code></pre>
<p>The unit of analysis is something like a line. We are interested in each word—also known as token—and their lengths <em>in each play</em>. We will clean some of the unwanted text—text that will only add noise to our analysis—and then count the number of words.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>wilde <span class="ot">&lt;-</span> wilde <span class="sc">%&gt;%</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>  <span class="co"># Some housekeeping </span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">title =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(title,<span class="st">&quot;Importance of Being&quot;</span>),<span class="st">&quot;The Importance of Being Earnest&quot;</span>, title)) <span class="sc">%&gt;%</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  <span class="co"># Filter out all empty rows</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>  <span class="fu">filter</span>(text <span class="sc">!=</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>  <span class="co"># This is a play. The name of each character before they speak </span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(text,<span class="st">&quot;[A-Z]{3,}&quot;</span>)<span class="sc">==</span><span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## mutate: changed 3,884 values (27%) of &#39;title&#39; (0 new NA)</code></pre>
<pre><code>## filter: removed 4,232 rows (29%), 10,303 rows remaining</code></pre>
<pre><code>## filter: removed 4,208 rows (41%), 6,095 rows remaining</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">print</span>(<span class="at">n=</span><span class="dv">25</span>,wilde[<span class="fu">c</span>(<span class="dv">51</span><span class="sc">:</span><span class="dv">75</span>),])</span></code></pre></div>
<pre><code>## # A tibble: 25 × 4
##    gutenberg_id text                                                title author
##           &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt; &lt;chr&gt; 
##  1          790 &quot;tea-table L._  _Window opening on to terrace L._ … Lady… Wilde…
##  2          790 &quot;home to any one who calls.&quot;                        Lady… Wilde…
##  3          790 &quot;                                                 … Lady… Wilde…
##  4          790 &quot;he’s come.&quot;                                        Lady… Wilde…
##  5          790 &quot;hands with you.  My hands are all wet with these … Lady… Wilde…
##  6          790 &quot;lovely?  They came up from Selby this morning.&quot;    Lady… Wilde…
##  7          790 &quot;table_.]  And what a wonderful fan!  May I look a… Lady… Wilde…
##  8          790 &quot;everything.  I have only just seen it myself.  It… Lady… Wilde…
##  9          790 &quot;present to me.  You know to-day is my birthday?&quot;   Lady… Wilde…
## 10          790 &quot;life, isn’t it?  That is why I am giving this par… Lady… Wilde…
## 11          790 &quot;down.  [_Still arranging flowers_.]&quot;               Lady… Wilde…
## 12          790 &quot;birthday, Lady Windermere.  I would have covered … Lady… Wilde…
## 13          790 &quot;front of your house with flowers for you to walk … Lady… Wilde…
## 14          790 &quot;you.&quot;                                              Lady… Wilde…
## 15          790 &quot;                                                 … Lady… Wilde…
## 16          790 &quot;Foreign Office.  I am afraid you are going to ann… Lady… Wilde…
## 17          790 &quot;with her pocket-handkerchief_, _goes to tea-table… Lady… Wilde…
## 18          790 &quot;Won’t you come over, Lord Darlington?&quot;             Lady… Wilde…
## 19          790 &quot;miserable, Lady Windermere.  You must tell me wha… Lady… Wilde…
## 20          790 &quot;table L._]&quot;                                        Lady… Wilde…
## 21          790 &quot;whole evening.&quot;                                    Lady… Wilde…
## 22          790 &quot;that the only pleasant things to pay _are_ compli… Lady… Wilde…
## 23          790 &quot;things we _can_ pay.&quot;                              Lady… Wilde…
## 24          790 &quot;You mustn’t laugh, I am quite serious.  I don’t l… Lady… Wilde…
## 25          790 &quot;don’t see why a man should think he is pleasing a… Lady… Wilde…</code></pre>
<p>Now, we can change our unit of analysis to the <strong>token</strong>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>wilde_words <span class="ot">&lt;-</span> wilde <span class="sc">%&gt;%</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>  <span class="co"># take the column text and divide it by words</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text) <span class="sc">%&gt;%</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>  <span class="co"># Remove some characters that add noise</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">word =</span> <span class="fu">str_remove_all</span>(word, <span class="st">&quot;</span><span class="sc">\\</span><span class="st">_&quot;</span>)) </span></code></pre></div>
<pre><code>## mutate: changed 1,225 values (2%) of &#39;word&#39; (0 new NA)</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>wilde_words</span></code></pre></div>
<pre><code>## # A tibble: 60,465 × 4
##    gutenberg_id title                 author       word     
##           &lt;int&gt; &lt;chr&gt;                 &lt;chr&gt;        &lt;chr&gt;    
##  1          790 Lady Windermere&#39;s Fan Wilde, Oscar by       
##  2          790 Lady Windermere&#39;s Fan Wilde, Oscar sixteenth
##  3          790 Lady Windermere&#39;s Fan Wilde, Oscar edition  
##  4          790 Lady Windermere&#39;s Fan Wilde, Oscar first    
##  5          790 Lady Windermere&#39;s Fan Wilde, Oscar published
##  6          790 Lady Windermere&#39;s Fan Wilde, Oscar 1893     
##  7          790 Lady Windermere&#39;s Fan Wilde, Oscar first    
##  8          790 Lady Windermere&#39;s Fan Wilde, Oscar issued   
##  9          790 Lady Windermere&#39;s Fan Wilde, Oscar by       
## 10          790 Lady Windermere&#39;s Fan Wilde, Oscar methuen  
## # ℹ 60,455 more rows</code></pre>
<p>That’s a lot of words! We will now create a column for word length, and then count the number of words by length (by play!).</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>wilde_words_ct <span class="ot">&lt;-</span> wilde_words <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>  <span class="co"># Length of each word</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">word_length =</span> <span class="fu">str_length</span>(word)) <span class="sc">%&gt;%</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>  <span class="co"># Group by word_length and count how many </span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>  <span class="fu">group_by</span>(word_length,title) <span class="sc">%&gt;%</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_word_length =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>  <span class="co"># Keep relevant</span></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>  <span class="fu">distinct</span>(word_length,title,<span class="at">.keep_all=</span>T) <span class="sc">%&gt;%</span></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>  <span class="fu">select</span>(word_length,title,author,total_word_length)</span></code></pre></div>
<pre><code>## mutate: new variable &#39;word_length&#39; (integer) with 17 unique values and 0% NA</code></pre>
<pre><code>## group_by: 2 grouping variables (word_length, title)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_word_length&#39; (integer) with 58 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 60,403 rows (&gt;99%), 62 rows remaining</code></pre>
<pre><code>## select: dropped 2 variables (gutenberg_id, word)</code></pre>
<p>Let’s see the distribution by play:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>wilde_words_ct <span class="sc">%&gt;%</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y=</span>total_word_length,<span class="at">x=</span>word_length,<span class="at">color=</span>title)) <span class="sc">+</span></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;right&quot;</span>) <span class="sc">+</span></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Length&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Total Number of Words&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>This is a problem. <em>Why?</em></p>
<p>Here is a solution (proposed by Mendenhall):</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>wilde_words <span class="sc">%&gt;%</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>  <span class="fu">group_by</span>(title) <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>  <span class="fu">slice_sample</span>(<span class="at">n=</span><span class="dv">10000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">word_length =</span> <span class="fu">str_length</span>(word),</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>         <span class="at">median_word_length =</span> <span class="fu">median</span>(word_length)) <span class="sc">%&gt;%</span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>  <span class="fu">group_by</span>(word_length,title) <span class="sc">%&gt;%</span></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_word_length =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>  <span class="fu">distinct</span>(word_length,title,<span class="at">.keep_all=</span>T) <span class="sc">%&gt;%</span></span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a>  <span class="fu">select</span>(word_length,title,author,total_word_length,median_word_length) <span class="sc">%&gt;%</span></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y=</span>total_word_length,<span class="at">x=</span>word_length,<span class="at">color=</span>title)) <span class="sc">+</span></span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> median_word_length,<span class="at">color=</span>title,<span class="at">linetype =</span> title))<span class="sc">+</span></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;right&quot;</span>) <span class="sc">+</span></span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Length&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Total Number of Words&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Line type shows median word length.&quot;</span>)</span></code></pre></div>
<pre><code>## group_by: one grouping variable (title)</code></pre>
<pre><code>## slice_sample (grouped): removed 20,465 rows (34%), 40,000 rows remaining</code></pre>
<pre><code>## mutate (grouped): new variable &#39;word_length&#39; (integer) with 17 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;median_word_length&#39; (double) with one unique value and 0% NA</code></pre>
<pre><code>## group_by: 2 grouping variables (word_length, title)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_word_length&#39; (integer) with 56 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 39,940 rows (&gt;99%), 60 rows remaining</code></pre>
<pre><code>## select: dropped 2 variables (gutenberg_id, word)</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Would you look at that. Mendenhall was into something: an author has a mark in terms of word length distribution. For Wilde, there is no observable change across time (each play was published in different years). But, what happens when we compare Wilde’s mark with Shakespeare’s? Let’s choose four plays (at random) by Shakespeare: A Midsummer Night’s Dream, The Merchant of Venice, Much Ado about Nothing, and The Tempest.</p>
</div>
<div id="comparing-shakespeare-and-wilde" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Comparing Shakespeare and Wilde</h2>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a>shakes <span class="ot">&lt;-</span> <span class="fu">gutenberg_download</span>(<span class="fu">c</span>(<span class="dv">1520</span>,<span class="dv">2242</span>,<span class="dv">2243</span>,<span class="dv">2235</span>),</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>                             <span class="at">meta_fields =</span> <span class="fu">c</span>(<span class="st">&quot;title&quot;</span>,<span class="st">&quot;author&quot;</span>))</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a><span class="fu">print</span>(<span class="at">n=</span><span class="dv">25</span>,shakes[<span class="fu">c</span>(<span class="dv">51</span><span class="sc">:</span><span class="dv">75</span>),])</span></code></pre></div>
<pre><code>## # A tibble: 25 × 4
##    gutenberg_id text                                                title author
##           &lt;int&gt; &lt;chr&gt;                                               &lt;chr&gt; &lt;chr&gt; 
##  1         1520 &quot;Leon.&quot;                                             Much… Shake…
##  2         1520 &quot;How many gentlemen have you lost in this action?&quot;  Much… Shake…
##  3         1520 &quot;&quot;                                                  Much… Shake…
##  4         1520 &quot;Mess.&quot;                                             Much… Shake…
##  5         1520 &quot;But few of any sort, and none of name.&quot;            Much… Shake…
##  6         1520 &quot;&quot;                                                  Much… Shake…
##  7         1520 &quot;Leon.&quot;                                             Much… Shake…
##  8         1520 &quot;A victory is twice itself when the achiever bring… Much… Shake…
##  9         1520 &quot;numbers.  I find here that Don Pedro hath bestowe… Much… Shake…
## 10         1520 &quot;a young Florentine, called Claudio.&quot;               Much… Shake…
## 11         1520 &quot;&quot;                                                  Much… Shake…
## 12         1520 &quot;Mess.&quot;                                             Much… Shake…
## 13         1520 &quot;Much deserved on his part, and equally remembered… Much… Shake…
## 14         1520 &quot;He hath borne himself beyond the promise of his a… Much… Shake…
## 15         1520 &quot;in the figure of a lamb, the feats of a lion: he … Much… Shake…
## 16         1520 &quot;better bettered expectation than you must expect … Much… Shake…
## 17         1520 &quot;you how.&quot;                                          Much… Shake…
## 18         1520 &quot;&quot;                                                  Much… Shake…
## 19         1520 &quot;Leon.&quot;                                             Much… Shake…
## 20         1520 &quot;He hath an uncle here in Messina will be very muc… Much… Shake…
## 21         1520 &quot;&quot;                                                  Much… Shake…
## 22         1520 &quot;Mess.&quot;                                             Much… Shake…
## 23         1520 &quot;I have already delivered him letters, and there a… Much… Shake…
## 24         1520 &quot;joy in him; even so much that joy could not show … Much… Shake…
## 25         1520 &quot;enough without a badge of bitterness.&quot;             Much… Shake…</code></pre>
<p>This text is cleaner than Wilde’s corpus, so we will leave it as is. Also, it is harder to systematically remove the name of the person speaking. <strong>Is this a problem? Why? Why not?</strong></p>
<p>We can put together both corpora and see differences in the distributions of word length.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>shakes_words <span class="ot">&lt;-</span> shakes <span class="sc">%&gt;%</span></span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>  <span class="co"># Filter out all empty rows</span></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>  <span class="fu">filter</span>(text <span class="sc">!=</span> <span class="st">&quot;&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a>  <span class="co"># This is a play. The name of each character before they speak </span></span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(text,<span class="st">&quot;[A-Z]{3,}&quot;</span>)<span class="sc">==</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>  <span class="co"># take the column text and divide it by words</span></span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, text) </span></code></pre></div>
<pre><code>## filter: removed 3,088 rows (22%), 11,135 rows remaining</code></pre>
<pre><code>## filter: removed 31 rows (&lt;1%), 11,104 rows remaining</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="co"># Bind both word dfs</span></span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a>words <span class="ot">&lt;-</span> <span class="fu">rbind.data.frame</span>(shakes_words,wilde_words)</span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a><span class="co"># Count words etc.</span></span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a>words <span class="sc">%&gt;%</span></span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a>  <span class="fu">group_by</span>(title,author) <span class="sc">%&gt;%</span></span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a>  <span class="fu">slice_sample</span>(<span class="at">n=</span><span class="dv">10000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">word_length =</span> <span class="fu">str_length</span>(word),</span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a>         <span class="at">median_word_length =</span> <span class="fu">median</span>(word_length)) <span class="sc">%&gt;%</span></span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>  <span class="fu">group_by</span>(word_length,title,author) <span class="sc">%&gt;%</span></span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_word_length =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a>  <span class="fu">distinct</span>(word_length,title,<span class="at">.keep_all=</span>T) <span class="sc">%&gt;%</span></span>
<span id="cb37-13"><a href="#cb37-13" tabindex="-1"></a>  <span class="fu">select</span>(word_length,title,author,total_word_length,median_word_length) <span class="sc">%&gt;%</span></span>
<span id="cb37-14"><a href="#cb37-14" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y=</span>total_word_length,<span class="at">x=</span>word_length,<span class="at">color=</span>author,<span class="at">group=</span>title)) <span class="sc">+</span></span>
<span id="cb37-15"><a href="#cb37-15" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha=</span><span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb37-16"><a href="#cb37-16" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">alpha=</span><span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb37-17"><a href="#cb37-17" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb37-18"><a href="#cb37-18" tabindex="-1"></a>  <span class="co"># facet_wrap(~author, ncol = 2)+</span></span>
<span id="cb37-19"><a href="#cb37-19" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb37-20"><a href="#cb37-20" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb37-21"><a href="#cb37-21" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Length&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Total Number of Words&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb37-22"><a href="#cb37-22" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Median word length is 4 for both authors.&quot;</span>)</span></code></pre></div>
<pre><code>## group_by: 2 grouping variables (title, author)</code></pre>
<pre><code>## slice_sample (grouped): removed 61,480 rows (43%), 80,000 rows remaining</code></pre>
<pre><code>## mutate (grouped): new variable &#39;word_length&#39; (integer) with 17 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;median_word_length&#39; (double) with one unique value and 0% NA</code></pre>
<pre><code>## group_by: 3 grouping variables (word_length, title, author)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_word_length&#39; (integer) with 102 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 79,884 rows (&gt;99%), 116 rows remaining</code></pre>
<pre><code>## select: dropped 2 variables (gutenberg_id, word)</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Are there any differences? What can we conclude from the evidence? What are the limitations of this approach? Are there alternative approaches to study what Mendenhall was getting at?</p>
</div>
<div id="exercise-optional" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Exercise (Optional)</h2>
<ol style="list-style-type: decimal">
<li>Extend the current analysis to other authors or to more works from the same author.</li>
<li>Are there better ways to compare the distribution of word length? Are there changes across time? Are there differences between different types of works (e.g., fiction vs. non-fiction, prose vs. poetry)?</li>
</ol>
</div>
<div id="final-words" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Final Words</h2>
<p>As will be often the case, we won’t be able to cover every single feature that the different packages have to offer, or how some objects that we create look like, or what else we can do with them. My advise is that you go home and explore the code in detail. Try applying the code to a different corpus and come next class with questions (or just show off what you were able to do).</p>
<!--chapter:end:01-week1.Rmd-->
</div>
</div>
<div id="week-2-tokenization-and-word-frequency" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Week 2: Tokenization and Word Frequency</h1>
<div id="slides-1" class="section level2 unnumbered">
<h2 class="unnumbered">Slides</h2>
<ul>
<li>3 Tokenization and Word Frequency (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/3%20Tokenization%20and%20Word%20Frequency.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup-1" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Setup</h2>
<p>As always, we first load the packages that we’ll be using:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># for wrangling data</span></span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a><span class="fu">library</span>(tidylog) <span class="co"># to know what we are wrangling</span></span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a><span class="fu">library</span>(tidytext) <span class="co"># for &#39;tidy&#39; manipulation of text data</span></span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a><span class="fu">library</span>(quanteda) <span class="co"># tokenization power house</span></span>
<span id="cb46-5"><a href="#cb46-5" tabindex="-1"></a><span class="fu">library</span>(quanteda.textstats)</span>
<span id="cb46-6"><a href="#cb46-6" tabindex="-1"></a><span class="fu">library</span>(quanteda.textplots)</span>
<span id="cb46-7"><a href="#cb46-7" tabindex="-1"></a><span class="fu">library</span>(wesanderson) <span class="co"># to prettify</span></span>
<span id="cb46-8"><a href="#cb46-8" tabindex="-1"></a><span class="fu">library</span>(readxl) <span class="co"># to read excel</span></span>
<span id="cb46-9"><a href="#cb46-9" tabindex="-1"></a><span class="fu">library</span>(kableExtra) <span class="co"># for displaying data in html format (relevant for formatting this worksheet mainly)</span></span></code></pre></div>
</div>
<div id="get-data-1" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Get Data:</h2>
<p>For this example, we will be using small corpus of song lyrics.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a>sample_lyrics <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;data/lyrics_sample.xlsx&quot;</span>)</span>
<span id="cb47-2"><a href="#cb47-2" tabindex="-1"></a><span class="fu">head</span>(sample_lyrics)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 5
##   artist                   album                      year song           lyrics
##   &lt;chr&gt;                    &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt; 
## 1 Rage Against the Machine Evil Empire                1996 Bulls on Para… &quot;Come…
## 2 Rage Against the Machine Rage Against the Machine   1992 Killing in th… &quot;Kill…
## 3 Rage Against the Machine Renegades                  2000 Renegades of … &quot;No m…
## 4 Rage Against the Machine The Battle of Los Angeles  1999 Sleep Now in … &quot;Yeah…
## 5 Rage Against the Machine The Battle of Los Angeles  1999 Guerrilla Rad… &quot;Tran…
## 6 Rage Against the Machine The Battle of Los Angeles  1999 Testify        &quot;Uh!\…</code></pre>
<p>Ok, so we have different artists, from different genres and year…</p>
<pre><code>## 
##      Megan Thee Stallion Rage Against the Machine         System of a Down 
##                        5                        6                        5 
##             Taylor Swift 
##                        5</code></pre>
<p>And we have the lyrics in the following form:</p>
<pre><code>## [1] &quot;Yeah\r\n\r\nThe world is my expense\r\nIt’s the cost of my desire\r\nJesus blessed me with its future\r\nAnd I protect it with fire\r\n\r\nSo raise your fists and march around\r\nJust don’t take what you need\r\nI’ll jail and bury those committed\r\nAnd smother the rest in greed\r\n\r\nCrawl with me into tomorrow\r\nOr I’ll drag you to your grave\r\nI’m deep inside your children\r\nThey’ll betray you in my name\r\n\r\nHey, hey, sleep now in the fire\r\nHey, hey, sleep now in the fire\r\n\r\nThe lie is my expense\r\nThe scope of my desire\r\nThe party blessed me with its future\r\nAnd I protect it with fire\r\n\r\nI am the Niña, the Pinta, the Santa María\r\nThe noose and the rapist, the fields overseer\r\nThe Agents of Orange, the Priests of Hiroshima\r\nThe cost of my desire, sleep now in the fire\r\n\r\nHey, hey, sleep now in the fire\r\nHey, hey, sleep now in the fire\r\n\r\nFor it’s the end of history\r\nIt’s caged and frozen still\r\nThere is no other pill to take\r\nSo swallow the one that made you ill\r\n\r\nThe Niña, the Pinta, the Santa María\r\nThe noose and the rapist, the fields overseer\r\nThe Agents of Orange, the Priests of Hiroshima\r\nThe cost of my desire to sleep now in the fire\r\n\r\nYeah\r\n\r\nSleep now in the fire\r\nSleep now in the fire\r\nSleep now in the fire\r\nSleep now in the fire&quot;</code></pre>
</div>
<div id="cleaning-the-text" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Cleaning the Text</h2>
<p>Much like music, text comes in different forms and qualities. From the Regex workshop, you might remember that there are special characters that can signal, for example, a new line (<code>\n</code>), or carriage return (<code>\r</code>). For this example, we can get rid of them <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Before working with text, always check the state of your documents once loaded into your program of choice.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" tabindex="-1"></a>sample_lyrics <span class="ot">&lt;-</span> sample_lyrics <span class="sc">%&gt;%</span></span>
<span id="cb51-2"><a href="#cb51-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">lyrics_clean =</span> <span class="fu">str_replace_all</span>(lyrics,<span class="st">&quot;</span><span class="sc">\\</span><span class="st">n&quot;</span>, <span class="st">&quot;</span><span class="sc">\\</span><span class="st">.&quot;</span>),</span>
<span id="cb51-3"><a href="#cb51-3" tabindex="-1"></a>         <span class="at">lyrics_clean =</span> <span class="fu">str_replace_all</span>(lyrics_clean,<span class="st">&quot;</span><span class="sc">\\</span><span class="st">r&quot;</span>, <span class="st">&quot;</span><span class="sc">\\</span><span class="st">.&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb51-4"><a href="#cb51-4" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>lyrics)</span></code></pre></div>
<pre><code>## mutate: new variable &#39;lyrics_clean&#39; (character) with 21 unique values and 0% NA</code></pre>
<pre><code>## select: dropped one variable (lyrics)</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" tabindex="-1"></a>sample_lyrics<span class="sc">$</span>lyrics_clean[<span class="dv">4</span>]</span></code></pre></div>
<pre><code>## [1] &quot;Yeah....The world is my expense..It’s the cost of my desire..Jesus blessed me with its future..And I protect it with fire....So raise your fists and march around..Just don’t take what you need..I’ll jail and bury those committed..And smother the rest in greed....Crawl with me into tomorrow..Or I’ll drag you to your grave..I’m deep inside your children..They’ll betray you in my name....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....The lie is my expense..The scope of my desire..The party blessed me with its future..And I protect it with fire....I am the Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire, sleep now in the fire....Hey, hey, sleep now in the fire..Hey, hey, sleep now in the fire....For it’s the end of history..It’s caged and frozen still..There is no other pill to take..So swallow the one that made you ill....The Niña, the Pinta, the Santa María..The noose and the rapist, the fields overseer..The Agents of Orange, the Priests of Hiroshima..The cost of my desire to sleep now in the fire....Yeah....Sleep now in the fire..Sleep now in the fire..Sleep now in the fire..Sleep now in the fire&quot;</code></pre>
</div>
<div id="tokenization" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Tokenization</h2>
<p>Our goal is to create a document-feature matrix, from which we will later extract information about word frequency. To do that, we start by crating a <code>corpus</code> object, from the <code>quanteda</code> package.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" tabindex="-1"></a>corpus_lyrics <span class="ot">&lt;-</span> <span class="fu">corpus</span>(sample_lyrics,</span>
<span id="cb56-2"><a href="#cb56-2" tabindex="-1"></a>                     <span class="at">text_field =</span> <span class="st">&quot;lyrics_clean&quot;</span>,</span>
<span id="cb56-3"><a href="#cb56-3" tabindex="-1"></a>                     <span class="at">unique_docnames =</span> <span class="cn">TRUE</span>)</span>
<span id="cb56-4"><a href="#cb56-4" tabindex="-1"></a></span>
<span id="cb56-5"><a href="#cb56-5" tabindex="-1"></a><span class="fu">summary</span>(corpus_lyrics)</span></code></pre></div>
<pre><code>## Corpus consisting of 21 documents, showing 21 documents:
## 
##    Text Types Tokens Sentences                   artist
##   text1   119    375        35 Rage Against the Machine
##   text2    52    853        83 Rage Against the Machine
##   text3   188    835        91 Rage Against the Machine
##   text4    97    352        38 Rage Against the Machine
##   text5   160    440        50 Rage Against the Machine
##   text6   133    535        67 Rage Against the Machine
##   text7   104    559        53         System of a Down
##   text8    67    365        40         System of a Down
##   text9    68    298        33         System of a Down
##  text10    65    258        32         System of a Down
##  text11   137    558        68         System of a Down
##  text12   131    876        70             Taylor Swift
##  text13   159    465        41             Taylor Swift
##  text14   162    544        62             Taylor Swift
##  text15   196    738        84             Taylor Swift
##  text16   169    549        50             Taylor Swift
##  text17   229    867        55      Megan Thee Stallion
##  text18   193    664        61      Megan Thee Stallion
##  text19   310   1190        87      Megan Thee Stallion
##  text20   198    656        48      Megan Thee Stallion
##  text21   256   1050        73      Megan Thee Stallion
##                       album year                  song
##                 Evil Empire 1996       Bulls on Parade
##    Rage Against the Machine 1992   Killing in the Name
##                   Renegades 2000     Renegades of Funk
##   The Battle of Los Angeles 1999 Sleep Now in the Fire
##   The Battle of Los Angeles 1999       Guerrilla Radio
##   The Battle of Los Angeles 1999               Testify
##                   Mezmerize 2005               B.Y.O.B
##                    Toxicity 2001            Chop Suey!
##                    Toxicity 2001               Aerials
##                    Toxicity 2001               Toxicty
##                    Toxicity 2001                 Sugar
##                        1989 2014          Shake it Off
##                   Midnights 2022             Anti-Hero
##                    Fearless 2008    You Belong With Me
##                        1989 2014           Blank Space
##                    Fearless 2008            Love Story
##                  Traumazine 2022                Plan B
##                        Suga 2020                Savage
##  Something for Thee Hotties 2021             Thot Shit
##                  Traumazine 2022                   Her
##                  Traumazine 2022            Ungrateful</code></pre>
<p>Looks good. Now we can tokenize our corpus (and reduce complexity). One benefit of creating a corpus object first is that we maintain all the metadata for every document when we tokenize. This will come in handy in the future.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" tabindex="-1"></a>lyrics_toks <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_lyrics,</span>
<span id="cb58-2"><a href="#cb58-2" tabindex="-1"></a>                   <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>, <span class="co"># Thinks about this</span></span>
<span id="cb58-3"><a href="#cb58-3" tabindex="-1"></a>                   <span class="at">remove_punct =</span> <span class="cn">TRUE</span>, <span class="co"># Remove punctuation!</span></span>
<span id="cb58-4"><a href="#cb58-4" tabindex="-1"></a>                   <span class="at">remove_url =</span> <span class="cn">TRUE</span>) <span class="co"># Might be helpful</span></span>
<span id="cb58-5"><a href="#cb58-5" tabindex="-1"></a>lyrics_toks[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">14</span>)]</span></code></pre></div>
<pre><code>## Tokens consisting of 2 documents and 4 docvars.
## text4 :
##  [1] &quot;Yeah&quot;    &quot;The&quot;     &quot;world&quot;   &quot;is&quot;      &quot;my&quot;      &quot;expense&quot; &quot;It’s&quot;   
##  [8] &quot;the&quot;     &quot;cost&quot;    &quot;of&quot;      &quot;my&quot;      &quot;desire&quot; 
## [ ... and 227 more ]
## 
## text14 :
##  [1] &quot;You&#39;re&quot;     &quot;on&quot;         &quot;the&quot;        &quot;phone&quot;      &quot;with&quot;      
##  [6] &quot;your&quot;       &quot;girlfriend&quot; &quot;she&#39;s&quot;      &quot;upset&quot;      &quot;She&#39;s&quot;     
## [11] &quot;going&quot;      &quot;off&quot;       
## [ ... and 385 more ]</code></pre>
<p>We got rid of the punctuation. Now let’s remove stop words, high and low frequency words, and stem the remaining tokens. Here I am cheating, though. I know that high and low frequency words because I checked my <code>dfm</code> (see next code chunk).</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" tabindex="-1"></a>lyrics_toks <span class="ot">&lt;-</span> <span class="fu">tokens_remove</span>(lyrics_toks,</span>
<span id="cb60-2"><a href="#cb60-2" tabindex="-1"></a>                          <span class="co"># you can change or add stopwords depending on the </span></span>
<span id="cb60-3"><a href="#cb60-3" tabindex="-1"></a>                          <span class="co"># language(s) of the documents</span></span>
<span id="cb60-4"><a href="#cb60-4" tabindex="-1"></a>                          <span class="fu">c</span>(<span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">&quot;en&quot;</span>),</span>
<span id="cb60-5"><a href="#cb60-5" tabindex="-1"></a>                            <span class="co"># Now is high frequency... there are many low</span></span>
<span id="cb60-6"><a href="#cb60-6" tabindex="-1"></a>                            <span class="co"># frequency tokens. We will check these later</span></span>
<span id="cb60-7"><a href="#cb60-7" tabindex="-1"></a>                            <span class="st">&quot;now&quot;</span>),</span>
<span id="cb60-8"><a href="#cb60-8" tabindex="-1"></a>                          <span class="at">padding =</span> F)</span>
<span id="cb60-9"><a href="#cb60-9" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" tabindex="-1"></a>lyrics_toks_stem <span class="ot">&lt;-</span> <span class="fu">tokens_wordstem</span>(lyrics_toks, <span class="at">language =</span> <span class="st">&quot;en&quot;</span>)</span>
<span id="cb60-11"><a href="#cb60-11" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" tabindex="-1"></a>lyrics_toks[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">14</span>)]</span></code></pre></div>
<pre><code>## Tokens consisting of 2 documents and 4 docvars.
## text4 :
##  [1] &quot;Yeah&quot;    &quot;world&quot;   &quot;expense&quot; &quot;It’s&quot;    &quot;cost&quot;    &quot;desire&quot;  &quot;Jesus&quot;  
##  [8] &quot;blessed&quot; &quot;future&quot;  &quot;protect&quot; &quot;fire&quot;    &quot;raise&quot;  
## [ ... and 105 more ]
## 
## text14 :
##  [1] &quot;phone&quot;      &quot;girlfriend&quot; &quot;upset&quot;      &quot;going&quot;      &quot;something&quot; 
##  [6] &quot;said&quot;       &quot;Cause&quot;      &quot;get&quot;        &quot;humor&quot;      &quot;like&quot;      
## [11] &quot;room&quot;       &quot;typical&quot;   
## [ ... and 133 more ]</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" tabindex="-1"></a>lyrics_toks_stem[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">14</span>)]</span></code></pre></div>
<pre><code>## Tokens consisting of 2 documents and 4 docvars.
## text4 :
##  [1] &quot;Yeah&quot;    &quot;world&quot;   &quot;expens&quot;  &quot;It’s&quot;    &quot;cost&quot;    &quot;desir&quot;   &quot;Jesus&quot;  
##  [8] &quot;bless&quot;   &quot;futur&quot;   &quot;protect&quot; &quot;fire&quot;    &quot;rais&quot;   
## [ ... and 105 more ]
## 
## text14 :
##  [1] &quot;phone&quot;      &quot;girlfriend&quot; &quot;upset&quot;      &quot;go&quot;         &quot;someth&quot;    
##  [6] &quot;said&quot;       &quot;Caus&quot;       &quot;get&quot;        &quot;humor&quot;      &quot;like&quot;      
## [11] &quot;room&quot;       &quot;typic&quot;     
## [ ... and 133 more ]</code></pre>
<p>We can compare the stemmed output and the non-stemmed output. Why did ‘future’ become ‘futur’? Because we are assuming that, for <em>our</em> purposes, ‘future=futuristic’. This is for the researcher to decide. And finally, we can create our document-feature matrix (<code>dfm</code>).</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" tabindex="-1"></a>lyrics_dfm <span class="ot">&lt;-</span> <span class="fu">dfm</span>(lyrics_toks)</span>
<span id="cb64-2"><a href="#cb64-2" tabindex="-1"></a>lyrics_dfm_stem <span class="ot">&lt;-</span> <span class="fu">dfm</span>(lyrics_toks_stem)</span>
<span id="cb64-3"><a href="#cb64-3" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" tabindex="-1"></a><span class="fu">head</span>(lyrics_dfm_stem)</span></code></pre></div>
<pre><code>## Document-feature matrix of: 6 documents, 1,165 features (93.12% sparse) and 4 docvars.
##        features
## docs    come wit microphon explod shatter mold either drop hit like
##   text1    4   4         1      1       1    1      1    3   1    1
##   text2    2   0         0      0       0    0      0    0   0    0
##   text3    0   0         0      0       0    0      0    0   0    4
##   text4    0   0         0      0       0    0      0    0   0    0
##   text5    0   0         0      0       0    0      0    0   0    1
##   text6    0   4         0      0       0    0      0    0   0    0
## [ reached max_nfeat ... 1,155 more features ]</code></pre>
<p>Note that once we create the <code>dfm</code> object, all the tokens become lowercase. Now we can check what are the 15 most frequent tokens.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" tabindex="-1"></a>lyrics_dfm_stem <span class="sc">%&gt;%</span></span>
<span id="cb66-2"><a href="#cb66-2" tabindex="-1"></a>  <span class="fu">textstat_frequency</span>(<span class="at">n=</span><span class="dv">30</span>) <span class="sc">%&gt;%</span></span>
<span id="cb66-3"><a href="#cb66-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(feature,frequency),<span class="at">y=</span>frequency,<span class="at">fill =</span> (frequency), <span class="at">color =</span> (frequency))) <span class="sc">+</span></span>
<span id="cb66-4"><a href="#cb66-4" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">alpha=</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb66-5"><a href="#cb66-5" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb66-6"><a href="#cb66-6" tabindex="-1"></a>  <span class="fu">scale_x_reordered</span>() <span class="sc">+</span></span>
<span id="cb66-7"><a href="#cb66-7" tabindex="-1"></a>  <span class="fu">scale_color_distiller</span>(<span class="at">palette =</span> <span class="st">&quot;PuOr&quot;</span>) <span class="sc">+</span></span>
<span id="cb66-8"><a href="#cb66-8" tabindex="-1"></a>  <span class="fu">scale_fill_distiller</span>(<span class="at">palette =</span> <span class="st">&quot;PuOr&quot;</span>) <span class="sc">+</span></span>
<span id="cb66-9"><a href="#cb66-9" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb66-10"><a href="#cb66-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>,<span class="at">y=</span><span class="st">&quot;Frequency&quot;</span>,<span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb66-11"><a href="#cb66-11" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>) </span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Does not tell us much, but I used the previous code to check for low-information tokens that I might want to remove from my analysis. We can also see how many tokens appear only once:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" tabindex="-1"></a>only_once <span class="ot">&lt;-</span> lyrics_dfm_stem <span class="sc">%&gt;%</span></span>
<span id="cb67-2"><a href="#cb67-2" tabindex="-1"></a>  <span class="fu">textstat_frequency</span>() <span class="sc">%&gt;%</span></span>
<span id="cb67-3"><a href="#cb67-3" tabindex="-1"></a>  <span class="fu">filter</span>(frequency <span class="sc">==</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## filter: removed 566 rows (49%), 599 rows remaining</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" tabindex="-1"></a><span class="fu">length</span>(only_once<span class="sc">$</span>feature)</span></code></pre></div>
<pre><code>## [1] 599</code></pre>
<p>More interesting for text analysis is to count words over time/space. In this case, our ‘space’ can be the artist.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" tabindex="-1"></a>lyrics_dfm_stem <span class="sc">%&gt;%</span></span>
<span id="cb71-2"><a href="#cb71-2" tabindex="-1"></a>  <span class="fu">textstat_frequency</span>(<span class="at">n=</span><span class="dv">15</span>, <span class="at">groups =</span> <span class="fu">c</span>(artist)) <span class="sc">%&gt;%</span></span>
<span id="cb71-3"><a href="#cb71-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder_within</span>(feature,frequency,group),<span class="at">y=</span>frequency,<span class="at">fill =</span> group, <span class="at">color =</span> group)) <span class="sc">+</span></span>
<span id="cb71-4"><a href="#cb71-4" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb71-5"><a href="#cb71-5" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb71-6"><a href="#cb71-6" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb71-7"><a href="#cb71-7" tabindex="-1"></a>  <span class="fu">scale_x_reordered</span>() <span class="sc">+</span></span>
<span id="cb71-8"><a href="#cb71-8" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;PuOr&quot;</span>) <span class="sc">+</span></span>
<span id="cb71-9"><a href="#cb71-9" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;PuOr&quot;</span>) <span class="sc">+</span></span>
<span id="cb71-10"><a href="#cb71-10" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb71-11"><a href="#cb71-11" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>,<span class="at">y=</span><span class="st">&quot;&quot;</span>,<span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb71-12"><a href="#cb71-12" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>) </span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Interesting. Not a lot of overlap (apart from a token from Megan Thee Stallion and Rage Against the Machine). However, it would be great if we could measure the importance of a word relative to how much it appears across documents (one way to denominate). Enter TF-IDF: “Term-Frequency / Inverse-Document-frequency”. TF-IDF weighting up-weights relatively rare words that do not appear in all documents. Using term frequency and inverse document frequency allows us to find words that are characteristic for one document within a collection of documents.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" tabindex="-1"></a>lyrics_dfm_tfidf <span class="ot">&lt;-</span> <span class="fu">dfm_tfidf</span>(lyrics_dfm_stem)</span>
<span id="cb72-2"><a href="#cb72-2" tabindex="-1"></a></span>
<span id="cb72-3"><a href="#cb72-3" tabindex="-1"></a>lyrics_dfm_tfidf <span class="sc">%&gt;%</span></span>
<span id="cb72-4"><a href="#cb72-4" tabindex="-1"></a>  <span class="fu">textstat_frequency</span>(<span class="at">n=</span><span class="dv">15</span>, <span class="at">groups =</span> <span class="fu">c</span>(artist), <span class="at">force=</span>T) <span class="sc">%&gt;%</span></span>
<span id="cb72-5"><a href="#cb72-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder_within</span>(feature,frequency,group),<span class="at">y=</span>frequency,<span class="at">fill =</span> group, <span class="at">color =</span> group)) <span class="sc">+</span></span>
<span id="cb72-6"><a href="#cb72-6" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb72-7"><a href="#cb72-7" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb72-8"><a href="#cb72-8" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>group, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb72-9"><a href="#cb72-9" tabindex="-1"></a>  <span class="fu">scale_x_reordered</span>() <span class="sc">+</span></span>
<span id="cb72-10"><a href="#cb72-10" tabindex="-1"></a>  <span class="fu">scale_color_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;PuOr&quot;</span>) <span class="sc">+</span></span>
<span id="cb72-11"><a href="#cb72-11" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;PuOr&quot;</span>) <span class="sc">+</span></span>
<span id="cb72-12"><a href="#cb72-12" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb72-13"><a href="#cb72-13" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>,<span class="at">y=</span><span class="st">&quot;TF-IDF&quot;</span>,<span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb72-14"><a href="#cb72-14" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>) </span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>If we are building a dictionary, for example, we might want to include words with high TF-IDF values. Another way to think about TF-IDF is in terms of predictive power. Words that are common to all documents do not have any predictive power and receive a TD-IDF value of 0. Words that appear, but only in relatively few document, have greater predictive power and receive a TD-IDF &gt; 0. Very rare words are also penalized, since these might provide only specific information about one document (i.e., high prediction for one document but no information about the rest). As you will read in Chapter 6/7 in Grimmer et al., the idea is to find the right balance.</p>
<p>Another useful tool (and concept) is keyness. Keyness is a two-by-two association score for features that occur differentially across different categories. We can estimate which features are associated more with one category (in this case, one artist), compared to the other. Let’s compare Megan Thee Stallion and Taylor Swift.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" tabindex="-1"></a>lyrics_dfm_ts_mts <span class="ot">&lt;-</span> <span class="fu">dfm_subset</span>(lyrics_dfm_stem, year <span class="sc">&gt;</span> <span class="dv">2006</span>)</span>
<span id="cb73-2"><a href="#cb73-2" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" tabindex="-1"></a>lyrics_key <span class="ot">&lt;-</span> <span class="fu">textstat_keyness</span>(lyrics_dfm_ts_mts, </span>
<span id="cb73-4"><a href="#cb73-4" tabindex="-1"></a>                              <span class="at">target =</span> lyrics_dfm_ts_mts<span class="sc">$</span>artist <span class="sc">==</span> <span class="st">&quot;Taylor Swift&quot;</span>)</span>
<span id="cb73-5"><a href="#cb73-5" tabindex="-1"></a><span class="fu">textplot_keyness</span>(lyrics_key)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Similar to what we would have implied from the TF-IDF graphs. Notice that stemming does not always works are expected. Taylor Swift sings about “shake, shake, shake” and Megan Thee Stallion sings about “shaking”. However, both words appear as distinct features from both artists.</p>
</div>
<div id="word-frequency-across-artist" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Word Frequency Across Artist</h2>
<p>We can do something similar to what we did last week to look at word frequencies. Rather than creating a <code>dfm</code>, we can use the dataset as is, and get some information. For example, the average number of tokens by artist.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" tabindex="-1"></a>sample_lyrics <span class="sc">%&gt;%</span></span>
<span id="cb74-2"><a href="#cb74-2" tabindex="-1"></a>  <span class="co"># take the column lyrics_clean and divide it by words</span></span>
<span id="cb74-3"><a href="#cb74-3" tabindex="-1"></a>  <span class="co"># this uses a similar tokenizer to quanteda</span></span>
<span id="cb74-4"><a href="#cb74-4" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, lyrics_clean) <span class="sc">%&gt;%</span></span>
<span id="cb74-5"><a href="#cb74-5" tabindex="-1"></a>  <span class="fu">group_by</span>(song) <span class="sc">%&gt;%</span></span>
<span id="cb74-6"><a href="#cb74-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_tk_song =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb74-7"><a href="#cb74-7" tabindex="-1"></a>  <span class="fu">distinct</span>(song,<span class="at">.keep_all=</span>T) <span class="sc">%&gt;%</span> </span>
<span id="cb74-8"><a href="#cb74-8" tabindex="-1"></a>  <span class="fu">group_by</span>(artist) <span class="sc">%&gt;%</span></span>
<span id="cb74-9"><a href="#cb74-9" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mean_tokens =</span> <span class="fu">mean</span>(total_tk_song)) <span class="sc">%&gt;%</span></span>
<span id="cb74-10"><a href="#cb74-10" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>song,<span class="at">y=</span>total_tk_song,<span class="at">fill=</span>artist,<span class="at">color=</span>artist)) <span class="sc">+</span></span>
<span id="cb74-11"><a href="#cb74-11" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">alpha=</span>.<span class="dv">8</span>) <span class="sc">+</span></span>
<span id="cb74-12"><a href="#cb74-12" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> mean_tokens, <span class="at">color =</span> artist), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>)<span class="sc">+</span></span>
<span id="cb74-13"><a href="#cb74-13" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb74-14"><a href="#cb74-14" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb74-15"><a href="#cb74-15" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>artist, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>, <span class="at">nrow =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb74-16"><a href="#cb74-16" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb74-17"><a href="#cb74-17" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>,</span>
<span id="cb74-18"><a href="#cb74-18" tabindex="-1"></a>        <span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">size =</span> <span class="dv">5</span>,<span class="at">vjust =</span> <span class="fl">0.5</span>, <span class="at">hjust=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb74-19"><a href="#cb74-19" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Total Tokens&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb74-20"><a href="#cb74-20" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Mean token length in dashed line.&quot;</span>)</span></code></pre></div>
<pre><code>## group_by: one grouping variable (song)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_tk_song&#39; (integer) with 20 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 8,958 rows (&gt;99%), 21 rows remaining</code></pre>
<pre><code>## group_by: one grouping variable (artist)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;mean_tokens&#39; (double) with 4 unique values and 0% NA</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Alternatively, we can estimate the frequency of a specific token by song.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" tabindex="-1"></a>lyrics_totals <span class="ot">&lt;-</span> sample_lyrics <span class="sc">%&gt;%</span></span>
<span id="cb80-2"><a href="#cb80-2" tabindex="-1"></a>  <span class="co"># take the column lyrics_clean and divide it by words</span></span>
<span id="cb80-3"><a href="#cb80-3" tabindex="-1"></a>  <span class="co"># this uses a similar tokenizer to quanteda</span></span>
<span id="cb80-4"><a href="#cb80-4" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, lyrics_clean) <span class="sc">%&gt;%</span></span>
<span id="cb80-5"><a href="#cb80-5" tabindex="-1"></a>  <span class="fu">group_by</span>(song) <span class="sc">%&gt;%</span></span>
<span id="cb80-6"><a href="#cb80-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_tk_song =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb80-7"><a href="#cb80-7" tabindex="-1"></a>  <span class="fu">distinct</span>(song,<span class="at">.keep_all=</span>T) </span></code></pre></div>
<pre><code>## group_by: one grouping variable (song)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_tk_song&#39; (integer) with 20 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 8,958 rows (&gt;99%), 21 rows remaining</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" tabindex="-1"></a><span class="co"># let&#39;s look for &quot;like&quot;</span></span>
<span id="cb84-2"><a href="#cb84-2" tabindex="-1"></a>lyrics_like <span class="ot">&lt;-</span> sample_lyrics <span class="sc">%&gt;%</span></span>
<span id="cb84-3"><a href="#cb84-3" tabindex="-1"></a>  <span class="co"># take the column lyrics_clean and divide it by words</span></span>
<span id="cb84-4"><a href="#cb84-4" tabindex="-1"></a>  <span class="co"># this uses a similar tokenizer to quanteda</span></span>
<span id="cb84-5"><a href="#cb84-5" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, lyrics_clean) <span class="sc">%&gt;%</span></span>
<span id="cb84-6"><a href="#cb84-6" tabindex="-1"></a>  <span class="fu">filter</span>(word<span class="sc">==</span><span class="st">&quot;like&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb84-7"><a href="#cb84-7" tabindex="-1"></a>  <span class="fu">group_by</span>(song) <span class="sc">%&gt;%</span></span>
<span id="cb84-8"><a href="#cb84-8" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_like_song =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb84-9"><a href="#cb84-9" tabindex="-1"></a>  <span class="fu">distinct</span>(song,total_like_song) </span></code></pre></div>
<pre><code>## filter: removed 8,934 rows (99%), 45 rows remaining</code></pre>
<pre><code>## group_by: one grouping variable (song)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_like_song&#39; (integer) with 7 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 33 rows (73%), 12 rows remaining</code></pre>
<p>We can now join these two data frames together with the <code>left_join()</code> function and join by the “song” column. We can then pipe the joined data into a plot.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" tabindex="-1"></a>lyrics_totals <span class="sc">%&gt;%</span></span>
<span id="cb89-2"><a href="#cb89-2" tabindex="-1"></a>  <span class="fu">left_join</span>(lyrics_like, <span class="at">by =</span> <span class="st">&quot;song&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb89-3"><a href="#cb89-3" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb89-4"><a href="#cb89-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">like_prop =</span> total_like_song<span class="sc">/</span>total_tk_song) <span class="sc">%&gt;%</span></span>
<span id="cb89-5"><a href="#cb89-5" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>song,<span class="at">y=</span>like_prop,<span class="at">fill=</span>artist,<span class="at">color=</span>artist)) <span class="sc">+</span></span>
<span id="cb89-6"><a href="#cb89-6" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">alpha=</span>.<span class="dv">8</span>) <span class="sc">+</span></span>
<span id="cb89-7"><a href="#cb89-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb89-8"><a href="#cb89-8" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;Royal2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb89-9"><a href="#cb89-9" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>artist, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>, <span class="at">nrow =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb89-10"><a href="#cb89-10" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb89-11"><a href="#cb89-11" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>,</span>
<span id="cb89-12"><a href="#cb89-12" tabindex="-1"></a>        <span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">size =</span> <span class="dv">5</span>,<span class="at">vjust =</span> <span class="fl">0.5</span>, <span class="at">hjust=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb89-13"><a href="#cb89-13" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Prop. of </span><span class="sc">\&#39;</span><span class="st">Like</span><span class="sc">\&#39;</span><span class="st">&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<pre><code>## left_join: added one column (total_like_song)</code></pre>
<pre><code>##            &gt; rows only in x    9</code></pre>
<pre><code>##            &gt; rows only in y  ( 0)</code></pre>
<pre><code>##            &gt; matched rows     12</code></pre>
<pre><code>##            &gt;                 ====</code></pre>
<pre><code>##            &gt; rows total       21</code></pre>
<pre><code>## ungroup: no grouping variables</code></pre>
<pre><code>## mutate: new variable &#39;like_prop&#39; (double) with 13 unique values and 43% NA</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
<div id="final-words-1" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Final Words</h2>
<p>As will be often the case, we won’t be able to cover every single feature that the different packages have to offer, or how some objects that we create look like, or what else we can do with them. My advise is that you go home and explore the code in detail. Try applying the code to a different corpus and come next class with questions (or just show off what you were able to do).</p>
<!--chapter:end:02-week2.Rmd-->
</div>
</div>
<div id="week-3-dictionary-based-approaches" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Week 3: Dictionary-Based Approaches</h1>
<div id="slides-2" class="section level2 unnumbered">
<h2 class="unnumbered">Slides</h2>
<ul>
<li>4 Dictionary-Based Approaches (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/4%20Dictionary%20Based%20Approaches.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup-2" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Setup</h2>
<p>As always, we first load the packages that we’ll be using:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># for wrangling data</span></span>
<span id="cb98-2"><a href="#cb98-2" tabindex="-1"></a><span class="fu">library</span>(tidylog) <span class="co"># to know what we are wrangling</span></span>
<span id="cb98-3"><a href="#cb98-3" tabindex="-1"></a><span class="fu">library</span>(tidytext) <span class="co"># for &#39;tidy&#39; manipulation of text data</span></span>
<span id="cb98-4"><a href="#cb98-4" tabindex="-1"></a><span class="fu">library</span>(textdata) <span class="co"># text datasets</span></span>
<span id="cb98-5"><a href="#cb98-5" tabindex="-1"></a><span class="fu">library</span>(quanteda) <span class="co"># tokenization power house</span></span>
<span id="cb98-6"><a href="#cb98-6" tabindex="-1"></a><span class="fu">library</span>(quanteda.textstats)</span>
<span id="cb98-7"><a href="#cb98-7" tabindex="-1"></a><span class="fu">library</span>(quanteda.dictionaries)</span>
<span id="cb98-8"><a href="#cb98-8" tabindex="-1"></a><span class="fu">library</span>(wesanderson) <span class="co"># to prettify</span></span>
<span id="cb98-9"><a href="#cb98-9" tabindex="-1"></a><span class="fu">library</span>(knitr) <span class="co"># for displaying data in html format (relevant for formatting this worksheet mainly)</span></span></code></pre></div>
</div>
<div id="get-data-2" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Get Data:</h2>
<p>For this example, we will be using data from <em>Ventura et al. (2021) - Connective effervescence and streaming chat during political debates</em>.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/ventura_etal_df.Rdata&quot;</span>)</span>
<span id="cb99-2"><a href="#cb99-2" tabindex="-1"></a><span class="fu">head</span>(ventura_etal_df)</span></code></pre></div>
<pre><code>##   text_id
## 1       1
## 2       2
## 3       3
## 4       4
## 5       5
## 6       6
##                                                                                                                                                                                                                                       comments
## 1 MORE:\n The coronavirus pandemic&#39;s impact on the race will be on display as the\n two candidates won&#39;t partake in a handshake, customary at the top of \nsuch events. The size of the audience will also be limited. https://abcn.ws/3kVyl16
## 2                                                                                                                                                           God please bless all Trump supporters. They need it for they know not what they do
## 3                                                                                                                               Trump  is  a  living  disaster!    What  an embarrassment  to  all  human  beings!    The  man  is  dangerous!
## 4                                                                                                                                                   This debate is why other counties laugh at us. School yard class president debate at best.
## 5                                                                    OMG\n ... shut up tRump ... so rude and out of control.  Obviously freaking \nout.  This is a debate NOT a convention or a speech or your platform.  \nLearn some manners
## 6                                                                                                      It’s\n hard to see what this country has become.  The Presidency is no longer a\n respected position it has lost all of it’s integrity.
##                      id likes                  debate
## 1              ABC News   100 abc_first_debate_manual
## 2            Anita Hill    61 abc_first_debate_manual
## 3          Dave Garland    99 abc_first_debate_manual
## 4              Carl Roy    47 abc_first_debate_manual
## 5 Lynda Martin-Chambers   154 abc_first_debate_manual
## 6         Nica Merchant   171 abc_first_debate_manual</code></pre>
</div>
<div id="tokenization-etc." class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Tokenization etc.</h2>
<p>The comments are mostly clean, but you can check (on your own) if they require more cleaning. In the previous code, I showed you how to lower, remove stopwords, etc., using quanteda. We can also do this using tidytext <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" tabindex="-1"></a>tidy_ventura <span class="ot">&lt;-</span> ventura_etal_df <span class="sc">%&gt;%</span> </span>
<span id="cb101-2"><a href="#cb101-2" tabindex="-1"></a>  <span class="co"># to lower:</span></span>
<span id="cb101-3"><a href="#cb101-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">comments =</span> <span class="fu">tolower</span>(comments)) <span class="sc">%&gt;%</span></span>
<span id="cb101-4"><a href="#cb101-4" tabindex="-1"></a>  <span class="co"># tokenize</span></span>
<span id="cb101-5"><a href="#cb101-5" tabindex="-1"></a>  <span class="fu">unnest_tokens</span>(word, comments) <span class="sc">%&gt;%</span></span>
<span id="cb101-6"><a href="#cb101-6" tabindex="-1"></a>  <span class="co"># keep only words (check regex)</span></span>
<span id="cb101-7"><a href="#cb101-7" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(word, <span class="st">&quot;[a-z]&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb101-8"><a href="#cb101-8" tabindex="-1"></a>  <span class="co"># remove stop words</span></span>
<span id="cb101-9"><a href="#cb101-9" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>word <span class="sc">%in%</span> stop_words<span class="sc">$</span>word)</span></code></pre></div>
<pre><code>## mutate: changed 29,261 values (99%) of &#39;comments&#39; (0 new NA)</code></pre>
<pre><code>## filter: removed 3,374 rows (1%), 494,341 rows remaining</code></pre>
<pre><code>## filter: removed 296,793 rows (60%), 197,548 rows remaining</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" tabindex="-1"></a><span class="fu">head</span>(tidy_ventura, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>##    text_id         id likes                  debate        word
## 1        1   ABC News   100 abc_first_debate_manual coronavirus
## 2        1   ABC News   100 abc_first_debate_manual  pandemic&#39;s
## 3        1   ABC News   100 abc_first_debate_manual      impact
## 4        1   ABC News   100 abc_first_debate_manual        race
## 5        1   ABC News   100 abc_first_debate_manual     display
## 6        1   ABC News   100 abc_first_debate_manual  candidates
## 7        1   ABC News   100 abc_first_debate_manual     partake
## 8        1   ABC News   100 abc_first_debate_manual   handshake
## 9        1   ABC News   100 abc_first_debate_manual   customary
## 10       1   ABC News   100 abc_first_debate_manual         top
## 11       1   ABC News   100 abc_first_debate_manual      events
## 12       1   ABC News   100 abc_first_debate_manual        size
## 13       1   ABC News   100 abc_first_debate_manual    audience
## 14       1   ABC News   100 abc_first_debate_manual     limited
## 15       1   ABC News   100 abc_first_debate_manual       https
## 16       1   ABC News   100 abc_first_debate_manual     abcn.ws
## 17       1   ABC News   100 abc_first_debate_manual     3kvyl16
## 18       2 Anita Hill    61 abc_first_debate_manual         god
## 19       2 Anita Hill    61 abc_first_debate_manual       bless
## 20       2 Anita Hill    61 abc_first_debate_manual       trump</code></pre>
</div>
<div id="keywords" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Keywords</h2>
<p>We can detect the occurrence of the words <strong>trump</strong> and <strong>biden</strong> in each comment (<code>text_id</code>).</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" tabindex="-1"></a>trump_biden <span class="ot">&lt;-</span> tidy_ventura <span class="sc">%&gt;%</span></span>
<span id="cb107-2"><a href="#cb107-2" tabindex="-1"></a>  <span class="co"># create a dummy</span></span>
<span id="cb107-3"><a href="#cb107-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trump_token =</span> <span class="fu">ifelse</span>(word<span class="sc">==</span><span class="st">&quot;trump&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb107-4"><a href="#cb107-4" tabindex="-1"></a>         <span class="at">biden_token =</span> <span class="fu">ifelse</span>(word<span class="sc">==</span><span class="st">&quot;biden&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb107-5"><a href="#cb107-5" tabindex="-1"></a>  <span class="co"># see which comments have the word trump / biden</span></span>
<span id="cb107-6"><a href="#cb107-6" tabindex="-1"></a>  <span class="fu">group_by</span>(text_id) <span class="sc">%&gt;%</span></span>
<span id="cb107-7"><a href="#cb107-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">trump_cmmnt =</span> <span class="fu">ifelse</span>(<span class="fu">sum</span>(trump_token)<span class="sc">&gt;</span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb107-8"><a href="#cb107-8" tabindex="-1"></a>         <span class="at">biden_cmmnt =</span> <span class="fu">ifelse</span>(<span class="fu">sum</span>(biden_token)<span class="sc">&gt;</span><span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb107-9"><a href="#cb107-9" tabindex="-1"></a>  <span class="co"># reduce to our unit of analysis (comment) </span></span>
<span id="cb107-10"><a href="#cb107-10" tabindex="-1"></a>  <span class="fu">distinct</span>(text_id, <span class="at">.keep_all =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb107-11"><a href="#cb107-11" tabindex="-1"></a>  <span class="fu">select</span>(text_id,trump_cmmnt,biden_cmmnt,likes,debate)</span></code></pre></div>
<pre><code>## mutate: new variable &#39;trump_token&#39; (double) with 2 unique values and 0% NA</code></pre>
<pre><code>##         new variable &#39;biden_token&#39; (double) with 2 unique values and 0% NA</code></pre>
<pre><code>## group_by: one grouping variable (text_id)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;trump_cmmnt&#39; (double) with 2 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;biden_cmmnt&#39; (double) with 2 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 168,013 rows (85%), 29,535 rows remaining</code></pre>
<pre><code>## select: dropped 4 variables (id, word, trump_token, biden_token)</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" tabindex="-1"></a><span class="fu">head</span>(trump_biden, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## # A tibble: 20 × 5
## # Groups:   text_id [20]
##    text_id trump_cmmnt biden_cmmnt likes debate                 
##      &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;                  
##  1       1           0           0   100 abc_first_debate_manual
##  2       2           1           0    61 abc_first_debate_manual
##  3       3           1           0    99 abc_first_debate_manual
##  4       4           0           0    47 abc_first_debate_manual
##  5       5           1           0   154 abc_first_debate_manual
##  6       6           0           0   171 abc_first_debate_manual
##  7       7           0           0    79 abc_first_debate_manual
##  8       8           0           0    39 abc_first_debate_manual
##  9       9           0           0    53 abc_first_debate_manual
## 10      10           0           0    36 abc_first_debate_manual
## 11      11           1           0    41 abc_first_debate_manual
## 12      12           0           0    28 abc_first_debate_manual
## 13      13           1           0    54 abc_first_debate_manual
## 14      14           0           0    30 abc_first_debate_manual
## 15      15           1           0    27 abc_first_debate_manual
## 16      16           1           1    31 abc_first_debate_manual
## 17      17           1           0    35 abc_first_debate_manual
## 18      18           1           1    32 abc_first_debate_manual
## 19      19           0           0    34 abc_first_debate_manual
## 20      20           1           0    37 abc_first_debate_manual</code></pre>
<p>Rather than replicating the results from Figure 3 in Ventura et al. (2021), we will estimate the median number of likes a comment mentioning Trump, Biden, Both, and None get:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" tabindex="-1"></a>trump_biden <span class="sc">%&gt;%</span></span>
<span id="cb117-2"><a href="#cb117-2" tabindex="-1"></a>  <span class="co"># Create categories</span></span>
<span id="cb117-3"><a href="#cb117-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">0</span>, <span class="st">&quot;1. None&quot;</span>, <span class="cn">NA</span>),</span>
<span id="cb117-4"><a href="#cb117-4" tabindex="-1"></a>         <span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">0</span>, <span class="st">&quot;2. Trump&quot;</span>, mentions_cat),</span>
<span id="cb117-5"><a href="#cb117-5" tabindex="-1"></a>         <span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;3. Biden&quot;</span>, mentions_cat),</span>
<span id="cb117-6"><a href="#cb117-6" tabindex="-1"></a>         <span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;4. Both&quot;</span>, mentions_cat)) <span class="sc">%&gt;%</span></span>
<span id="cb117-7"><a href="#cb117-7" tabindex="-1"></a>  <span class="fu">group_by</span>(mentions_cat) <span class="sc">%&gt;%</span></span>
<span id="cb117-8"><a href="#cb117-8" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">likes_mean =</span> <span class="fu">median</span>(likes, <span class="at">na.rm =</span> T)) <span class="sc">%&gt;%</span></span>
<span id="cb117-9"><a href="#cb117-9" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb117-10"><a href="#cb117-10" tabindex="-1"></a>  <span class="co"># Remove the ones people like too much</span></span>
<span id="cb117-11"><a href="#cb117-11" tabindex="-1"></a>  <span class="fu">filter</span>(likes <span class="sc">&lt;</span> <span class="dv">26</span>) <span class="sc">%&gt;%</span></span>
<span id="cb117-12"><a href="#cb117-12" tabindex="-1"></a>  <span class="co"># Plot</span></span>
<span id="cb117-13"><a href="#cb117-13" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>likes,<span class="at">fill =</span> mentions_cat, <span class="at">color =</span> mentions_cat)) <span class="sc">+</span></span>
<span id="cb117-14"><a href="#cb117-14" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb117-15"><a href="#cb117-15" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb117-16"><a href="#cb117-16" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb117-17"><a href="#cb117-17" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>mentions_cat, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb117-18"><a href="#cb117-18" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb117-19"><a href="#cb117-19" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> likes_mean, <span class="at">color =</span> mentions_cat), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>)<span class="sc">+</span></span>
<span id="cb117-20"><a href="#cb117-20" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb117-21"><a href="#cb117-21" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb117-22"><a href="#cb117-22" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Median likes in dashed lines.&quot;</span>)</span></code></pre></div>
<pre><code>## mutate (grouped): new variable &#39;mentions_cat&#39; (character) with 4 unique values and 0% NA</code></pre>
<pre><code>## group_by: one grouping variable (mentions_cat)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;likes_mean&#39; (double) with 4 unique values and 0% NA</code></pre>
<pre><code>## ungroup: no grouping variables</code></pre>
<pre><code>## filter: removed 8,136 rows (28%), 21,399 rows remaining</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>And we can also see if there are differences across news media:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" tabindex="-1"></a>trump_biden <span class="sc">%&gt;%</span></span>
<span id="cb123-2"><a href="#cb123-2" tabindex="-1"></a>  <span class="co"># Create categories</span></span>
<span id="cb123-3"><a href="#cb123-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">0</span>, <span class="st">&quot;1. None&quot;</span>, <span class="cn">NA</span>),</span>
<span id="cb123-4"><a href="#cb123-4" tabindex="-1"></a>         <span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">0</span>, <span class="st">&quot;2. Trump&quot;</span>, mentions_cat),</span>
<span id="cb123-5"><a href="#cb123-5" tabindex="-1"></a>         <span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">0</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;3. Biden&quot;</span>, mentions_cat),</span>
<span id="cb123-6"><a href="#cb123-6" tabindex="-1"></a>         <span class="at">mentions_cat =</span> <span class="fu">ifelse</span>(trump_cmmnt<span class="sc">==</span><span class="dv">1</span> <span class="sc">&amp;</span> biden_cmmnt<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;4. Both&quot;</span>, mentions_cat),</span>
<span id="cb123-7"><a href="#cb123-7" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;abc&quot;</span>), <span class="st">&quot;ABC&quot;</span>, <span class="cn">NA</span>),</span>
<span id="cb123-8"><a href="#cb123-8" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;nbc&quot;</span>), <span class="st">&quot;NBC&quot;</span>, media),</span>
<span id="cb123-9"><a href="#cb123-9" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;fox&quot;</span>), <span class="st">&quot;FOX&quot;</span>, media)) <span class="sc">%&gt;%</span></span>
<span id="cb123-10"><a href="#cb123-10" tabindex="-1"></a>  <span class="fu">group_by</span>(mentions_cat,media) <span class="sc">%&gt;%</span></span>
<span id="cb123-11"><a href="#cb123-11" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">median_like =</span> <span class="fu">median</span>(likes,<span class="at">na.rm =</span> T)) <span class="sc">%&gt;%</span></span>
<span id="cb123-12"><a href="#cb123-12" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb123-13"><a href="#cb123-13" tabindex="-1"></a>  <span class="co"># Remove the ones people like too much</span></span>
<span id="cb123-14"><a href="#cb123-14" tabindex="-1"></a>  <span class="fu">filter</span>(likes <span class="sc">&lt;</span> <span class="dv">26</span>) <span class="sc">%&gt;%</span></span>
<span id="cb123-15"><a href="#cb123-15" tabindex="-1"></a>  <span class="co"># Plot</span></span>
<span id="cb123-16"><a href="#cb123-16" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>likes,<span class="at">fill =</span> mentions_cat, <span class="at">color =</span> mentions_cat)) <span class="sc">+</span></span>
<span id="cb123-17"><a href="#cb123-17" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb123-18"><a href="#cb123-18" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb123-19"><a href="#cb123-19" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb123-20"><a href="#cb123-20" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>media, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb123-21"><a href="#cb123-21" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> median_like, <span class="at">color =</span> mentions_cat), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>)<span class="sc">+</span></span>
<span id="cb123-22"><a href="#cb123-22" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb123-23"><a href="#cb123-23" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb123-24"><a href="#cb123-24" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb123-25"><a href="#cb123-25" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Median likes in dashed lines.&quot;</span>)</span></code></pre></div>
<pre><code>## mutate (grouped): new variable &#39;mentions_cat&#39; (character) with 4 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;media&#39; (character) with 3 unique values and 0% NA</code></pre>
<pre><code>## group_by: 2 grouping variables (mentions_cat, media)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;median_like&#39; (double) with 6 unique values and 0% NA</code></pre>
<pre><code>## ungroup: no grouping variables</code></pre>
<pre><code>## filter: removed 8,136 rows (28%), 21,399 rows remaining</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Similar to Young and Soroka (2012), we can also explore our keywords of interest in context. This is a good way to validate our proposed measure (e.g., Is mentioning <em>trump</em> a reflection of interest? Of relevance?).</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" tabindex="-1"></a>corpus_ventura <span class="ot">&lt;-</span> <span class="fu">corpus</span>(ventura_etal_df,</span>
<span id="cb130-2"><a href="#cb130-2" tabindex="-1"></a>                     <span class="at">text_field =</span> <span class="st">&quot;comments&quot;</span>,</span>
<span id="cb130-3"><a href="#cb130-3" tabindex="-1"></a>                     <span class="at">unique_docnames =</span> <span class="cn">TRUE</span>)</span>
<span id="cb130-4"><a href="#cb130-4" tabindex="-1"></a></span>
<span id="cb130-5"><a href="#cb130-5" tabindex="-1"></a>toks_ventura <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_ventura)</span>
<span id="cb130-6"><a href="#cb130-6" tabindex="-1"></a>kw_trump <span class="ot">&lt;-</span> <span class="fu">kwic</span>(toks_ventura, <span class="at">pattern =</span> <span class="st">&quot;Trump&quot;</span>)</span>
<span id="cb130-7"><a href="#cb130-7" tabindex="-1"></a></span>
<span id="cb130-8"><a href="#cb130-8" tabindex="-1"></a><span class="do">## The number determines the size of the window: how many tokens before and after</span></span>
<span id="cb130-9"><a href="#cb130-9" tabindex="-1"></a><span class="fu">head</span>(kw_trump, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## Keyword-in-context with 20 matches.                                                 
##    [text2, 5]      God please bless all | Trump |
##    [text3, 1]                           | Trump |
##    [text5, 7]               ... shut up | tRump |
##  [text11, 11] a bad opiate problem then | trump |
##   [text13, 4]                 This is a | TRUMP |
##   [text15, 1]                           | Trump |
##   [text16, 8]  this SO much better than | Trump |
##   [text17, 3]                    I love | Trump |
##   [text18, 4]            Biden is right | Trump |
##   [text20, 1]                           | Trump |
##  [text22, 12]     being a decent human. | Trump |
##   [text23, 1]                           | Trump |
##  [text27, 11]          for once, i wish | trump |
##  [text28, 10]             it America... | Trump |
##   [text30, 1]                           | Trump |
##   [text31, 1]                           | Trump |
##   [text32, 1]                           | Trump |
##  [text32, 15]    People open your eyes. | Trump |
##   [text34, 1]                           | Trump |
##   [text36, 1]                           | Trump |
##                                 
##  supporters. They need it       
##  is a living disaster!          
##  ... so rude                    
##  brings up about bidens son     
##  all about ME debate and        
##  is looking pretty flushed right
##  and I wasn’t even going        
##  ! He is the best               
##  doesn’t have a plan for        
##  worse president EVER 😡 thank  
##  doesn&#39;t know the meaning of    
##  such a hateful person he       
##  would shut his trap for        
##  IS NOT smarter than a          
##  has improved our economy and   
##  has done so much harm          
##  is a clown and after           
##  is evil.                       
##  is so broke that is            
##  is literally making this debate</code></pre>
<p>We can also look for more than one word at the same time:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" tabindex="-1"></a>kw_best <span class="ot">&lt;-</span> <span class="fu">kwic</span>(toks_ventura, <span class="at">pattern =</span> <span class="fu">c</span>(<span class="st">&quot;best&quot;</span>,<span class="st">&quot;worst&quot;</span>))</span>
<span id="cb132-2"><a href="#cb132-2" tabindex="-1"></a><span class="fu">head</span>(kw_best, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>## Keyword-in-context with 20 matches.                                                       
##    [text4, 17] yard class president debate at | best  |
##    [text10, 1]                                | Worst |
##    [text17, 8]               Trump! He is the | best  |
##    [text43, 6]           This is gonna be the | best  |
##   [text81, 31]  an incompetent President, the | worst |
##   [text81, 33]          President, the worst, | worst |
##   [text81, 35]              the worst, worst, | worst |
##   [text82, 11]         was totally one sided! | Worst |
##    [text86, 8]           right - Trump is the | worst |
##   [text100, 9]             !! BRAVO BRAVO THE | BEST  |
##   [text102, 4]                  Obama was the | worst |
##  [text119, 10]            he said he would do | Best  |
##  [text138, 13]               think. He is the | worst |
##  [text141, 22]           puppet could be? The | worst |
##   [text143, 6]           Trump may not be the | best  |
##  [text158, 15]              This man is a the | worst |
##   [text167, 3]                         He the | worst |
##  [text221, 34]           by far have been the | worst |
##  [text221, 36]           have been the worst, | WORST |
##  [text221, 38]              the worst, WORST, | WORST |
##                                     
##  .                                  
##  debate I’ve ever seen!             
##  president ever! Thank you          
##  show on TV in 4                    
##  , worst, worst in                  
##  , worst in history.                
##  in history.                        
##  ever! Our president kept           
##  president America ever had!        
##  PRESIDENT OF THE WORLD.            
##  president ever!!!                  
##  President ever Crybabies don&#39;t like
##  president ever                     
##  president in our time ever         
##  choice but I will choose           
##  thing that has ever happened       
##  president we had in the            
##  , WORST, WORST PRESIDENT           
##  , WORST PRESIDENT!!                
##  PRESIDENT!!!</code></pre>
<p>Alternatively, we can see what are the most common words that happen together. These are called collocations (which is a similar concept to n-grams). We want to see the most common names mentioned (first and last name).</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" tabindex="-1"></a>toks_ventura <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_ventura, <span class="at">remove_punct =</span> <span class="cn">TRUE</span>)</span>
<span id="cb134-2"><a href="#cb134-2" tabindex="-1"></a>col_ventura <span class="ot">&lt;-</span> <span class="fu">tokens_select</span>(toks_ventura, </span>
<span id="cb134-3"><a href="#cb134-3" tabindex="-1"></a>                                <span class="co"># Keep only tokens that start with a capital letter</span></span>
<span id="cb134-4"><a href="#cb134-4" tabindex="-1"></a>                                <span class="at">pattern =</span> <span class="st">&quot;^[A-Z]&quot;</span>, </span>
<span id="cb134-5"><a href="#cb134-5" tabindex="-1"></a>                                <span class="at">valuetype =</span> <span class="st">&quot;regex&quot;</span>, </span>
<span id="cb134-6"><a href="#cb134-6" tabindex="-1"></a>                                <span class="at">case_insensitive =</span> <span class="cn">FALSE</span>, </span>
<span id="cb134-7"><a href="#cb134-7" tabindex="-1"></a>                                <span class="at">padding =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb134-8"><a href="#cb134-8" tabindex="-1"></a>                  <span class="fu">textstat_collocations</span>(<span class="at">min_count =</span> <span class="dv">20</span>) <span class="co"># Minimum number of collocations to be taken into account.</span></span>
<span id="cb134-9"><a href="#cb134-9" tabindex="-1"></a><span class="fu">head</span>(col_ventura, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>##          collocation count count_nested length    lambda         z
## 1      chris wallace  1693            0      2  6.753757 128.18781
## 2    president trump   831            0      2  3.752001  84.18127
## 3          joe biden   430            0      2  3.387851  59.35890
## 4           fox news   267            0      2  8.946604  53.79136
## 5       mr president   152            0      2  4.991810  45.90814
## 6      united states   144            0      2 12.106625  36.13436
## 7       donald trump   141            0      2  4.737341  35.49434
## 8         mike pence    40            0      2  8.952702  34.74382
## 9       jo jorgensen    78            0      2 10.969527  34.45630
## 10             HE IS    43            0      2  6.211846  34.14875
## 11    vice president   343            0      2  8.415032  33.26922
## 12  democratic party    38            0      2  9.093730  31.88339
## 13     CHRIS WALLACE    38            0      2  9.634206  31.78885
## 14   PRESIDENT TRUMP    37            0      2  5.852576  30.58102
## 15          TRUMP IS    42            0      2  5.197507  30.15495
## 16       white house    46            0      2 11.318748  29.41979
## 17 african americans    35            0      2  7.749976  29.38678
## 18         JOE BIDEN    25            0      2  7.541467  28.86445
## 19           YOU ARE    27            0      2  6.656140  28.82971
## 20            IS NOT    34            0      2  5.512521  28.74916</code></pre>
<p>(The <span class="math inline">\(\lambda\)</span> score is something like the likelihood of, for example, <em>chris</em> and <em>wallace</em> happening one next to the other. For a complete explanation, you can <a href="http://web.science.mq.edu.au/~mjohnson/papers/2001/dpb-colloc01.pdf">read this paper</a>.)</p>
<p>We can also discover collocations longer than two words. In the example below we identify collocations consisting of three words.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" tabindex="-1"></a>col_ventura <span class="ot">&lt;-</span> <span class="fu">tokens_select</span>(toks_ventura, </span>
<span id="cb136-2"><a href="#cb136-2" tabindex="-1"></a>                                <span class="at">case_insensitive =</span> <span class="cn">FALSE</span>, </span>
<span id="cb136-3"><a href="#cb136-3" tabindex="-1"></a>                                <span class="at">padding =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb136-4"><a href="#cb136-4" tabindex="-1"></a>              <span class="fu">textstat_collocations</span>(<span class="at">min_count =</span> <span class="dv">100</span>, <span class="at">size =</span> <span class="dv">3</span>)</span>
<span id="cb136-5"><a href="#cb136-5" tabindex="-1"></a><span class="fu">head</span>(col_ventura, <span class="dv">20</span>)</span></code></pre></div>
<pre><code>##            collocation count count_nested length      lambda            z
## 1          know how to   115            0      3 3.098900337 11.327580933
## 2  the american people   220            0      3 2.601543689 10.158047857
## 3          this is the   158            0      3 1.393091382  9.012926334
## 4           to do with   108            0      3 4.011182538  7.217176889
## 5       this debate is   167            0      3 0.997383245  6.159091461
## 6             is not a   139            0      3 0.796582289  6.084664757
## 7     wallace needs to   172            0      3 1.634217570  4.628866454
## 8         is the worst   110            0      3 1.840591657  3.639602823
## 9         trump is the   153            0      3 0.283527984  2.554551024
## 10           is such a   107            0      3 0.776224121  2.541279959
## 11           is a joke   247            0      3 2.091736055  2.524522767
## 12      trump has done   105            0      3 0.646275113  2.285231341
## 13          trump is a   322            0      3 0.202976986  2.002649763
## 14         this is not   119            0      3 0.446372828  1.986517242
## 15      trump needs to   131            0      3 0.580848241  1.941689788
## 16         what a joke   141            0      3 2.379466544  1.672336835
## 17   the united states   132            0      3 0.738367705  1.431647428
## 18         going to be   122            0      3 1.914497779  1.348587450
## 19         is going to   210            0      3 0.101463083  0.603531369
## 20          biden is a   164            0      3 0.001198663  0.009724797</code></pre>
</div>
<div id="dictionary-approaches" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Dictionary Approaches</h2>
<p>We can extend the previous analysis by using dictionaries. You can create you own, you can use previously validates dictionaries, or you can use previously validates dictionaries that are already included with <code>tidytext</code> or <code>quanteda</code> (for sentiment analysis).</p>
<div id="sentiment-analysis" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Sentiment Analysis</h3>
<p>Let’s look at some pre-loaded sentiment dictionaries in <code>tidytext</code>:</p>
<ul>
<li><code>AFFIN</code>: measures sentiment with a numeric score between -5 and 5, and were validated in <a href="http://www2.imm.dtu.dk/pubdb/edoc/imm6006.pdf">this paper</a>.</li>
</ul>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" tabindex="-1"></a><span class="fu">get_sentiments</span>(<span class="st">&quot;afinn&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2,477 × 2
##    word       value
##    &lt;chr&gt;      &lt;dbl&gt;
##  1 abandon       -2
##  2 abandoned     -2
##  3 abandons      -2
##  4 abducted      -2
##  5 abduction     -2
##  6 abductions    -2
##  7 abhor         -3
##  8 abhorred      -3
##  9 abhorrent     -3
## 10 abhors        -3
## # ℹ 2,467 more rows</code></pre>
<ul>
<li><code>bing</code>: sentiment words found in online forums. More information <a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">here</a>.</li>
</ul>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" tabindex="-1"></a><span class="fu">get_sentiments</span>(<span class="st">&quot;bing&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 6,786 × 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 2-faces     negative 
##  2 abnormal    negative 
##  3 abolish     negative 
##  4 abominable  negative 
##  5 abominably  negative 
##  6 abominate   negative 
##  7 abomination negative 
##  8 abort       negative 
##  9 aborted     negative 
## 10 aborts      negative 
## # ℹ 6,776 more rows</code></pre>
<ul>
<li><code>nrc</code>: underpaid workers from Amazon mechanical Turk coded the emotional valence of a long list of terms, which were validated in <a href="https://arxiv.org/pdf/1308.6297.pdf">this paper</a>.</li>
</ul>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" tabindex="-1"></a><span class="fu">get_sentiments</span>(<span class="st">&quot;nrc&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 13,872 × 2
##    word        sentiment
##    &lt;chr&gt;       &lt;chr&gt;    
##  1 abacus      trust    
##  2 abandon     fear     
##  3 abandon     negative 
##  4 abandon     sadness  
##  5 abandoned   anger    
##  6 abandoned   fear     
##  7 abandoned   negative 
##  8 abandoned   sadness  
##  9 abandonment anger    
## 10 abandonment fear     
## # ℹ 13,862 more rows</code></pre>
<p>Each dictionary classifies and quantifies words in a different way. Let’s use the <code>nrc</code> sentiment dictionary to analyze our comments dataset. <code>nrc</code> classifies words as whether having <em>positive</em> or <em>negative</em> sentiment.</p>
<p>Each dictionary classifies and quantifies words in a different way. Let’s use the <code>nrc</code> sentiment dictionary to analyze our comments dataset. <code>nrc</code> classifies words as whether reflecting:</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" tabindex="-1"></a>nrc <span class="ot">&lt;-</span> <span class="fu">get_sentiments</span>(<span class="st">&quot;nrc&quot;</span>) </span>
<span id="cb144-2"><a href="#cb144-2" tabindex="-1"></a><span class="fu">table</span>(nrc<span class="sc">$</span>sentiment)</span></code></pre></div>
<pre><code>## 
##        anger anticipation      disgust         fear          joy     negative 
##         1245          837         1056         1474          687         3316 
##     positive      sadness     surprise        trust 
##         2308         1187          532         1230</code></pre>
<p>We will focus solely on <em>positive</em> or <em>negative</em> sentiment:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" tabindex="-1"></a>nrc_pos_neg <span class="ot">&lt;-</span> <span class="fu">get_sentiments</span>(<span class="st">&quot;nrc&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb146-2"><a href="#cb146-2" tabindex="-1"></a>  <span class="fu">filter</span>(sentiment <span class="sc">==</span> <span class="st">&quot;positive&quot;</span> <span class="sc">|</span> sentiment <span class="sc">==</span> <span class="st">&quot;negative&quot;</span>)</span></code></pre></div>
<pre><code>## filter: removed 8,248 rows (59%), 5,624 rows remaining</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" tabindex="-1"></a>ventura_pos_neg <span class="ot">&lt;-</span> tidy_ventura <span class="sc">%&gt;%</span></span>
<span id="cb148-2"><a href="#cb148-2" tabindex="-1"></a>  <span class="fu">left_join</span>(nrc_pos_neg)</span></code></pre></div>
<pre><code>## Joining with `by = join_by(word)`</code></pre>
<pre><code>## left_join: added one column (sentiment)</code></pre>
<pre><code>##            &gt; rows only in x   147,204</code></pre>
<pre><code>##            &gt; rows only in y  (  3,402)</code></pre>
<pre><code>##            &gt; matched rows      52,059    (includes duplicates)</code></pre>
<pre><code>##            &gt;                 =========</code></pre>
<pre><code>##            &gt; rows total       199,263</code></pre>
<p>Let’s check the top <em>positive</em> words and the top <em>negative</em> words:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" tabindex="-1"></a>ventura_pos_neg <span class="sc">%&gt;%</span></span>
<span id="cb156-2"><a href="#cb156-2" tabindex="-1"></a>  <span class="fu">group_by</span>(sentiment) <span class="sc">%&gt;%</span></span>
<span id="cb156-3"><a href="#cb156-3" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## group_by: one grouping variable (sentiment)</code></pre>
<pre><code>## count: now 14,242 rows and 3 columns, one group variable remaining (sentiment)</code></pre>
<pre><code>## # A tibble: 14,242 × 3
## # Groups:   sentiment [3]
##    sentiment word          n
##    &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
##  1 &lt;NA&gt;      trump     11676
##  2 &lt;NA&gt;      biden      7847
##  3 positive  president  4920
##  4 &lt;NA&gt;      wallace    4188
##  5 positive  debate     2693
##  6 &lt;NA&gt;      people     2591
##  7 &lt;NA&gt;      chris      2559
##  8 &lt;NA&gt;      joe        2380
##  9 &lt;NA&gt;      country    1589
## 10 &lt;NA&gt;      time       1226
## # ℹ 14,232 more rows</code></pre>
<p>Some make sense: ‘love’ is <em>positive</em>, ‘bully’ is <em>negative</em>. Some, not so much: ‘talk’ is positive? ‘joke’ is negative? Some are out of context: A ‘vice’ is negative, but THE ‘vice’-president is not (especially since presidente is considered ‘positive’, which… really?). And then ‘vote’ is both positive and negative which… what? Let’s turn a blind eye for now (but, once again, go back to Grimmer et al. Ch. 15 for best practices).</p>
<p>Are people watching different news media using different language? Let’s see what the data tells us. As always, check the unit of analysis of your dataset. In this case, each observation is a word, but we have the grouping variable of the comment (<code>text_id</code>), so we can count how many <em>positive</em> and <em>negative</em> words per comment. We will calculate a net sentiment score by subtracting the number of negative words from positive word (in each comment).</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" tabindex="-1"></a>comment_pos_neg <span class="ot">&lt;-</span> ventura_pos_neg <span class="sc">%&gt;%</span></span>
<span id="cb160-2"><a href="#cb160-2" tabindex="-1"></a>  <span class="co"># Create dummies of pos and neg for counting</span></span>
<span id="cb160-3"><a href="#cb160-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pos_dum =</span> <span class="fu">ifelse</span>(sentiment <span class="sc">==</span> <span class="st">&quot;positive&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb160-4"><a href="#cb160-4" tabindex="-1"></a>         <span class="at">neg_dum =</span> <span class="fu">ifelse</span>(sentiment <span class="sc">==</span> <span class="st">&quot;negative&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb160-5"><a href="#cb160-5" tabindex="-1"></a>  <span class="co"># Estimate total number of tokens per comment, pos , and negs</span></span>
<span id="cb160-6"><a href="#cb160-6" tabindex="-1"></a>  <span class="fu">group_by</span>(text_id) <span class="sc">%&gt;%</span></span>
<span id="cb160-7"><a href="#cb160-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">total_words =</span> <span class="fu">n</span>(),</span>
<span id="cb160-8"><a href="#cb160-8" tabindex="-1"></a>         <span class="at">total_pos =</span> <span class="fu">sum</span>(pos_dum, <span class="at">na.rm =</span> T),</span>
<span id="cb160-9"><a href="#cb160-9" tabindex="-1"></a>         <span class="at">total_neg =</span> <span class="fu">sum</span>(neg_dum, <span class="at">na.rm =</span> T)) <span class="sc">%&gt;%</span></span>
<span id="cb160-10"><a href="#cb160-10" tabindex="-1"></a>  <span class="co"># These values are aggregated at the text_id level so we can eliminate repeated text_id</span></span>
<span id="cb160-11"><a href="#cb160-11" tabindex="-1"></a>  <span class="fu">distinct</span>(text_id,<span class="at">.keep_all=</span><span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb160-12"><a href="#cb160-12" tabindex="-1"></a>  <span class="co"># Now we estimate the net sentiment score. You can change this and get a different way to measure the ratio of positive to negative</span></span>
<span id="cb160-13"><a href="#cb160-13" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">net_sent =</span> total_pos <span class="sc">-</span> total_neg) <span class="sc">%&gt;%</span></span>
<span id="cb160-14"><a href="#cb160-14" tabindex="-1"></a>  <span class="fu">ungroup</span>() </span></code></pre></div>
<pre><code>## mutate: new variable &#39;pos_dum&#39; (double) with 3 unique values and 74% NA</code></pre>
<pre><code>##         new variable &#39;neg_dum&#39; (double) with 3 unique values and 74% NA</code></pre>
<pre><code>## group_by: one grouping variable (text_id)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;total_words&#39; (integer) with 25 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;total_pos&#39; (double) with 14 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;total_neg&#39; (double) with 10 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 169,728 rows (85%), 29,535 rows remaining</code></pre>
<pre><code>## mutate (grouped): new variable &#39;net_sent&#39; (double) with 21 unique values and 0% NA</code></pre>
<pre><code>## ungroup: no grouping variables</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="#cb170-1" tabindex="-1"></a><span class="co"># Note that the `word` and `sentiment` columns are meaningless now</span></span>
<span id="cb170-2"><a href="#cb170-2" tabindex="-1"></a><span class="fu">head</span>(comment_pos_neg, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## # A tibble: 10 × 12
##    text_id id           likes debate word  sentiment pos_dum neg_dum total_words
##      &lt;int&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;       &lt;int&gt;
##  1       1 ABC News       100 abc_f… coro… &lt;NA&gt;           NA      NA          17
##  2       2 Anita Hill      61 abc_f… god   positive        1       0           4
##  3       3 Dave Garland    99 abc_f… trump &lt;NA&gt;           NA      NA           6
##  4       4 Carl Roy        47 abc_f… deba… positive        1       0           8
##  5       5 Lynda Marti…   154 abc_f… omg   &lt;NA&gt;           NA      NA          12
##  6       6 Nica Mercha…   171 abc_f… it’s  &lt;NA&gt;           NA      NA           9
##  7       7 Connie Sage     79 abc_f… happ… &lt;NA&gt;           NA      NA           7
##  8       8 Tammy Eisen     39 abc_f… expe… &lt;NA&gt;           NA      NA           4
##  9       9 Susan Weyant    53 abc_f… smart &lt;NA&gt;           NA      NA          13
## 10      10 Dana Spencer    36 abc_f… worst &lt;NA&gt;           NA      NA          15
## # ℹ 3 more variables: total_pos &lt;dbl&gt;, total_neg &lt;dbl&gt;, net_sent &lt;dbl&gt;</code></pre>
<p>Ok, now we can plot the differences:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="#cb172-1" tabindex="-1"></a>comment_pos_neg <span class="sc">%&gt;%</span></span>
<span id="cb172-2"><a href="#cb172-2" tabindex="-1"></a>    <span class="co"># Create categories</span></span>
<span id="cb172-3"><a href="#cb172-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;abc&quot;</span>), <span class="st">&quot;ABC&quot;</span>, <span class="cn">NA</span>),</span>
<span id="cb172-4"><a href="#cb172-4" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;nbc&quot;</span>), <span class="st">&quot;NBC&quot;</span>, media),</span>
<span id="cb172-5"><a href="#cb172-5" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;fox&quot;</span>), <span class="st">&quot;FOX&quot;</span>, media)) <span class="sc">%&gt;%</span></span>
<span id="cb172-6"><a href="#cb172-6" tabindex="-1"></a>  <span class="fu">group_by</span>(media) <span class="sc">%&gt;%</span></span>
<span id="cb172-7"><a href="#cb172-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">median_sent =</span> <span class="fu">mean</span>(net_sent)) <span class="sc">%&gt;%</span></span>
<span id="cb172-8"><a href="#cb172-8" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>net_sent,<span class="at">color=</span>media,<span class="at">fill=</span>media)) <span class="sc">+</span></span>
<span id="cb172-9"><a href="#cb172-9" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>,</span>
<span id="cb172-10"><a href="#cb172-10" tabindex="-1"></a>                 <span class="at">binwidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb172-11"><a href="#cb172-11" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb172-12"><a href="#cb172-12" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb172-13"><a href="#cb172-13" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>media, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb172-14"><a href="#cb172-14" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> median_sent, <span class="at">color =</span> media), <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>)<span class="sc">+</span></span>
<span id="cb172-15"><a href="#cb172-15" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb172-16"><a href="#cb172-16" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb172-17"><a href="#cb172-17" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb172-18"><a href="#cb172-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb172-19"><a href="#cb172-19" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Mean net sentiment in dashed lines.&quot;</span>)</span></code></pre></div>
<pre><code>## mutate: new variable &#39;media&#39; (character) with 3 unique values and 0% NA</code></pre>
<pre><code>## group_by: one grouping variable (media)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;median_sent&#39; (double) with 3 unique values and 0% NA</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
</div>
<div id="domain-specific-dictionaries" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Domain-Specific Dictionaries</h3>
<p>Sentiment dictionaries are common. But you can make a dictionary of whatever concept you are interested in. After all, as long as you can create a lexicon (and validate it), then you can conduct an analysis similar to the one we just carried out. This time, rather than using an off-the-shelf (sentiment) dictionary, we will create our own. Let’s try a dictionary for two topics: the economy and migration.</p>
<p>As long as the dictionary has the same shape as our <code>nrc_pos_neg</code> object, we can follow the same process that we followed for the sentiment dictionaries.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1" tabindex="-1"></a><span class="co"># First, we define the economy and migration as a concept, and then find words that signal that concept:</span></span>
<span id="cb176-2"><a href="#cb176-2" tabindex="-1"></a>economy <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(<span class="fu">c</span>(<span class="st">&quot;economy&quot;</span>,<span class="st">&quot;taxes&quot;</span>,<span class="st">&quot;inflation&quot;</span>,<span class="st">&quot;debt&quot;</span>,<span class="st">&quot;employment&quot;</span>,<span class="st">&quot;jobs&quot;</span>),<span class="st">&quot;economy&quot;</span>)</span>
<span id="cb176-3"><a href="#cb176-3" tabindex="-1"></a><span class="fu">colnames</span>(economy) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;word&quot;</span>,<span class="st">&quot;topic&quot;</span>)</span>
<span id="cb176-4"><a href="#cb176-4" tabindex="-1"></a>migration <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(<span class="fu">c</span>(<span class="st">&quot;immigrants&quot;</span>,<span class="st">&quot;border&quot;</span>,<span class="st">&quot;wall&quot;</span>,<span class="st">&quot;alien&quot;</span>,<span class="st">&quot;migrant&quot;</span>,<span class="st">&quot;visa&quot;</span>,<span class="st">&quot;daca&quot;</span>,<span class="st">&quot;dreamer&quot;</span>),<span class="st">&quot;migration&quot;</span>) </span>
<span id="cb176-5"><a href="#cb176-5" tabindex="-1"></a><span class="fu">colnames</span>(migration) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;word&quot;</span>,<span class="st">&quot;topic&quot;</span>)</span>
<span id="cb176-6"><a href="#cb176-6" tabindex="-1"></a></span>
<span id="cb176-7"><a href="#cb176-7" tabindex="-1"></a>dict <span class="ot">&lt;-</span> <span class="fu">rbind.data.frame</span>(economy,migration)</span>
<span id="cb176-8"><a href="#cb176-8" tabindex="-1"></a>dict</span></code></pre></div>
<pre><code>##          word     topic
## 1     economy   economy
## 2       taxes   economy
## 3   inflation   economy
## 4        debt   economy
## 5  employment   economy
## 6        jobs   economy
## 7  immigrants migration
## 8      border migration
## 9        wall migration
## 10      alien migration
## 11    migrant migration
## 12       visa migration
## 13       daca migration
## 14    dreamer migration</code></pre>
<p>Let’s see if we find some of these words in our comments:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="#cb178-1" tabindex="-1"></a>ventura_topic <span class="ot">&lt;-</span> tidy_ventura <span class="sc">%&gt;%</span></span>
<span id="cb178-2"><a href="#cb178-2" tabindex="-1"></a>  <span class="fu">left_join</span>(dict)</span></code></pre></div>
<pre><code>## Joining with `by = join_by(word)`
## left_join: added one column (topic)
## &gt; rows only in x 196,175
## &gt; rows only in y ( 3)
## &gt; matched rows 1,373
## &gt; =========
## &gt; rows total 197,548</code></pre>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="#cb180-1" tabindex="-1"></a>ventura_topic <span class="sc">%&gt;%</span></span>
<span id="cb180-2"><a href="#cb180-2" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(topic)) <span class="sc">%&gt;%</span></span>
<span id="cb180-3"><a href="#cb180-3" tabindex="-1"></a>  <span class="fu">group_by</span>(topic) <span class="sc">%&gt;%</span></span>
<span id="cb180-4"><a href="#cb180-4" tabindex="-1"></a>  <span class="fu">count</span>(word, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## filter: removed 196,175 rows (99%), 1,373 rows remaining
## group_by: one grouping variable (topic)
## count: now 11 rows and 3 columns, one group variable remaining (topic)</code></pre>
<pre><code>## # A tibble: 11 × 3
## # Groups:   topic [2]
##    topic     word           n
##    &lt;chr&gt;     &lt;chr&gt;      &lt;int&gt;
##  1 economy   taxes        680
##  2 economy   economy      328
##  3 economy   jobs         273
##  4 migration wall          34
##  5 economy   debt          32
##  6 migration immigrants    12
##  7 migration border         7
##  8 economy   employment     3
##  9 migration alien          2
## 10 migration daca           1
## 11 migration visa           1</code></pre>
<p>Not that many. Note that we did not stem or lemmatized our corpus, so in order to get ‘job’ <em>and</em> ‘jobs’ we must have both in our dictionary. That means that the same pre-processing step that we carry our in our corpus, we must also carry our in our dictionary.</p>
<p>If you are a bit more versed in R language, you will notice that dictionaries are actually lists. <code>quanteda</code> understand dictionaries as lists so we can actually build them as such and use its function <code>likcalike()</code> to find words in text. The added benefit is that we can use <a href="https://linuxhint.com/bash_globbing_tutorial/">glob</a> to find variations of the same word (e.g., <code>job*</code> will match ‘job’ and ‘jobs’ and ‘jobless’).</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" tabindex="-1"></a>dict <span class="ot">&lt;-</span> <span class="fu">dictionary</span>(<span class="fu">list</span>(<span class="at">economy =</span> <span class="fu">c</span>(<span class="st">&quot;econom*&quot;</span>,<span class="st">&quot;tax*&quot;</span>,<span class="st">&quot;inflation&quot;</span>,<span class="st">&quot;debt*&quot;</span>,<span class="st">&quot;employ*&quot;</span>,<span class="st">&quot;job*&quot;</span>),</span>
<span id="cb183-2"><a href="#cb183-2" tabindex="-1"></a>                        <span class="at">immigration =</span> <span class="fu">c</span>(<span class="st">&quot;immigrant*&quot;</span>,<span class="st">&quot;border&quot;</span>,<span class="st">&quot;wall&quot;</span>,<span class="st">&quot;alien&quot;</span>,<span class="st">&quot;migrant*&quot;</span>,<span class="st">&quot;visa*&quot;</span>,<span class="st">&quot;daca&quot;</span>,<span class="st">&quot;dreamer*&quot;</span>))) </span>
<span id="cb183-3"><a href="#cb183-3" tabindex="-1"></a></span>
<span id="cb183-4"><a href="#cb183-4" tabindex="-1"></a><span class="co"># liwcalike lowercases input text</span></span>
<span id="cb183-5"><a href="#cb183-5" tabindex="-1"></a>ventura_topics <span class="ot">&lt;-</span> <span class="fu">liwcalike</span>(ventura_etal_df<span class="sc">$</span>comments,</span>
<span id="cb183-6"><a href="#cb183-6" tabindex="-1"></a>                               <span class="at">dictionary =</span> dict)</span>
<span id="cb183-7"><a href="#cb183-7" tabindex="-1"></a></span>
<span id="cb183-8"><a href="#cb183-8" tabindex="-1"></a><span class="co"># liwcalike keeps the order so we can cbind them directly</span></span>
<span id="cb183-9"><a href="#cb183-9" tabindex="-1"></a>topics <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(ventura_etal_df,ventura_topics) </span>
<span id="cb183-10"><a href="#cb183-10" tabindex="-1"></a></span>
<span id="cb183-11"><a href="#cb183-11" tabindex="-1"></a><span class="co"># Look only at the comments that mention the economy and immigration</span></span>
<span id="cb183-12"><a href="#cb183-12" tabindex="-1"></a><span class="fu">head</span>(topics[topics<span class="sc">$</span>economy<span class="sc">&gt;</span><span class="dv">0</span> <span class="sc">&amp;</span> topics<span class="sc">$</span>immigration<span class="sc">&gt;</span><span class="dv">0</span>,])</span></code></pre></div>
<pre><code>##       text_id
## 4998     4999
## 6475     6477
## 8098     8113
## 12331   32211
## 14345   34225
## 19889   62164
##                                                                                                                                              comments
## 4998                           Trump is going to create jobs to finish that wall,  hows that working for ya?  I don’t see Mexico paying for it either
## 6475                           Trump is trash illegal immigrants pay more taxes than this man and you guys support this broke failure con billionaire
## 8098                                  $750.00 in taxes in two years?????   BUT HE&#39;S ALL OVER THE PLACE INSULTING IMMIGRANTS WHO PAID MORE IN TAXES!!!
## 12331    Ask\n Biden how much he will raise taxes to pay for all the things he says he\n is going to provide everyone - including illegal immigrants!
## 14345 Trump has been living the life and does not care for the hard working American...His taxes are not the only rip off...Investigate Wall Money...
## 19889                                                               Vote trump out. He needs to pay taxes too ... immigrants pay more than that thief
##                        id likes                  debate   docname Segment
## 4998         Ellen Lustic    NA abc_first_debate_manual  text4998    4998
## 6475      Kevin G Vazquez     1 abc_first_debate_manual  text6475    6475
## 8098      Prince M Dorbor     1 abc_first_debate_manual  text8098    8098
## 12331 Lynne Basista Shine     6 fox_first_debate_manual text12331   12331
## 14345          RJ Jimenez     4 fox_first_debate_manual text14345   14345
## 19889      Nicole Brennan    13 nbc_first_debate_manual text19889   19889
##            WPS WC Sixltr   Dic economy immigration AllPunc Period Comma Colon
## 4998  12.50000 25   4.00  8.00    4.00        4.00   12.00   0.00     4     0
## 6475  20.00000 20  25.00 10.00    5.00        5.00    0.00   0.00     0     0
## 8098  14.00000 28   7.14 10.71    7.14        3.57   35.71   3.57     0     0
## 12331 27.00000 27  18.52  7.41    3.70        3.70    7.41   0.00     0     0
## 14345 11.66667 35   8.57  5.71    2.86        2.86   25.71  25.71     0     0
## 19889  9.50000 19   5.26 10.53    5.26        5.26   21.05  21.05     0     0
##       SemiC QMark Exclam Dash Quote Apostro Parenth OtherP
## 4998      0  4.00   0.00  0.0  4.00    4.00       0   8.00
## 6475      0  0.00   0.00  0.0  0.00    0.00       0   0.00
## 8098      0 17.86  10.71  0.0  3.57    3.57       0  35.71
## 12331     0  0.00   3.70  3.7  0.00    0.00       0   3.70
## 14345     0  0.00   0.00  0.0  0.00    0.00       0  25.71
## 19889     0  0.00   0.00  0.0  0.00    0.00       0  21.05</code></pre>
<p>The output provides some interesting information. First, <code>economy</code> and <code>immigration</code> gives us the <em>percentage</em> of words in the text that are about the economy or immigration. In general, we would not expect too many words in a sentence to mention, for example, ‘jobs’ to argue that the sentences talks about the economy. So, any number above 0% can be counted as mentioning the economy (unless you have some theoretical grounds where 3% of words mentioning the economy &gt; 2% of words mentioning the economy). For the rest of variables:</p>
<ul>
<li><code>WPS</code>: Words per sentence.</li>
<li><code>WC</code>: Word count.</li>
<li><code>Sixltr</code>: Six-letter words (%).</li>
<li><code>Dic</code>: % of words in the dictionary.</li>
<li><code>Allpunct</code>: % of all punctuation marks.</li>
<li><code>Period</code> to <code>OtherP</code>: % of specific punctuation marks.</li>
</ul>
<p>With the information obtained, we can find which users were focused more on what topic:</p>
<pre><code>## mutate: new variable &#39;media&#39; (character) with 3 unique values and 0% NA</code></pre>
<pre><code>##         new variable &#39;economy_dum&#39; (double) with 2 unique values and 0% NA</code></pre>
<pre><code>##         new variable &#39;immigration_dum&#39; (double) with 2 unique values and 0% NA</code></pre>
<pre><code>## group_by: one grouping variable (media)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;pct_econ&#39; (double) with 3 unique values and 0% NA</code></pre>
<pre><code>##                   new variable &#39;pct_migr&#39; (double) with 3 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 29,544 rows (&gt;99%), 3 rows remaining</code></pre>
<table>
<caption>
(#tab:unnamed-chunk-50)% of mentions by topic and media outlet.
</caption>
<thead>
<tr>
<th style="text-align:left;">
media
</th>
<th style="text-align:right;">
pct_econ
</th>
<th style="text-align:right;">
pct_migr
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ABC
</td>
<td style="text-align:right;">
0.0641299
</td>
<td style="text-align:right;">
0.0030441
</td>
</tr>
<tr>
<td style="text-align:left;">
FOX
</td>
<td style="text-align:right;">
0.0856325
</td>
<td style="text-align:right;">
0.0008175
</td>
</tr>
<tr>
<td style="text-align:left;">
NBC
</td>
<td style="text-align:right;">
0.0708661
</td>
<td style="text-align:right;">
0.0018171
</td>
</tr>
</tbody>
</table>
</div>
<div id="using-pre-built-dictionaries" class="section level3" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Using Pre-Built Dictionaries</h3>
<p>So far we have seen how to apply pre-loaded dictionaries (e.g., sentiment) and our own dictionaries. What if you have a pre-built dictionary that you want to apply to your corpus? As long as the pre-built dictionary has the correct shape, we can use the techniques we have applied so far. This also means that you will need to do some data-wrangling as pre-built dictionaries will come in different shapes.</p>
<p>Let’s use the NRC Affect Intensity Lexicon (created by the same people who made the pre-loaded <code>nrc</code> sentiment dictionary). The NRC Affect Intensity Lexicon measure the intensity of an emotion in a scale of 0 (low) to 1 (high). For example, ‘defiance’ has an anger intensity of 0.51 and ‘hate’ an anger intensity of 0.83.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1" tabindex="-1"></a>intense_lex <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="at">file =</span> <span class="st">&quot;data/NRC-AffectIntensity-Lexicon.txt&quot;</span>, <span class="at">fill =</span> <span class="cn">TRUE</span>,</span>
<span id="cb192-2"><a href="#cb192-2" tabindex="-1"></a>                          <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb192-3"><a href="#cb192-3" tabindex="-1"></a><span class="fu">head</span>(intense_lex)</span></code></pre></div>
<pre><code>##         term score AffectDimension
## 1   outraged 0.964           anger
## 2  brutality 0.959           anger
## 3     hatred 0.953           anger
## 4    hateful 0.940           anger
## 5  terrorize 0.939           anger
## 6 infuriated 0.938           anger</code></pre>
<p>This is more than a dictionary, and the best use of it to include the intensity of each word to obtain more variation in our analysis of the text (e.g., rather than showing anger-no anger, we can analyze a degree of anger). We will use the <code>tidytext</code> approach to analyze the degrees of ‘joy’ in our corpus.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="#cb194-1" tabindex="-1"></a>joy_lex <span class="ot">&lt;-</span> intense_lex <span class="sc">%&gt;%</span></span>
<span id="cb194-2"><a href="#cb194-2" tabindex="-1"></a>  <span class="fu">filter</span>(AffectDimension<span class="sc">==</span><span class="st">&quot;joy&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb194-3"><a href="#cb194-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">word=</span>term) <span class="sc">%&gt;%</span></span>
<span id="cb194-4"><a href="#cb194-4" tabindex="-1"></a>  <span class="fu">select</span>(word,AffectDimension,score)</span></code></pre></div>
<pre><code>## filter: removed 4,546 rows (78%), 1,268 rows remaining</code></pre>
<pre><code>## mutate: new variable &#39;word&#39; (character) with 1,268 unique values and 0% NA</code></pre>
<pre><code>## select: dropped one variable (term)</code></pre>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="#cb198-1" tabindex="-1"></a>ventura_joy <span class="ot">&lt;-</span> tidy_ventura <span class="sc">%&gt;%</span></span>
<span id="cb198-2"><a href="#cb198-2" tabindex="-1"></a>  <span class="fu">left_join</span>(joy_lex) <span class="sc">%&gt;%</span></span>
<span id="cb198-3"><a href="#cb198-3" tabindex="-1"></a>  <span class="do">## Most of the comments have no joy words so we will change these NAs to 0 but this is an ad-hoc decision. This decision must be theoretically motivated and justified</span></span>
<span id="cb198-4"><a href="#cb198-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">score =</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(score),<span class="dv">0</span>,score))</span></code></pre></div>
<pre><code>## Joining with `by = join_by(word)`</code></pre>
<pre><code>## left_join: added 2 columns (AffectDimension, score)</code></pre>
<pre><code>##            &gt; rows only in x   184,943</code></pre>
<pre><code>##            &gt; rows only in y  (    769)</code></pre>
<pre><code>##            &gt; matched rows      12,605</code></pre>
<pre><code>##            &gt;                 =========</code></pre>
<pre><code>##            &gt; rows total       197,548</code></pre>
<pre><code>## mutate: changed 184,943 values (94%) of &#39;score&#39; (184943 fewer NA)</code></pre>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="#cb207-1" tabindex="-1"></a><span class="fu">head</span>(ventura_joy[ventura_joy<span class="sc">$</span>score<span class="sc">&gt;</span><span class="dv">0</span>,])</span></code></pre></div>
<pre><code>##    text_id           id likes                  debate       word
## 18       2   Anita Hill    61 abc_first_debate_manual        god
## 19       2   Anita Hill    61 abc_first_debate_manual      bless
## 23       3 Dave Garland    99 abc_first_debate_manual     living
## 30       4     Carl Roy    47 abc_first_debate_manual      laugh
## 64       8  Tammy Eisen    39 abc_first_debate_manual experience
## 65       8  Tammy Eisen    39 abc_first_debate_manual      share
##    AffectDimension score
## 18             joy 0.545
## 19             joy 0.561
## 23             joy 0.312
## 30             joy 0.891
## 64             joy 0.375
## 65             joy 0.438</code></pre>
<p>Now, we can see the relationship between <code>likes</code> and <code>joy</code>:</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="#cb209-1" tabindex="-1"></a><span class="fu">library</span>(MASS) <span class="co"># To add the negative binomial fitted line</span></span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidylog&#39;:
## 
##     select</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="#cb213-1" tabindex="-1"></a>ventura_joy <span class="sc">%&gt;%</span></span>
<span id="cb213-2"><a href="#cb213-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;abc&quot;</span>), <span class="st">&quot;ABC&quot;</span>, <span class="cn">NA</span>),</span>
<span id="cb213-3"><a href="#cb213-3" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;nbc&quot;</span>), <span class="st">&quot;NBC&quot;</span>, media),</span>
<span id="cb213-4"><a href="#cb213-4" tabindex="-1"></a>         <span class="at">media =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(debate, <span class="st">&quot;fox&quot;</span>), <span class="st">&quot;FOX&quot;</span>, media)) <span class="sc">%&gt;%</span></span>
<span id="cb213-5"><a href="#cb213-5" tabindex="-1"></a>  <span class="co"># Calculate mean joy in each comment</span></span>
<span id="cb213-6"><a href="#cb213-6" tabindex="-1"></a>  <span class="fu">group_by</span>(text_id) <span class="sc">%&gt;%</span></span>
<span id="cb213-7"><a href="#cb213-7" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mean_joy =</span> <span class="fu">mean</span>(score)) <span class="sc">%&gt;%</span></span>
<span id="cb213-8"><a href="#cb213-8" tabindex="-1"></a>  <span class="fu">distinct</span>(text_id,mean_joy,likes,media) <span class="sc">%&gt;%</span></span>
<span id="cb213-9"><a href="#cb213-9" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span></span>
<span id="cb213-10"><a href="#cb213-10" tabindex="-1"></a>  <span class="co"># Let&#39;s only look at comments that had SOME joy in them</span></span>
<span id="cb213-11"><a href="#cb213-11" tabindex="-1"></a>  <span class="fu">filter</span>(mean_joy <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb213-12"><a href="#cb213-12" tabindex="-1"></a>  <span class="co"># Remove the ones people like too much</span></span>
<span id="cb213-13"><a href="#cb213-13" tabindex="-1"></a>  <span class="fu">filter</span>(likes <span class="sc">&lt;</span> <span class="dv">26</span>) <span class="sc">%&gt;%</span></span>
<span id="cb213-14"><a href="#cb213-14" tabindex="-1"></a>  <span class="co"># Plot</span></span>
<span id="cb213-15"><a href="#cb213-15" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>mean_joy,<span class="at">y=</span>likes,<span class="at">color=</span>media,<span class="at">fill=</span>media)) <span class="sc">+</span></span>
<span id="cb213-16"><a href="#cb213-16" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb213-17"><a href="#cb213-17" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;glm.nb&quot;</span>) <span class="sc">+</span></span>
<span id="cb213-18"><a href="#cb213-18" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb213-19"><a href="#cb213-19" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb213-20"><a href="#cb213-20" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>media, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb213-21"><a href="#cb213-21" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb213-22"><a href="#cb213-22" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;none&quot;</span>) <span class="sc">+</span></span>
<span id="cb213-23"><a href="#cb213-23" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Mean Joy&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Likes&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<pre><code>## mutate: new variable &#39;media&#39; (character) with 3 unique values and 0% NA</code></pre>
<pre><code>## group_by: one grouping variable (text_id)</code></pre>
<pre><code>## mutate (grouped): new variable &#39;mean_joy&#39; (double) with 3,118 unique values and 0% NA</code></pre>
<pre><code>## distinct (grouped): removed 168,013 rows (85%), 29,535 rows remaining</code></pre>
<pre><code>## ungroup: no grouping variables</code></pre>
<pre><code>## filter: removed 20,355 rows (69%), 9,180 rows remaining</code></pre>
<pre><code>## filter: removed 2,518 rows (27%), 6,662 rows remaining</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>Finally, for the sake of showing the process, I will write the code to load the dictionary using <code>quanteda</code>, but note that this approach loses all the intensity information.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="#cb222-1" tabindex="-1"></a>affect_dict <span class="ot">&lt;-</span> <span class="fu">dictionary</span>(<span class="fu">list</span>(<span class="at">anger =</span> intense_lex<span class="sc">$</span>term[intense_lex<span class="sc">$</span>AffectDimension<span class="sc">==</span><span class="st">&quot;anger&quot;</span>],</span>
<span id="cb222-2"><a href="#cb222-2" tabindex="-1"></a>                        <span class="at">fear =</span> intense_lex<span class="sc">$</span>term[intense_lex<span class="sc">$</span>AffectDimension<span class="sc">==</span><span class="st">&quot;fear&quot;</span>],</span>
<span id="cb222-3"><a href="#cb222-3" tabindex="-1"></a>                        <span class="at">joy =</span> intense_lex<span class="sc">$</span>term[intense_lex<span class="sc">$</span>AffectDimension<span class="sc">==</span><span class="st">&quot;joy&quot;</span>],</span>
<span id="cb222-4"><a href="#cb222-4" tabindex="-1"></a>                        <span class="at">sadness =</span> intense_lex<span class="sc">$</span>term[intense_lex<span class="sc">$</span>AffectDimension<span class="sc">==</span><span class="st">&quot;sadness&quot;</span>])) </span>
<span id="cb222-5"><a href="#cb222-5" tabindex="-1"></a></span>
<span id="cb222-6"><a href="#cb222-6" tabindex="-1"></a>ventura_affect <span class="ot">&lt;-</span> <span class="fu">liwcalike</span>(ventura_etal_df<span class="sc">$</span>comments,</span>
<span id="cb222-7"><a href="#cb222-7" tabindex="-1"></a>                               <span class="at">dictionary =</span> affect_dict)</span>
<span id="cb222-8"><a href="#cb222-8" tabindex="-1"></a></span>
<span id="cb222-9"><a href="#cb222-9" tabindex="-1"></a><span class="co"># liwcalike keeps the order so we can cbind them directly</span></span>
<span id="cb222-10"><a href="#cb222-10" tabindex="-1"></a>affect <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(ventura_etal_df,ventura_affect) </span>
<span id="cb222-11"><a href="#cb222-11" tabindex="-1"></a></span>
<span id="cb222-12"><a href="#cb222-12" tabindex="-1"></a><span class="co"># Look only at the comments that have anger and fear</span></span>
<span id="cb222-13"><a href="#cb222-13" tabindex="-1"></a><span class="fu">head</span>(affect[affect<span class="sc">$</span>anger<span class="sc">&gt;</span><span class="dv">0</span> <span class="sc">&amp;</span> affect<span class="sc">$</span>fear<span class="sc">&gt;</span><span class="dv">0</span>,])</span></code></pre></div>
<pre><code>##    text_id
## 3        3
## 7        7
## 9        9
## 11      11
## 12      12
## 23      23
##                                                                                                                                                                                       comments
## 3                                                                               Trump  is  a  living  disaster!    What  an embarrassment  to  all  human  beings!    The  man  is  dangerous!
## 7                                                                                  What happened to the days when it was a debate not a bully session! I am so ashamed of this administration!
## 9  ......\n a smart president?  A thief, a con man, and a liar that has taken tax \npayers money to his own properties.  A liar that knew the magnitude of \nthe virus and did not address it.
## 11                             with\n the usa having such a bad opiate problem then trump brings up about \nbidens son is the most disgraceful thing any human being could do...vote\n him out
## 12   Trump’s\n only recourse in the debate is to demean his opponent and talk about \nwhat a great man he, himself is. Turn his mic off when it’s not his turn\n to speak. Nothing but babble!
## 23                                                                                           Trump such a hateful person he has no moral or respect in a debate he blames everyone except him.
##              id likes                  debate docname Segment       WPS WC
## 3  Dave Garland    99 abc_first_debate_manual   text3       3  6.333333 19
## 7   Connie Sage    79 abc_first_debate_manual   text7       7 11.500000 23
## 9  Susan Weyant    53 abc_first_debate_manual   text9       9 15.333333 46
## 11  Lynn Kohler    41 abc_first_debate_manual  text11      11 32.000000 32
## 12     Jim Lape    28 abc_first_debate_manual  text12      12 13.000000 39
## 23   Joe Sonera    65 abc_first_debate_manual  text23      23 20.000000 20
##    Sixltr   Dic anger  fear  joy sadness AllPunc Period Comma Colon SemiC QMark
## 3   15.79 36.84  5.26 15.79 5.26   10.53   15.79   0.00  0.00     0     0  0.00
## 7   17.39 17.39  4.35  4.35 0.00    8.70    8.70   0.00  0.00     0     0  0.00
## 9    8.70 13.04  4.35  2.17 2.17    4.35   23.91  17.39  4.35     0     0  2.17
## 11   6.25 28.12  9.38  6.25 3.12    9.38    9.38   9.38  0.00     0     0  0.00
## 12  12.82  5.13  2.56  2.56 0.00    0.00   15.38   5.13  2.56     0     0  0.00
## 23  15.00 25.00 10.00  5.00 5.00    5.00    5.00   5.00  0.00     0     0  0.00
##    Exclam Dash Quote Apostro Parenth OtherP
## 3   15.79    0  0.00    0.00       0  15.79
## 7    8.70    0  0.00    0.00       0   8.70
## 9    0.00    0  0.00    0.00       0  23.91
## 11   0.00    0  0.00    0.00       0   9.38
## 12   2.56    0  5.13    5.13       0  10.26
## 23   0.00    0  0.00    0.00       0   5.00</code></pre>
</div>
</div>
<div id="homework" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Homework</h2>
<ol style="list-style-type: decimal">
<li>Replicate the results from the left-most column of Figure 3 in Ventura et al. (2021)</li>
<li>Look at the keywords in context for <em>Biden</em> in the <code>ventura_etal_df</code> dataset, and compare the results with the same data, but pre-processed (i.e., lower-case, remove stopwords, etc.). Which provides more information about the context in which <em>Biden</em> appears in the comments?</li>
<li>Do a different collocation approach with the <code>ventura_etal_df</code> dataset, but pre-process the data (i.e., lower-case, remove stopwords, etc.). Which approach (pre-processed and not pre-processed) provides a better picture of the corpus or of the collocations you found?</li>
<li>Compare the <strong>positive</strong> sentiments of comments mentioning <em>trump</em> and comments mentioning <em>biden</em> obtained using <code>bing</code> and <code>afinn</code>. Note that <code>afinn</code> gives a numeric value, so you will need to choose a threshold to determine <strong>positive</strong> sentiment.</li>
<li>Using <code>bing</code>, compare the sentiment of comments mentioning <em>trump</em> and comments mentioning <em>biden</em> using different metrics (e.g., Young and Soroka 2012, Martins and Baumard 2020, Ventura et al. 2021).</li>
<li>Create your own domain-specific dictionary and apply it to the <code>ventura_etal_df</code> dataset. Show the limitation of your dictionary (e.g., false positives) and comment on how much of a problem this would be if you wanted to conduct an analysis of this corpus.</li>
</ol>
<!--chapter:end:03-week3.Rmd-->
</div>
</div>
<div id="week-4-complexity-and-similarity" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Week 4: Complexity and Similarity</h1>
<div id="slides-3" class="section level2 unnumbered">
<h2 class="unnumbered">Slides</h2>
<ul>
<li>5 Complexity and Similarity (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/5%20Complexity%20and%20Similarity.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup-3" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Setup</h2>
<p>As always, we first load the packages that we’ll be using:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="#cb224-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># for wrangling data</span></span>
<span id="cb224-2"><a href="#cb224-2" tabindex="-1"></a><span class="fu">library</span>(tidylog) <span class="co"># to know what we are wrangling</span></span>
<span id="cb224-3"><a href="#cb224-3" tabindex="-1"></a><span class="fu">library</span>(tidytext) <span class="co"># for &#39;tidy&#39; manipulation of text data</span></span>
<span id="cb224-4"><a href="#cb224-4" tabindex="-1"></a><span class="fu">library</span>(textdata) <span class="co"># text datasets</span></span>
<span id="cb224-5"><a href="#cb224-5" tabindex="-1"></a><span class="fu">library</span>(quanteda) <span class="co"># tokenization power house</span></span>
<span id="cb224-6"><a href="#cb224-6" tabindex="-1"></a><span class="fu">library</span>(quanteda.textstats)</span>
<span id="cb224-7"><a href="#cb224-7" tabindex="-1"></a><span class="fu">library</span>(quanteda.textplots)</span>
<span id="cb224-8"><a href="#cb224-8" tabindex="-1"></a><span class="fu">library</span>(wesanderson) <span class="co"># to prettify</span></span>
<span id="cb224-9"><a href="#cb224-9" tabindex="-1"></a><span class="fu">library</span>(stringdist) <span class="co"># measure string distance</span></span>
<span id="cb224-10"><a href="#cb224-10" tabindex="-1"></a><span class="fu">library</span>(reshape2)</span></code></pre></div>
</div>
<div id="replicating-the-lecture" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Replicating the Lecture</h2>
<p>In this weeks <a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/5%20Complexity%20and%20Similarity.pptx">lecture</a>, we learned about similarity and complexity measures at the word- and document-level. We will follow the same order from the lecture slides.</p>
</div>
<div id="comparing-text" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Comparing Text</h2>
<p>There are different ways to compare text, depending on the unit of analysis:
- Character-level comparisons
- Token-level comparison</p>
<div id="character-level-comparisons" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Character-Level Comparisons:</h3>
<p>Let’s start by using character-level comparisons tools to evaluate two documents (in this case, two statements made by me on any given Ontario winter day):</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="#cb225-1" tabindex="-1"></a>doc_1 <span class="ot">&lt;-</span> <span class="st">&quot;By the professor’s standards, the weather in Ontario during the Winter term is miserable.&quot;</span></span>
<span id="cb225-2"><a href="#cb225-2" tabindex="-1"></a>doc_2 <span class="ot">&lt;-</span> <span class="st">&quot;By the professor’s high standards, the weather in London during the Winter term is depressive.&quot;</span></span></code></pre></div>
<p>From <code>?stringdist</code>, we know that “the longest common substring distance is defined as the number of unpaired characters. The distance is equivalent to the edit distance allowing only deletions and insertions, each with weight one.” We also learned about <em>Levenshtein</em> distance and <em>Jaro</em> distance. We can easily implement these using the <code>stringdist</code> function:</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="#cb226-1" tabindex="-1"></a><span class="fu">stringdist</span>(doc_1,doc_2,<span class="at">method =</span> <span class="st">&quot;lcs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 27</code></pre>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="#cb228-1" tabindex="-1"></a><span class="fu">stringdist</span>(doc_1,doc_2,<span class="at">method =</span> <span class="st">&quot;lv&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 20</code></pre>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="#cb230-1" tabindex="-1"></a><span class="fu">stringdist</span>(doc_1,doc_2,<span class="at">method =</span> <span class="st">&quot;jw&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.1768849</code></pre>
<p>Each distance provides slightly different information about the relation between both documents. There are other distances that the <code>stringdist</code> function can compute. If this is something that interests you, there is more information about each measure in this <a href="https://www.dcc.uchile.cl/TR/1999/TR_DCC-1999-005.pdf_a">paper</a>.</p>
<p>Have <em>I</em> ever used these measure in my own work? Actually, yes. When combining a corpus of legislative speeches from the Ecuadorian Congress with a data set of Ecuadorian legislators, I matched the names of both data set using <em>fuzzy matching</em> or matching names that were closely related (even if they were not a perfect match). Here is an example of the code:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="#cb232-1" tabindex="-1"></a><span class="co"># I have a dataframe df_a and df_b. I want to match names from b to a. I run a loop that goes through all the names b and gives a Jaro distance score for a name in a. I assume that the names are a match when the Jaro distance score is highest AND it is above a threshold (0.4).</span></span>
<span id="cb232-2"><a href="#cb232-2" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(df_a<span class="sc">$</span>CANDIDATO_to_MATCH)){</span>
<span id="cb232-3"><a href="#cb232-3" tabindex="-1"></a>  score_temp <span class="ot">&lt;-</span> <span class="fu">stringdist</span>(df_a<span class="sc">$</span>CANDIDATO_to_MATCH[i],df_b<span class="sc">$</span>CANDIDATO_MERGE,<span class="at">method =</span> <span class="st">&quot;jw&quot;</span>) </span>
<span id="cb232-4"><a href="#cb232-4" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">max</span>(score_temp)<span class="sc">&gt;</span>.<span class="dv">4</span> <span class="sc">&amp;</span> <span class="fu">length</span>(<span class="fu">which</span>(score_temp <span class="sc">==</span> <span class="fu">max</span>(score_temp)))<span class="sc">&lt;</span><span class="dv">2</span>){</span>
<span id="cb232-5"><a href="#cb232-5" tabindex="-1"></a>    df_a<span class="sc">$</span>CANDIDATO_MERGE[i] <span class="ot">&lt;-</span> df_b<span class="sc">$</span>CANDIDATO_MERGE[<span class="fu">which</span>(score_temp <span class="sc">==</span> <span class="fu">max</span>(score_temp))]}</span>
<span id="cb232-6"><a href="#cb232-6" tabindex="-1"></a>  <span class="cf">else</span>{</span>
<span id="cb232-7"><a href="#cb232-7" tabindex="-1"></a>    df_a<span class="sc">$</span>CANDIDATO_MERGE[i] <span class="ot">&lt;-</span> <span class="cn">NA</span>}</span>
<span id="cb232-8"><a href="#cb232-8" tabindex="-1"></a>}</span></code></pre></div>
<p>It saved me a lot of time. I still needed to <strong>validate</strong> all the matches and manually match the unmatched names.</p>
</div>
<div id="token-level-comparisons" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Token-Level Comparisons:</h3>
<p>To compare documents at the token level (i.e., how many and how often to token coincide), we can think of each document as a row/column in a matrix and each word as a row/column in a matrix. We call these matrices, document-feature matrices or <code>dfm</code>. To do that using <code>quanteda</code> we first need to tokenize our corpus:</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="#cb233-1" tabindex="-1"></a>doc_3 <span class="ot">&lt;-</span> <span class="st">&quot;The professor has strong evidence that the weather in London (Ontario) is miserable and depressive.&quot;</span></span>
<span id="cb233-2"><a href="#cb233-2" tabindex="-1"></a></span>
<span id="cb233-3"><a href="#cb233-3" tabindex="-1"></a>docs_toks <span class="ot">&lt;-</span> <span class="fu">tokens</span>(<span class="fu">rbind</span>(doc_1,doc_2,doc_3),</span>
<span id="cb233-4"><a href="#cb233-4" tabindex="-1"></a>                            <span class="at">remove_punct =</span> T)</span>
<span id="cb233-5"><a href="#cb233-5" tabindex="-1"></a>docs_toks <span class="ot">&lt;-</span> <span class="fu">tokens_remove</span>(docs_toks,</span>
<span id="cb233-6"><a href="#cb233-6" tabindex="-1"></a>                           <span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">&quot;en&quot;</span>))</span>
<span id="cb233-7"><a href="#cb233-7" tabindex="-1"></a>docs_toks</span></code></pre></div>
<pre><code>## Tokens consisting of 3 documents.
## text1 :
## [1] &quot;professor’s&quot; &quot;standards&quot;   &quot;weather&quot;     &quot;Ontario&quot;     &quot;Winter&quot;     
## [6] &quot;term&quot;        &quot;miserable&quot;  
## 
## text2 :
## [1] &quot;professor’s&quot; &quot;high&quot;        &quot;standards&quot;   &quot;weather&quot;     &quot;London&quot;     
## [6] &quot;Winter&quot;      &quot;term&quot;        &quot;depressive&quot; 
## 
## text3 :
## [1] &quot;professor&quot;  &quot;strong&quot;     &quot;evidence&quot;   &quot;weather&quot;    &quot;London&quot;    
## [6] &quot;Ontario&quot;    &quot;miserable&quot;  &quot;depressive&quot;</code></pre>
<p>Now we are ready to create a <code>dfm</code>:</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="#cb235-1" tabindex="-1"></a>docs_dmf <span class="ot">&lt;-</span> <span class="fu">dfm</span>(docs_toks)</span>
<span id="cb235-2"><a href="#cb235-2" tabindex="-1"></a>docs_dmf</span></code></pre></div>
<pre><code>## Document-feature matrix of: 3 documents, 13 features (41.03% sparse) and 0 docvars.
##        features
## docs    professor’s standards weather ontario winter term miserable high london
##   text1           1         1       1       1      1    1         1    0      0
##   text2           1         1       1       0      1    1         0    1      1
##   text3           0         0       1       1      0    0         1    0      1
##        features
## docs    depressive
##   text1          0
##   text2          1
##   text3          1
## [ reached max_nfeat ... 3 more features ]</code></pre>
<p>Just a matrix (are really sparse matrix which becomes even more sparse as the corpus grows). Now we can measure the similarity or distance between these two text. The most straightforward way is to just correlate the occurrence of 1s and 0s across texts. An intuitive way to see this is by transposing the <code>dfm</code> and presenting it in a shape that you are more familiar with:</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="#cb237-1" tabindex="-1"></a>dfm_df <span class="ot">&lt;-</span> <span class="fu">convert</span>(docs_dmf, <span class="at">to =</span> <span class="st">&quot;matrix&quot;</span>)</span>
<span id="cb237-2"><a href="#cb237-2" tabindex="-1"></a>dfm_df_t <span class="ot">&lt;-</span> <span class="fu">t</span>(dfm_df)</span>
<span id="cb237-3"><a href="#cb237-3" tabindex="-1"></a>dfm_df_t</span></code></pre></div>
<pre><code>##              docs
## features      text1 text2 text3
##   professor’s     1     1     0
##   standards       1     1     0
##   weather         1     1     1
##   ontario         1     0     1
##   winter          1     1     0
##   term            1     1     0
##   miserable       1     0     1
##   high            0     1     0
##   london          0     1     1
##   depressive      0     1     1
##   professor       0     0     1
##   strong          0     0     1
##   evidence        0     0     1</code></pre>
<p>Ok, now we just use a simple correlation test:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="#cb239-1" tabindex="-1"></a><span class="fu">cor</span>(dfm_df_t[,<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)])</span></code></pre></div>
<pre><code>##            text1      text2      text3
## text1  1.0000000  0.2195775 -0.4147575
## text2  0.2195775  1.0000000 -0.6250000
## text3 -0.4147575 -0.6250000  1.0000000</code></pre>
<p>From this we can see that text1 is more highly correlated to text2 than to text 3. Alternatively, we can use the built-in functions in <code>quanteda</code> to obtain similar results without having to transform our <code>dfm</code>:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="#cb241-1" tabindex="-1"></a><span class="fu">textstat_simil</span>(docs_dmf, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;correlation&quot;</span>)</span></code></pre></div>
<pre><code>## textstat_simil object; method = &quot;correlation&quot;
##        text1  text2  text3
## text1  1.000  0.220 -0.415
## text2  0.220  1.000 -0.625
## text3 -0.415 -0.625  1.000</code></pre>
<p>We can use <code>textstat_simil</code> for the a whole bunch of similarity/distance methods:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="#cb243-1" tabindex="-1"></a><span class="fu">textstat_simil</span>(docs_dmf, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;cosine&quot;</span>)</span></code></pre></div>
<pre><code>## textstat_simil object; method = &quot;cosine&quot;
##       text1 text2 text3
## text1 1.000 0.668 0.401
## text2 0.668 1.000 0.375
## text3 0.401 0.375 1.000</code></pre>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="#cb245-1" tabindex="-1"></a><span class="fu">textstat_simil</span>(docs_dmf, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;jaccard&quot;</span>)</span></code></pre></div>
<pre><code>## textstat_simil object; method = &quot;jaccard&quot;
##       text1 text2 text3
## text1  1.00 0.500 0.250
## text2  0.50 1.000 0.231
## text3  0.25 0.231 1.000</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="#cb247-1" tabindex="-1"></a><span class="fu">textstat_dist</span>(docs_dmf, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>)</span></code></pre></div>
<pre><code>## textstat_dist object; method = &quot;euclidean&quot;
##       text1 text2 text3
## text1     0  2.24  3.00
## text2  2.24     0  3.16
## text3  3.00  3.16     0</code></pre>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="#cb249-1" tabindex="-1"></a><span class="fu">textstat_dist</span>(docs_dmf, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;manhattan&quot;</span>)</span></code></pre></div>
<pre><code>## textstat_dist object; method = &quot;manhattan&quot;
##       text1 text2 text3
## text1     0     5     9
## text2     5     0    10
## text3     9    10     0</code></pre>
<p>We can also present these matrices as nice plots:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="#cb251-1" tabindex="-1"></a>cos_sim_doc <span class="ot">&lt;-</span> <span class="fu">textstat_simil</span>(docs_dmf, <span class="at">margin =</span> <span class="st">&quot;documents&quot;</span>, <span class="at">method =</span> <span class="st">&quot;cosine&quot;</span>)</span>
<span id="cb251-2"><a href="#cb251-2" tabindex="-1"></a>cos_sim_doc <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(cos_sim_doc)</span>
<span id="cb251-3"><a href="#cb251-3" tabindex="-1"></a>  </span>
<span id="cb251-4"><a href="#cb251-4" tabindex="-1"></a><span class="co"># We do this to use ggplot</span></span>
<span id="cb251-5"><a href="#cb251-5" tabindex="-1"></a>cos_sim_doc_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(cos_sim_doc)</span>
<span id="cb251-6"><a href="#cb251-6" tabindex="-1"></a>cos_sim_doc_df <span class="sc">%&gt;%</span></span>
<span id="cb251-7"><a href="#cb251-7" tabindex="-1"></a>    <span class="fu">rownames_to_column</span>() <span class="sc">%&gt;%</span></span>
<span id="cb251-8"><a href="#cb251-8" tabindex="-1"></a>  <span class="co"># ggplot prefers </span></span>
<span id="cb251-9"><a href="#cb251-9" tabindex="-1"></a>    <span class="fu">melt</span>() <span class="sc">%&gt;%</span></span>
<span id="cb251-10"><a href="#cb251-10" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">as.character</span>(variable),<span class="at">y =</span> <span class="fu">as.character</span>(rowname), <span class="at">col =</span> value)) <span class="sc">+</span></span>
<span id="cb251-11"><a href="#cb251-11" tabindex="-1"></a>    <span class="fu">geom_tile</span>(<span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">fill=</span><span class="st">&quot;white&quot;</span>) <span class="sc">+</span> </span>
<span id="cb251-12"><a href="#cb251-12" tabindex="-1"></a>    <span class="co"># coord_fixed() +</span></span>
<span id="cb251-13"><a href="#cb251-13" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;&quot;</span>,<span class="at">y=</span><span class="st">&quot;&quot;</span>,<span class="at">col =</span> <span class="st">&quot;Cosine Sim&quot;</span>, <span class="at">fill=</span><span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb251-14"><a href="#cb251-14" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb251-15"><a href="#cb251-15" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(</span>
<span id="cb251-16"><a href="#cb251-16" tabindex="-1"></a>      <span class="at">angle =</span> <span class="dv">90</span>,</span>
<span id="cb251-17"><a href="#cb251-17" tabindex="-1"></a>      <span class="at">vjust =</span> <span class="dv">1</span>,</span>
<span id="cb251-18"><a href="#cb251-18" tabindex="-1"></a>      <span class="at">hjust =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb251-19"><a href="#cb251-19" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">size =</span> value)) <span class="sc">+</span> </span>
<span id="cb251-20"><a href="#cb251-20" tabindex="-1"></a>    <span class="fu">scale_size</span>(<span class="at">guide =</span> <span class="st">&#39;none&#39;</span>) <span class="sc">+</span></span>
<span id="cb251-21"><a href="#cb251-21" tabindex="-1"></a>    <span class="fu">scale_color_gradient2</span>(<span class="at">mid=</span><span class="st">&quot;#A63446&quot;</span>,<span class="at">low=</span> <span class="st">&quot;#A63446&quot;</span>,<span class="at">high=</span><span class="st">&quot;#0C6291&quot;</span>) <span class="sc">+</span></span>
<span id="cb251-22"><a href="#cb251-22" tabindex="-1"></a>    <span class="fu">scale_x_discrete</span>(<span class="at">expand=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb251-23"><a href="#cb251-23" tabindex="-1"></a>    <span class="fu">scale_y_discrete</span>(<span class="at">expand=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>))</span></code></pre></div>
<pre><code>## Using rowname as id variables</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>Noise!</p>
</div>
</div>
<div id="complexity" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Complexity</h2>
<p>From this week’s lecture (and one of the readings) we know that another way of analyzing text is by computing its complexity. In <em>Schoonvelde et al. (2019) - Liberals lecture, conservatives communicate: Analyzing complexity and ideology in 381,609 political speeches</em>, the authors use Flesch’s Reading Ease Score as a measure of “complexity” or readability of a text (see <code>??textstat_readability</code> for the formula and other readability measures). Flesch’s Reading Ease Score ranges from 0 to 100, where higher values suggest less complex/more readable text (e.g., a score between 90 and 100 is a text that can be understood by a 5th grade; a score between 0 and 30 is a text that can be understood by a college graduate and professional). It obtains the score taking into consideration the average length of a sentence, the number of words, and the number of syllables.</p>
<p>Let’s apply the readability score to some open-ended questions from the 2020 ANES survey, and see how these correlate to the characteristics of the respondents.</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="#cb253-1" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/anes_sample.Rdata&quot;</span>)</span>
<span id="cb253-2"><a href="#cb253-2" tabindex="-1"></a><span class="fu">head</span>(open_srvy)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 9
##   V200001 like_dem_pres   dislike_dem_pres like_rep_pres dislike_rep_pres income
##     &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt;             &lt;int&gt;
## 1  200015 &lt;NA&gt;            nothing about s… belife in a … &lt;NA&gt;                 21
## 2  200022 &lt;NA&gt;            He wants to tak… &lt;NA&gt;          &lt;NA&gt;                 13
## 3  200039 He is not Dona… &lt;NA&gt;             &lt;NA&gt;          He is a racist,…     17
## 4  200046 he look honest… &lt;NA&gt;             &lt;NA&gt;          racism, equalit…      7
## 5  200053 &lt;NA&gt;            Open borders, l… No war, No o… Ridiculous Covi…     22
## 6  200060 he is NOT Dona… &lt;NA&gt;             &lt;NA&gt;          He is a crimina…      3
## # ℹ 3 more variables: pid &lt;int&gt;, edu &lt;int&gt;, age &lt;int&gt;</code></pre>
<p>We have open-ended survey questions that ask respondents what they like and dislike about the Democratic (Joe Biden) and Republican (Donald Trump) 2020 US presidential candidates before the election. Note that survey respondent could opt out of the question and are given an <em>NA</em>.</p>
<p>Let’s check:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="#cb255-1" tabindex="-1"></a>read_like_dem_pres <span class="ot">&lt;-</span> <span class="fu">textstat_readability</span>(open_srvy<span class="sc">$</span>like_dem_pres,<span class="at">measure =</span> <span class="st">&quot;Flesch&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: NA is replaced by empty string</code></pre>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="#cb257-1" tabindex="-1"></a>open_srvy<span class="sc">$</span>read_like_dem_pres <span class="ot">&lt;-</span> read_like_dem_pres<span class="sc">$</span>Flesch</span>
<span id="cb257-2"><a href="#cb257-2" tabindex="-1"></a><span class="fu">head</span>(open_srvy[,<span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">10</span>)],<span class="dv">15</span>)</span></code></pre></div>
<pre><code>## # A tibble: 15 × 2
##    like_dem_pres                                              read_like_dem_pres
##    &lt;chr&gt;                                                                   &lt;dbl&gt;
##  1 &lt;NA&gt;                                                                     NA  
##  2 &lt;NA&gt;                                                                     NA  
##  3 He is not Donald Trump.                                                 100. 
##  4 he look honest and his politics history.                                 54.7
##  5 &lt;NA&gt;                                                                     NA  
##  6 he is NOT Donald Trump !!!!!!                                           100. 
##  7 he has been in gov for almost 50 yrs and was vice for 8 a…               89.6
##  8 &lt;NA&gt;                                                                     NA  
##  9 he is wanting to do things to help the people of the US.                 96.0
## 10 &lt;NA&gt;                                                                     NA  
## 11 Candidato adecuado para liderar un pais.                                -10.8
## 12 &lt;NA&gt;                                                                     NA  
## 13 &lt;NA&gt;                                                                     NA  
## 14 Everything he stands for.                                                75.9
## 15 He is very intuned with his voters and their point of view               81.9</code></pre>
<p>Makes sense: the third row is quite easy to ready, the fourth row is a bit more complex, and the eleventh row is impossible to read because it is in Spanish.</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="#cb259-1" tabindex="-1"></a>open_srvy <span class="sc">%&gt;%</span></span>
<span id="cb259-2"><a href="#cb259-2" tabindex="-1"></a>  <span class="co"># Remove people who did not answer</span></span>
<span id="cb259-3"><a href="#cb259-3" tabindex="-1"></a>  <span class="fu">filter</span>(edu<span class="sc">&gt;</span><span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb259-4"><a href="#cb259-4" tabindex="-1"></a>  <span class="co"># Remove negative scores</span></span>
<span id="cb259-5"><a href="#cb259-5" tabindex="-1"></a>  <span class="fu">filter</span>(read_like_dem_pres<span class="sc">&gt;</span><span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb259-6"><a href="#cb259-6" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span><span class="fu">as.factor</span>(edu),<span class="at">y=</span>read_like_dem_pres)) <span class="sc">+</span></span>
<span id="cb259-7"><a href="#cb259-7" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">alpha=</span><span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb259-8"><a href="#cb259-8" tabindex="-1"></a>  <span class="co"># scale_color_manual(values = wes_palette(&quot;BottleRocket2&quot;)) +</span></span>
<span id="cb259-9"><a href="#cb259-9" tabindex="-1"></a>  <span class="co"># scale_fill_manual(values = wes_palette(&quot;BottleRocket2&quot;)) +</span></span>
<span id="cb259-10"><a href="#cb259-10" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb259-11"><a href="#cb259-11" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;bottom&quot;</span>) <span class="sc">+</span></span>
<span id="cb259-12"><a href="#cb259-12" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Education&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Flesch Score&quot;</span>, </span>
<span id="cb259-13"><a href="#cb259-13" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">&quot;Note: Education goes from 1 - Less the high school credentials to 5 - Graduate Degree&quot;</span>)</span></code></pre></div>
<pre><code>## filter: removed 131 rows (2%), 8,149 rows remaining</code></pre>
<pre><code>## filter: removed 4,413 rows (54%), 3,736 rows remaining</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>Look at that… having a degree makes you speak more complicated.</p>
</div>
<div id="exercise-optional-1" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Exercise (Optional)</h2>
<ol style="list-style-type: decimal">
<li>Extend the analysis of the ANES data using other readiability scores and/or other variables.</li>
<li>If you wanted to use a similarity/distance measure to explore the ANES data, how would you go about it? What would you be able to compare using only the data provided?</li>
</ol>
<!--chapter:end:04-week4.Rmd-->
</div>
</div>
<div id="week-5-scaling-techniques-and-topic-modeling" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Week 5: Scaling Techniques and Topic Modeling</h1>
<div id="slides-4" class="section level2 unnumbered">
<h2 class="unnumbered">Slides</h2>
<ul>
<li>6 Scaling Techniques and Topic Modeling (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/6%20Scaling%20Techniques%20and%20Topic%20Modeling.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup-4" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Setup</h2>
<p>As always, we first load the packages that we’ll be using:</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="#cb262-1" tabindex="-1"></a><span class="co"># devtools::install_github(&quot;conjugateprior/austin&quot;)</span></span>
<span id="cb262-2"><a href="#cb262-2" tabindex="-1"></a><span class="fu">library</span>(austin) <span class="co"># just for those sweet wordscores</span></span>
<span id="cb262-3"><a href="#cb262-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse) <span class="co"># for wrangling data</span></span>
<span id="cb262-4"><a href="#cb262-4" tabindex="-1"></a><span class="fu">library</span>(tidylog) <span class="co"># to know what we are wrangling</span></span>
<span id="cb262-5"><a href="#cb262-5" tabindex="-1"></a><span class="fu">library</span>(tidytext) <span class="co"># for &#39;tidy&#39; manipulation of text data</span></span>
<span id="cb262-6"><a href="#cb262-6" tabindex="-1"></a><span class="fu">library</span>(quanteda) <span class="co"># tokenization power house</span></span>
<span id="cb262-7"><a href="#cb262-7" tabindex="-1"></a><span class="fu">library</span>(quanteda.textmodels)</span>
<span id="cb262-8"><a href="#cb262-8" tabindex="-1"></a><span class="fu">library</span>(stm) <span class="co"># run structural topic models</span></span>
<span id="cb262-9"><a href="#cb262-9" tabindex="-1"></a><span class="fu">library</span>(wesanderson) <span class="co"># to prettify</span></span></code></pre></div>
</div>
<div id="wordscores" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Wordscores</h2>
<p>Laver et al. (2003) propose a supervised scaling technique called <em>wordscores</em>. We learned about the intuition in this weeks lecture. We will now replicate Table 1 from Laver and Benoit (2003) using the <code>austin</code> package. The package includes some sample data we will be using:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="#cb263-1" tabindex="-1"></a><span class="fu">data</span>(lbg)</span></code></pre></div>
<p>Let’s keep only the reference documents:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="#cb264-1" tabindex="-1"></a>ref <span class="ot">&lt;-</span> <span class="fu">getdocs</span>(lbg, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span>
<span id="cb264-2"><a href="#cb264-2" tabindex="-1"></a>ref</span></code></pre></div>
<pre><code>##      docs
## words  R1  R2  R3  R4  R5
##    A    2   0   0   0   0
##    B    3   0   0   0   0
##    C   10   0   0   0   0
##    D   22   0   0   0   0
##    E   45   0   0   0   0
##    F   78   2   0   0   0
##    G  115   3   0   0   0
##    H  146  10   0   0   0
##    I  158  22   0   0   0
##    J  146  45   0   0   0
##    K  115  78   2   0   0
##    L   78 115   3   0   0
##    M   45 146  10   0   0
##    N   22 158  22   0   0
##    O   10 146  45   0   0
##    P    3 115  78   2   0
##    Q    2  78 115   3   0
##    R    0  45 146  10   0
##    S    0  22 158  22   0
##    T    0  10 146  45   0
##    U    0   3 115  78   2
##    V    0   2  78 115   3
##    W    0   0  45 146  10
##    X    0   0  22 158  22
##    Y    0   0  10 146  45
##    Z    0   0   3 115  78
##    ZA   0   0   2  78 115
##    ZB   0   0   0  45 146
##    ZC   0   0   0  22 158
##    ZD   0   0   0  10 146
##    ZE   0   0   0   3 115
##    ZF   0   0   0   2  78
##    ZG   0   0   0   0  45
##    ZH   0   0   0   0  22
##    ZI   0   0   0   0  10
##    ZJ   0   0   0   0   3
##    ZK   0   0   0   0   2</code></pre>
<p>This is the same matrix from Figure 1, where we have a count for the count of each word (in this case, letters) by reference document (the documents that have been previously labelled). We can give scores <code>A_scores</code> to each reference text to place them in a ideological scale (or whatever scale we want). We then estimate the wordscores for each word.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="#cb266-1" tabindex="-1"></a><span class="co"># We do this in the order of the reference texts:</span></span>
<span id="cb266-2"><a href="#cb266-2" tabindex="-1"></a>A_score <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="sc">-</span><span class="fl">0.75</span>,<span class="dv">0</span>,<span class="fl">0.75</span>,<span class="fl">1.5</span>)</span>
<span id="cb266-3"><a href="#cb266-3" tabindex="-1"></a>ws <span class="ot">&lt;-</span> <span class="fu">classic.wordscores</span>(ref, <span class="at">scores=</span>A_score)</span>
<span id="cb266-4"><a href="#cb266-4" tabindex="-1"></a>ws<span class="sc">$</span>pi</span></code></pre></div>
<pre><code>##         Score
## A  -1.5000000
## B  -1.5000000
## C  -1.5000000
## D  -1.5000000
## E  -1.5000000
## F  -1.4812500
## G  -1.4809322
## H  -1.4519231
## I  -1.4083333
## J  -1.3232984
## K  -1.1846154
## L  -1.0369898
## M  -0.8805970
## N  -0.7500000
## O  -0.6194030
## P  -0.4507576
## Q  -0.2992424
## R  -0.1305970
## S   0.0000000
## T   0.1305970
## U   0.2992424
## V   0.4507576
## W   0.6194030
## X   0.7500000
## Y   0.8805970
## Z   1.0369898
## ZA  1.1846154
## ZB  1.3232984
## ZC  1.4083333
## ZD  1.4519231
## ZE  1.4809322
## ZF  1.4812500
## ZG  1.5000000
## ZH  1.5000000
## ZI  1.5000000
## ZJ  1.5000000
## ZK  1.5000000</code></pre>
<p>Now we get the virgin text and predict the textscore by estimating the average of the weighted wordscores for the virgin document:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="#cb268-1" tabindex="-1"></a>vir <span class="ot">&lt;-</span> <span class="fu">getdocs</span>(lbg, <span class="st">&#39;V1&#39;</span>)</span>
<span id="cb268-2"><a href="#cb268-2" tabindex="-1"></a>vir</span></code></pre></div>
<pre><code>##      docs
## words  V1
##    A    0
##    B    0
##    C    0
##    D    0
##    E    0
##    F    0
##    G    0
##    H    2
##    I    3
##    J   10
##    K   22
##    L   45
##    M   78
##    N  115
##    O  146
##    P  158
##    Q  146
##    R  115
##    S   78
##    T   45
##    U   22
##    V   10
##    W    3
##    X    2
##    Y    0
##    Z    0
##    ZA   0
##    ZB   0
##    ZC   0
##    ZD   0
##    ZE   0
##    ZF   0
##    ZG   0
##    ZH   0
##    ZI   0
##    ZJ   0
##    ZK   0</code></pre>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="#cb270-1" tabindex="-1"></a><span class="co"># predict textscores for the virgin documents</span></span>
<span id="cb270-2"><a href="#cb270-2" tabindex="-1"></a><span class="fu">predict</span>(ws, <span class="at">newdata=</span>vir)</span></code></pre></div>
<pre><code>## 37 of 37 words (100%) are scorable
## 
##     Score Std. Err. Rescaled  Lower  Upper
## V1 -0.448    0.0119   -0.448 -0.459 -0.437</code></pre>
<p>Cool.</p>
</div>
<div id="wordfish" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Wordfish</h2>
<p>Slapin and Proksch (2008) propose an unsupervised scaling model that places texts in a one-dimensional scale. The underlying assumption is that in</p>
<p><span class="math display">\[w_{ik} ∼ Poisson(\lambda _{ik})\]</span>
<span class="math display">\[\lambda_{ik} = exp(α_i +ψ_k +β_k ×θ_i)\]</span></p>
<p><span class="math inline">\(\lambda_{ik}\)</span> is generated by <span class="math inline">\(α_i\)</span> (the “loquaciousness” of politician <span class="math inline">\(i\)</span> or document fixed-effects), <span class="math inline">\(ψ_k\)</span> (the frequency of word k), <span class="math inline">\(β_k\)</span> (the discrimination parameter of word <span class="math inline">\(k\)</span>) and, most importantly, <span class="math inline">\(θ_i\)</span> (the politician’s ideological position). Let’s believe for a moment that the peer-review system works and use the <code>textmodel_wordfish()</code> function to estimate the ideological positions of U.S. Presidents using their inaugural speeches.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="#cb272-1" tabindex="-1"></a>us_pres <span class="ot">&lt;-</span> readxl<span class="sc">::</span><span class="fu">read_xlsx</span>(<span class="at">path =</span> <span class="st">&quot;data/inaugTexts.xlsx&quot;</span>)</span>
<span id="cb272-2"><a href="#cb272-2" tabindex="-1"></a><span class="fu">head</span>(us_pres)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
##   inaugSpeech                                               Year President party
##   &lt;chr&gt;                                                    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;
## 1 &quot;My Countrymen, It a relief to feel that no heart but m…  1853 Pierce    Demo…
## 2 &quot;Fellow citizens, I appear before you this day to take …  1857 Buchanan  Demo…
## 3 &quot;Fellow-Citizens of the United States: In compliance wi…  1861 Lincoln   Repu…
## 4 &quot;Fellow-Countrymen:\r\n\r\nAt this second appearing to …  1865 Lincoln   Repu…
## 5 &quot;Citizens of the United States:\r\n\r\nYour suffrages h…  1869 Grant     Repu…
## 6 &quot;Fellow-Citizens:\r\n\r\nUnder Providence I have been c…  1873 Grant     Repu…</code></pre>
<p>The text is pretty clean, so we can change it into a corpus object and then a <code>dfm</code> and apply <code>textmodel_wordfish()</code>:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="#cb274-1" tabindex="-1"></a>corpus_us_pres <span class="ot">&lt;-</span> <span class="fu">corpus</span>(us_pres,</span>
<span id="cb274-2"><a href="#cb274-2" tabindex="-1"></a>                     <span class="at">text_field =</span> <span class="st">&quot;inaugSpeech&quot;</span>,</span>
<span id="cb274-3"><a href="#cb274-3" tabindex="-1"></a>                     <span class="at">unique_docnames =</span> <span class="cn">TRUE</span>)</span>
<span id="cb274-4"><a href="#cb274-4" tabindex="-1"></a></span>
<span id="cb274-5"><a href="#cb274-5" tabindex="-1"></a><span class="fu">summary</span>(corpus_us_pres)</span></code></pre></div>
<pre><code>## Corpus consisting of 41 documents, showing 41 documents:
## 
##    Text Types Tokens Sentences Year    President      party
##   text1  1164   3631       104 1853       Pierce   Democrat
##   text2   944   3080        89 1857     Buchanan   Democrat
##   text3  1074   3992       135 1861      Lincoln Republican
##   text4   359    774        26 1865      Lincoln Republican
##   text5   484   1223        40 1869        Grant Republican
##   text6   551   1469        43 1873        Grant Republican
##   text7   830   2698        59 1877        Hayes Republican
##   text8  1020   3206       111 1881     Garfield Republican
##   text9   675   1812        44 1885    Cleveland   Democrat
##  text10  1351   4720       157 1889     Harrison Republican
##  text11   821   2125        58 1893    Cleveland   Democrat
##  text12  1231   4345       130 1897     McKinley Republican
##  text13   854   2437       100 1901     McKinley Republican
##  text14   404   1079        33 1905  T Roosevelt Republican
##  text15  1437   5821       158 1909         Taft Republican
##  text16   658   1882        68 1913       Wilson   Democrat
##  text17   548   1648        59 1917       Wilson   Democrat
##  text18  1168   3717       148 1921      Harding Republican
##  text19  1220   4440       196 1925     Coolidge Republican
##  text20  1089   3855       158 1929       Hoover Republican
##  text21   742   2052        85 1933 FD Roosevelt   Democrat
##  text22   724   1981        96 1937 FD Roosevelt   Democrat
##  text23   525   1494        68 1941 FD Roosevelt   Democrat
##  text24   274    619        27 1945 FD Roosevelt   Democrat
##  text25   780   2495       116 1949       Truman   Democrat
##  text26   899   2729       119 1953   Eisenhower Republican
##  text27   620   1883        92 1957   Eisenhower Republican
##  text28   565   1516        52 1961      Kennedy   Democrat
##  text29   567   1697        93 1965      Johnson   Democrat
##  text30   742   2395       103 1969        Nixon Republican
##  text31   543   1978        68 1973        Nixon Republican
##  text32   527   1363        52 1977       Carter   Democrat
##  text33   901   2771       129 1981       Reagan Republican
##  text34   924   2897       124 1985       Reagan Republican
##  text35   794   2666       141 1989         Bush Republican
##  text36   642   1833        81 1993      Clinton   Democrat
##  text37   772   2423       111 1997      Clinton   Democrat
##  text38   620   1804        97 2001         Bush Republican
##  text39   773   2321       100 2005         Bush Republican
##  text40   937   2667       110 2009        Obama   Democrat
##  text41   814   2317        88 2013        Obama   Democrat</code></pre>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="#cb276-1" tabindex="-1"></a><span class="co"># We do the whole tokenization sequence</span></span>
<span id="cb276-2"><a href="#cb276-2" tabindex="-1"></a>toks_us_pres <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_us_pres,</span>
<span id="cb276-3"><a href="#cb276-3" tabindex="-1"></a>                   <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>, <span class="co"># Thinks about this</span></span>
<span id="cb276-4"><a href="#cb276-4" tabindex="-1"></a>                   <span class="at">remove_punct =</span> <span class="cn">TRUE</span>, <span class="co"># Remove punctuation!</span></span>
<span id="cb276-5"><a href="#cb276-5" tabindex="-1"></a>                   <span class="at">remove_url =</span> <span class="cn">TRUE</span>) <span class="co"># Might be helpful</span></span>
<span id="cb276-6"><a href="#cb276-6" tabindex="-1"></a></span>
<span id="cb276-7"><a href="#cb276-7" tabindex="-1"></a>toks_us_pres <span class="ot">&lt;-</span> <span class="fu">tokens_remove</span>(toks_us_pres,</span>
<span id="cb276-8"><a href="#cb276-8" tabindex="-1"></a>                              <span class="co"># Should we though? See Denny and Spirling (2018)</span></span>
<span id="cb276-9"><a href="#cb276-9" tabindex="-1"></a>                              <span class="fu">c</span>(<span class="fu">stopwords</span>(<span class="at">language =</span> <span class="st">&quot;en&quot;</span>)),</span>
<span id="cb276-10"><a href="#cb276-10" tabindex="-1"></a>                              <span class="at">padding =</span> F)</span>
<span id="cb276-11"><a href="#cb276-11" tabindex="-1"></a></span>
<span id="cb276-12"><a href="#cb276-12" tabindex="-1"></a>toks_us_pres <span class="ot">&lt;-</span> <span class="fu">tokens_wordstem</span>(toks_us_pres, <span class="at">language =</span> <span class="st">&quot;en&quot;</span>)</span>
<span id="cb276-13"><a href="#cb276-13" tabindex="-1"></a></span>
<span id="cb276-14"><a href="#cb276-14" tabindex="-1"></a>dfm_us_pres <span class="ot">&lt;-</span> <span class="fu">dfm</span>(toks_us_pres)</span>
<span id="cb276-15"><a href="#cb276-15" tabindex="-1"></a></span>
<span id="cb276-16"><a href="#cb276-16" tabindex="-1"></a>wfish_us_pres <span class="ot">&lt;-</span> <span class="fu">textmodel_wordfish</span>(dfm_us_pres, <span class="at">dir =</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">30</span>)) <span class="co">#Does not really matter what the starting values are, they just serve as anchors for the relative position of the rest of the texts. In this case, I chose Kennedy and Nixon.  </span></span>
<span id="cb276-17"><a href="#cb276-17" tabindex="-1"></a><span class="fu">summary</span>(wfish_us_pres)</span></code></pre></div>
<pre><code>## 
## Call:
## textmodel_wordfish.dfm(x = dfm_us_pres, dir = c(28, 30))
## 
## Estimated Document Positions:
##           theta      se
## text1  -0.95629 0.03619
## text2  -1.27077 0.03413
## text3  -1.40878 0.02863
## text4  -0.37189 0.08907
## text5  -1.19374 0.05617
## text6  -0.98770 0.05747
## text7  -1.25058 0.03681
## text8  -1.15827 0.03506
## text9  -1.06959 0.04866
## text10 -1.37028 0.02601
## text11 -1.09563 0.04336
## text12 -1.36432 0.02716
## text13 -0.96921 0.04392
## text14  0.14969 0.07992
## text15 -1.67350 0.01837
## text16  0.04362 0.05968
## text17 -0.14872 0.06481
## text18 -0.23053 0.04050
## text19 -0.64321 0.03619
## text20 -0.81689 0.03636
## text21 -0.26532 0.05470
## text22  0.26579 0.05564
## text23  0.56628 0.06543
## text24  0.82820 0.09574
## text25  0.09676 0.04999
## text26  0.37480 0.04777
## text27  0.60678 0.05655
## text28  0.92039 0.05651
## text29  0.95967 0.05604
## text30  1.42421 0.03773
## text31  0.93162 0.05098
## text32  0.87564 0.06215
## text33  1.13335 0.04084
## text34  1.19832 0.03745
## text35  1.26112 0.03960
## text36  1.38125 0.04374
## text37  1.37962 0.03680
## text38  0.86932 0.05416
## text39  0.74023 0.04862
## text40  1.18811 0.03946
## text41  1.05044 0.04446
## 
## Estimated Feature Scores:
##      countrymen  relief    feel   heart    can   know  person  regret   bitter
## beta    -0.5490 -0.9578 -0.4909 0.80654 0.1395 0.9422 -0.1117 -0.2531  0.00357
## psi     -0.5455 -1.8438 -0.3950 0.08677 2.1155 0.6955 -0.1544 -2.8008 -2.01944
##       sorrow    born   posit suitabl  other    rather   desir circumst   call
## beta  0.5176  0.7626 -0.9823  -4.120 0.3938  0.005296 -0.6213  -0.6355 0.3744
## psi  -2.0333 -0.8259 -1.2129  -6.134 0.1718 -0.591942 -0.5404  -1.3620 0.5610
##         limit    period presid destini   republ    fill profound    sens
## beta -0.07365 -0.004602 0.4897  0.2596 -0.27095  0.6832 -0.05067 -0.1156
## psi  -0.02462 -1.040467 0.4726 -0.2598  0.08116 -1.6845 -1.52213 -0.2337
##      respons     noth    like  shrink
## beta  0.2263 -0.08437 0.01591  0.1689
## psi   0.8392 -0.25100 0.19881 -1.7082</code></pre>
<p>Let’s see if this made any sense. Since we have the party of the president, we should see the Republican cluster together and apart from Democrats (or something):</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="#cb278-1" tabindex="-1"></a><span class="co"># Get predictions:</span></span>
<span id="cb278-2"><a href="#cb278-2" tabindex="-1"></a>wfish_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(wfish_us_pres, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb278-3"><a href="#cb278-3" tabindex="-1"></a></span>
<span id="cb278-4"><a href="#cb278-4" tabindex="-1"></a><span class="co"># Tidy everything up:</span></span>
<span id="cb278-5"><a href="#cb278-5" tabindex="-1"></a>posi_us_pres <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">docvars</span>(corpus_us_pres),</span>
<span id="cb278-6"><a href="#cb278-6" tabindex="-1"></a>                      wfish_preds<span class="sc">$</span>fit) <span class="sc">%&gt;%</span></span>
<span id="cb278-7"><a href="#cb278-7" tabindex="-1"></a>  <span class="fu">arrange</span>(fit)</span>
<span id="cb278-8"><a href="#cb278-8" tabindex="-1"></a></span>
<span id="cb278-9"><a href="#cb278-9" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb278-10"><a href="#cb278-10" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb278-11"><a href="#cb278-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> fit, <span class="at">y =</span> <span class="fu">reorder</span>(President,fit), <span class="at">xmin =</span> lwr, <span class="at">xmax =</span> upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb278-12"><a href="#cb278-12" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb278-13"><a href="#cb278-13" tabindex="-1"></a>  <span class="fu">geom_errorbarh</span>(<span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb278-14"><a href="#cb278-14" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb278-15"><a href="#cb278-15" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb278-16"><a href="#cb278-16" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb278-17"><a href="#cb278-17" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
<p>Two things to note. First, the <em>direction</em> of the scale is a theoretically-based decision that the researcher has to make (not the algorithm). In our case, based on the results, we could say that the positive values are more left-leaning and the negative values are more right-leaning. We can switch that (for visualization purposes) just by multiplying by -1:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="#cb279-1" tabindex="-1"></a><span class="co"># Plot inverse</span></span>
<span id="cb279-2"><a href="#cb279-2" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb279-3"><a href="#cb279-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span>fit, <span class="at">y =</span> <span class="fu">reorder</span>(President,fit), <span class="at">xmin =</span> <span class="sc">-</span>lwr, <span class="at">xmax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb279-4"><a href="#cb279-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb279-5"><a href="#cb279-5" tabindex="-1"></a>  <span class="fu">geom_errorbarh</span>(<span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb279-6"><a href="#cb279-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb279-7"><a href="#cb279-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb279-8"><a href="#cb279-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb279-9"><a href="#cb279-9" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-84-1.png" width="672" />
Second, there seems to be a mismatch between our theoretical expectations and our empirical observations. We would assume that Republicans (Democrats) will talk similarly to other Republicans (Democrats) and different from Democrats (Republicans). However, this is not the case. <em>What could be happening?</em></p>
<p>One answer could be that language changes over time or that issues change over time or that what it meant to be a Democrat and Republican changed over time, and that change is being picked up by the model:</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="#cb280-1" tabindex="-1"></a><span class="co"># Plot time</span></span>
<span id="cb280-2"><a href="#cb280-2" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb280-3"><a href="#cb280-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="sc">-</span>fit, <span class="at">x =</span> Year, <span class="at">ymin =</span> <span class="sc">-</span>lwr, <span class="at">ymax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb280-4"><a href="#cb280-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb280-5"><a href="#cb280-5" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>() <span class="sc">+</span></span>
<span id="cb280-6"><a href="#cb280-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb280-7"><a href="#cb280-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb280-8"><a href="#cb280-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb280-9"><a href="#cb280-9" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>That seems to be one possible explanation. The other could be that the pre-processing steps substantively modified the texts (see Denny and Spirling 2018). We can estimate the model again using a different pre-processed text:</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="#cb281-1" tabindex="-1"></a><span class="co"># Tokenization only removing punctuation</span></span>
<span id="cb281-2"><a href="#cb281-2" tabindex="-1"></a>toks_us_pres2 <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_us_pres,</span>
<span id="cb281-3"><a href="#cb281-3" tabindex="-1"></a>                   <span class="at">remove_punct =</span> <span class="cn">TRUE</span>) </span>
<span id="cb281-4"><a href="#cb281-4" tabindex="-1"></a></span>
<span id="cb281-5"><a href="#cb281-5" tabindex="-1"></a>dfm_us_pres2 <span class="ot">&lt;-</span> <span class="fu">dfm</span>(toks_us_pres2)</span>
<span id="cb281-6"><a href="#cb281-6" tabindex="-1"></a>wfish_us_pres <span class="ot">&lt;-</span> <span class="fu">textmodel_wordfish</span>(dfm_us_pres2, <span class="at">dir =</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">30</span>))  </span>
<span id="cb281-7"><a href="#cb281-7" tabindex="-1"></a></span>
<span id="cb281-8"><a href="#cb281-8" tabindex="-1"></a><span class="co"># Get predictions:</span></span>
<span id="cb281-9"><a href="#cb281-9" tabindex="-1"></a>wfish_preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(wfish_us_pres, <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb281-10"><a href="#cb281-10" tabindex="-1"></a></span>
<span id="cb281-11"><a href="#cb281-11" tabindex="-1"></a><span class="co"># Tidy everything up:</span></span>
<span id="cb281-12"><a href="#cb281-12" tabindex="-1"></a>posi_us_pres <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">docvars</span>(corpus_us_pres),</span>
<span id="cb281-13"><a href="#cb281-13" tabindex="-1"></a>                      wfish_preds<span class="sc">$</span>fit) <span class="sc">%&gt;%</span></span>
<span id="cb281-14"><a href="#cb281-14" tabindex="-1"></a>  <span class="fu">arrange</span>(fit)</span>
<span id="cb281-15"><a href="#cb281-15" tabindex="-1"></a></span>
<span id="cb281-16"><a href="#cb281-16" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb281-17"><a href="#cb281-17" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb281-18"><a href="#cb281-18" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="sc">-</span>fit, <span class="at">y =</span> <span class="fu">reorder</span>(President,fit), <span class="at">xmin =</span> <span class="sc">-</span>lwr, <span class="at">xmax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb281-19"><a href="#cb281-19" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb281-20"><a href="#cb281-20" tabindex="-1"></a>  <span class="fu">geom_errorbarh</span>(<span class="at">height =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb281-21"><a href="#cb281-21" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb281-22"><a href="#cb281-22" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb281-23"><a href="#cb281-23" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb281-24"><a href="#cb281-24" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions (No Pre-Processing&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-86-1.png" width="672" />
At the very least, the within president differences in estimates have narrowed, but time seems to still be the best predictor:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="#cb282-1" tabindex="-1"></a><span class="co"># Plot time</span></span>
<span id="cb282-2"><a href="#cb282-2" tabindex="-1"></a>posi_us_pres <span class="sc">%&gt;%</span></span>
<span id="cb282-3"><a href="#cb282-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="sc">-</span>fit, <span class="at">x =</span> Year, <span class="at">ymin =</span> <span class="sc">-</span>lwr, <span class="at">ymax =</span> <span class="sc">-</span>upr, <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb282-4"><a href="#cb282-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb282-5"><a href="#cb282-5" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>() <span class="sc">+</span></span>
<span id="cb282-6"><a href="#cb282-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Position&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb282-7"><a href="#cb282-7" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb282-8"><a href="#cb282-8" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb282-9"><a href="#cb282-9" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Estimated Positions&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>If time is the main predictor, then maybe we need to think about periods of time that are comparable for both parties (e.g., after the Civil Rights Act).</p>
</div>
<div id="structural-topic-models" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Structural Topic Models</h2>
<p>STM provides two ways to include contextual information to “guide” the estimation of the model. First, topic prevalence can vary by metadata (e.g. Republicans talk about military issues more than Democrats). Second, topic content can vary by metadata (e.g. Republicans talk about military issues differently from Democrats).</p>
<p>We can run STM using the <code>stm</code> package. The <code>stm</code> package includes the complete workflow (i.e. from raw text to figures), and if you are planning to use it in the future I highly encourage you to check <a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">this</a> and <a href="https://www.jstor.org/stable/pdf/24363543.pdf?casa_token=b_rJjIOUUScAAAAA:KXNQeVBQMzB7-kIEhl-1qo6uyD7vHvRTHhMinMdZVT6G3M3olzKzPv00XMJQd7mRw9Nm9UqJDmWHv3N_0cXBmbdeu2XZv8-jy1RYxvpm7Ab3WEOmApXP">this</a> and <a href="https://juliasilge.com/blog/evaluating-stm/">this</a> and <a href="https://juliasilge.com/blog/sherlock-holmes-stm/">this</a>. <code>stm()</code> takes our <em>dfm</em> and produces topics. If we do not specify any prevalence terms, then it will estimate an LDA. Since this is a Bayesian approach, it is recommended you set a seed value for future replication. We also need to set <span class="math inline">\(K\)</span> number of topics. How many topics are the right number of topics? There is no good number. Too many pre-specified topics and the categories might be meaningless. Too few, and you might be piling together two or more topics. Note that changes to a) the number of topics, b) the prevalence term, c) the omitted words, d) the seed value, can (greatly) change the outcome. Here is where validation becomes crucial (for a review see <a href="https://www.researchgate.net/profile/Andreu_Casas/publication/317140610_Large-Scale_Computerized_Text_Analysis_in_Political_Science_Opportunities_and_Challenges/links/59285e6f0f7e9b9979a35ec4/Large-Scale-Computerized-Text-Analysis-in-Political-Science-Opportunities-and-Challenges.pdf">Wilkerson and Casas 2017</a>).</p>
<p>Using our presidential speeches data, I will use <code>stm</code> to estimate the topics surrounding the inaugural addresses. As my prevalence term, I add the party of the speaker. I set my number of topics at 10 (but with a corpus this big I should probably set it at ~30 and work my way up from there).</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="#cb283-1" tabindex="-1"></a>stm_us_pres <span class="ot">&lt;-</span> <span class="fu">stm</span>(dfm_us_pres, <span class="at">K =</span> <span class="dv">10</span>, <span class="at">seed =</span> <span class="dv">1984</span>,</span>
<span id="cb283-2"><a href="#cb283-2" tabindex="-1"></a>                   <span class="at">prevalence =</span> <span class="sc">~</span>party,</span>
<span id="cb283-3"><a href="#cb283-3" tabindex="-1"></a>                   <span class="at">init.type =</span> <span class="st">&quot;Spectral&quot;</span>)</span></code></pre></div>
<pre><code>## Beginning Spectral Initialization 
##   Calculating the gram matrix...
##   Finding anchor words...
##      ..........
##   Recovering initialization...
##      ..............................................
## Initialization complete.
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 1 (approx. per word bound = -7.071) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 2 (approx. per word bound = -6.881, relative change = 2.689e-02) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 3 (approx. per word bound = -6.819, relative change = 8.961e-03) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 4 (approx. per word bound = -6.790, relative change = 4.253e-03) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 5 (approx. per word bound = -6.780, relative change = 1.521e-03) 
## Topic 1: us, new, world, nation, let 
##  Topic 2: new, can, us, nation, work 
##  Topic 3: constitut, state, union, can, law 
##  Topic 4: nation, must, us, peopl, can 
##  Topic 5: govern, peopl, upon, state, law 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, shall 
##  Topic 9: world, nation, peopl, peac, can 
##  Topic 10: us, nation, govern, must, peopl 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 6 (approx. per word bound = -6.775, relative change = 6.929e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 7 (approx. per word bound = -6.771, relative change = 5.320e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 8 (approx. per word bound = -6.768, relative change = 5.267e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 9 (approx. per word bound = -6.765, relative change = 4.328e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 10 (approx. per word bound = -6.763, relative change = 3.022e-04) 
## Topic 1: us, new, world, let, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, union, can, shall 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, world, peopl, peac, can 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 11 (approx. per word bound = -6.762, relative change = 2.094e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 12 (approx. per word bound = -6.760, relative change = 1.745e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 13 (approx. per word bound = -6.759, relative change = 1.485e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 14 (approx. per word bound = -6.759, relative change = 1.147e-04) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 15 (approx. per word bound = -6.758, relative change = 1.018e-04) 
## Topic 1: us, new, let, world, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, union, can, shall 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 16 (approx. per word bound = -6.757, relative change = 9.728e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 17 (approx. per word bound = -6.757, relative change = 8.328e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 18 (approx. per word bound = -6.756, relative change = 7.150e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 19 (approx. per word bound = -6.756, relative change = 5.364e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 20 (approx. per word bound = -6.756, relative change = 3.903e-05) 
## Topic 1: us, new, let, world, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, shall, union 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, america, must, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 21 (approx. per word bound = -6.755, relative change = 3.678e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 22 (approx. per word bound = -6.755, relative change = 3.378e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 23 (approx. per word bound = -6.755, relative change = 3.008e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 24 (approx. per word bound = -6.755, relative change = 3.311e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 25 (approx. per word bound = -6.754, relative change = 3.247e-05) 
## Topic 1: us, new, let, world, nation 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, must, peopl, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, must 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 26 (approx. per word bound = -6.754, relative change = 2.886e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 27 (approx. per word bound = -6.754, relative change = 2.778e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 28 (approx. per word bound = -6.754, relative change = 2.814e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 29 (approx. per word bound = -6.754, relative change = 4.342e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 30 (approx. per word bound = -6.753, relative change = 2.111e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 31 (approx. per word bound = -6.753, relative change = 1.621e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 32 (approx. per word bound = -6.753, relative change = 1.678e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 33 (approx. per word bound = -6.753, relative change = 1.686e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 34 (approx. per word bound = -6.753, relative change = 1.705e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 35 (approx. per word bound = -6.753, relative change = 1.793e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 36 (approx. per word bound = -6.753, relative change = 2.033e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 37 (approx. per word bound = -6.753, relative change = 2.216e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 38 (approx. per word bound = -6.752, relative change = 1.878e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 39 (approx. per word bound = -6.752, relative change = 1.591e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 40 (approx. per word bound = -6.752, relative change = 1.384e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, govern, peopl 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 41 (approx. per word bound = -6.752, relative change = 1.337e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 42 (approx. per word bound = -6.752, relative change = 1.381e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 43 (approx. per word bound = -6.752, relative change = 1.388e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 44 (approx. per word bound = -6.752, relative change = 1.720e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 45 (approx. per word bound = -6.752, relative change = 1.363e-05) 
## Topic 1: us, new, let, nation, world 
##  Topic 2: us, new, can, nation, work 
##  Topic 3: constitut, state, govern, peopl, shall 
##  Topic 4: nation, peopl, must, us, world 
##  Topic 5: govern, peopl, upon, law, state 
##  Topic 6: nation, freedom, america, peopl, govern 
##  Topic 7: us, must, america, nation, american 
##  Topic 8: upon, nation, govern, peopl, can 
##  Topic 9: nation, peopl, world, can, peac 
##  Topic 10: us, govern, nation, peopl, world 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 46 (approx. per word bound = -6.752, relative change = 1.725e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Completing Iteration 47 (approx. per word bound = -6.752, relative change = 1.410e-05) 
## .........................................
## Completed E-Step (0 seconds). 
## Completed M-Step. 
## Model Converged</code></pre>
<p>The nice thing about the <code>stm()</code> function is that it allows us to see in “real-time” what is going on within the black box. We can summarize the process in the following way (this is similar to a collapsed Gibbs sampling, which the <code>stm()</code> function sort of uses):</p>
<ol style="list-style-type: decimal">
<li><p>Go through each document, and randomly assign each word in the document to one of the topics <span class="math inline">\(\displaystyle t\in k\)</span>.</p></li>
<li><p>Notice that this random assignment already gives both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).</p></li>
<li><p>So to improve on them, for each document <span class="math inline">\(\displaystyle W\)</span> do the following:</p></li>
</ol>
<p>3.1 Go through each word <span class="math inline">\(\displaystyle w\)</span> in <span class="math inline">\(\displaystyle W\)</span></p>
<p>3.1.1 And for each topic <span class="math inline">\(\displaystyle t\)</span>, compute two things:</p>
<p>3.1.1.1 <span class="math inline">\(\displaystyle p(t|W)\)</span> = the proportion of words in document <span class="math inline">\(\displaystyle W\)</span> that are currently assigned to topic <span class="math inline">\(\displaystyle t\)</span>, and</p>
<p>3.1.1.2 <span class="math inline">\(\displaystyle p(w|t)\)</span> = the proportion of assignments to topic <span class="math inline">\(\displaystyle t\)</span> over all documents that come from this word <span class="math inline">\(\displaystyle w\)</span>. Reassign <span class="math inline">\(\displaystyle w\)</span> a new topic, where we choose topic <span class="math inline">\(\displaystyle t\)</span> with probability <span class="math inline">\(\displaystyle p(t|W)*p(w|t)\)</span>. It is worth noting that according to our generative model, this is essentially the probability that topic <span class="math inline">\(\displaystyle t\)</span> generated word <span class="math inline">\(\displaystyle w\)</span>, so it makes sense that we resample the current word’s topic with this probability. (Also, I’m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)</p>
<p>3.1.1.3 In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.</p>
<ol start="4" style="list-style-type: decimal">
<li>After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated with each topic (by counting the proportion of words assigned to each topic overall).</li>
</ol>
<p>(This explanation was taken from <a href="https://wiki.ubc.ca/Course:CPSC522/Latent_Dirichlet_Allocation#cite_note-rcode-4">here</a>). Let’s explore the topics produced:</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="#cb285-1" tabindex="-1"></a><span class="fu">labelTopics</span>(stm_us_pres)</span></code></pre></div>
<pre><code>## Topic 1 Top Words:
##       Highest Prob: us, new, let, nation, world, can, america 
##       FREX: let, centuri, togeth, dream, new, promis, weak 
##       Lift: 200th, 20th, micah, rhetor, 18th, 19th, accident 
##       Score: role, dream, abroad, third, explor, shape, proud 
## Topic 2 Top Words:
##       Highest Prob: us, new, can, nation, work, world, great 
##       FREX: friend, mr, thing, breez, word, blow, fact 
##       Lift: breez, crucial, addict, alloc, assistanc, bacteria, bicentenni 
##       Score: breez, crucial, blow, page, manger, thank, sometim 
## Topic 3 Top Words:
##       Highest Prob: constitut, state, govern, peopl, shall, can, law 
##       FREX: case, constitut, slave, union, territori, slaveri, minor 
##       Lift: alleg, anarchi, besid, compulsori, constru, cover, dissatisfi 
##       Score: case, slaveri, territori, slave, invas, provis, fli 
## Topic 4 Top Words:
##       Highest Prob: nation, peopl, must, us, world, can, govern 
##       FREX: activ, republ, task, confid, industri, inspir, normal 
##       Lift: abnorm, acclaim, aright, changer, comiti, frugal, gaze 
##       Score: normal, activ, amid, readjust, self-reli, relationship, unshaken 
## Topic 5 Top Words:
##       Highest Prob: govern, peopl, upon, law, state, countri, nation 
##       FREX: revenu, tariff, offic, appoint, busi, consider, proper 
##       Lift: ampli, antitrust, board, box, boycott, congression, dakota 
##       Score: revenu, legisl, enforc, polici, negro, interst, tariff 
## Topic 6 Top Words:
##       Highest Prob: nation, freedom, america, peopl, govern, know, democraci 
##       FREX: democraci, ideal, million, liberti, freedom, came, seen 
##       Lift: paint, &gt;, aught, autocrat, baffl, baggag, beli 
##       Score: democraci, paint, million, magna, excus, seen, encount 
## Topic 7 Top Words:
##       Highest Prob: us, must, america, nation, american, world, peopl 
##       FREX: journey, stori, generat, storm, america, job, ideal 
##       Lift: afghanistan, aids, alongsid, anchor, anybodi, apathi, appalachia 
##       Score: stori, journey, job, capitol, storm, thank, drift 
## Topic 8 Top Words:
##       Highest Prob: upon, nation, govern, peopl, can, shall, great 
##       FREX: enforc, counsel, organ, island, thought, upon, integr 
##       Lift: cuba, eighteenth, adapt, aspect, creation, cuban, dilig 
##       Score: enforc, island, cuba, counsel, organ, eighteenth, adapt 
## Topic 9 Top Words:
##       Highest Prob: nation, peopl, world, can, peac, must, free 
##       FREX: resourc, contribut, repres, everywher, result, free, europ 
##       Lift: display, joint, likewis, mockeri, philosophi, array, barter 
##       Score: europ, philosophi, commun, contribut, precept, tax, program 
## Topic 10 Top Words:
##       Highest Prob: us, govern, nation, peopl, world, must, american 
##       FREX: weapon, tax, believ, hero, man, reduc, dream 
##       Lift: 50th, absent, adam, alamo, anger, ant, artilleri 
##       Score: weapon, hero, monument, nuclear, spend, tax, soviet</code></pre>
<p><em>FREX</em> weights words by their overall frequency and how exclusive they are to the topic. <em>Lift</em> weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics. Similar to Lift, <em>Score</em> divides the log frequency of the word in the topic by the log frequency of the word in other topics <a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">(Roberts et al. 2013)</a>. <a href="https://icml.cc/2012/papers/113.pdf">Bischof and Airoldi (2012)</a> show the value of using <strong>FREX</strong> over the other measures.</p>
<p>You can use the <code>plot()</code> function to show the topics.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="#cb287-1" tabindex="-1"></a><span class="fu">plot</span>(stm_us_pres, <span class="at">type =</span> <span class="st">&quot;summary&quot;</span>, <span class="at">labeltype =</span> <span class="st">&quot;frex&quot;</span>) <span class="co"># or prob, lift score</span></span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<p>Topic 5 seems to be about the economy: revenue, tariffs, etc. Topic 3 about slavery adn the Civil War. If you want to see a sample of a specific topic:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="#cb288-1" tabindex="-1"></a><span class="fu">findThoughts</span>(stm_us_pres, <span class="at">texts =</span> <span class="fu">as.character</span>(corpus_us_pres)[<span class="fu">docnames</span>(dfm_us_pres)], <span class="at">topics =</span> <span class="dv">3</span>)  </span></code></pre></div>
<p>That is a long speech.</p>
<p>We can (should/must) run some diagnostics. There are two qualities that were are looking for in our model: semantic coherence and exclusivity. Exclusivity is based on the FREX labeling matrix. Semantic coherence is a criterion developed by Mimno et al. (2011) and it maximizes when the most probable words in a given topic frequently co-occur together. Mimno et al. (2011) show that the metric correlates well with human judgement of topic quality. Yet, it is fairly easy to obtain high semantic coherence so it is important to see it in tandem with exclusivity. Let’s see how exclusive are the words in each topic:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="#cb289-1" tabindex="-1"></a><span class="fu">dotchart</span>(<span class="fu">exclusivity</span>(stm_us_pres), <span class="at">labels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<p>We can also see the semantic coherence of our topics –words a topic generates should co-occur often in the same document–:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="#cb290-1" tabindex="-1"></a><span class="fu">dotchart</span>(<span class="fu">semanticCoherence</span>(stm_us_pres,dfm_us_pres), <span class="at">labels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>We can also see the overall quality of our topic model:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="#cb291-1" tabindex="-1"></a><span class="fu">topicQuality</span>(stm_us_pres,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -5.287875  -7.035510 -12.913601  -3.154439  -8.562729 -11.770514
##  [7]  -4.095783  -5.495206  -5.782951  -4.794982
##  [1] 8.975188 9.296784 8.794789 8.229003 7.886663 9.119321 8.616780 7.907198
##  [9] 8.689432 8.813805</code></pre>
<p><img src="main_files/figure-html/Quality-1.png" width="672" /></p>
<p>On their own, both metrics are not really useful (what do those numbers even mean?). They are useful when we are looking for the “optimal” number of topics.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="#cb293-1" tabindex="-1"></a>stm_us_pres_10_15_20 <span class="ot">&lt;-</span> <span class="fu">manyTopics</span>(dfm_us_pres,</span>
<span id="cb293-2"><a href="#cb293-2" tabindex="-1"></a>                       <span class="at">prevalence =</span> <span class="sc">~</span> party,</span>
<span id="cb293-3"><a href="#cb293-3" tabindex="-1"></a>                       <span class="at">K =</span> <span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>), <span class="at">runs=</span><span class="dv">2</span>,</span>
<span id="cb293-4"><a href="#cb293-4" tabindex="-1"></a>                       <span class="co"># max.em.its = 100, </span></span>
<span id="cb293-5"><a href="#cb293-5" tabindex="-1"></a>                       <span class="at">init.type =</span> <span class="st">&quot;Spectral&quot;</span>) <span class="co"># It takes around 250 iterations for the model to converge. Depending on your computer, this can take a while.</span></span></code></pre></div>
<p>We can now compare the performance of each model based on their semantic coherence and exclusivity. We are looking for high exclusivity and high coherence (top-right corner):</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="#cb294-1" tabindex="-1"></a>k_10 <span class="ot">&lt;-</span> stm_us_pres_10_15_20<span class="sc">$</span>out[[<span class="dv">1</span>]] <span class="co"># k_10 is an stm object which can be explored and used like any other topic model. </span></span>
<span id="cb294-2"><a href="#cb294-2" tabindex="-1"></a>k_15 <span class="ot">&lt;-</span> stm_us_pres_10_15_20<span class="sc">$</span>out[[<span class="dv">2</span>]]</span>
<span id="cb294-3"><a href="#cb294-3" tabindex="-1"></a>k_20 <span class="ot">&lt;-</span> stm_us_pres_10_15_20<span class="sc">$</span>out[[<span class="dv">3</span>]]</span>
<span id="cb294-4"><a href="#cb294-4" tabindex="-1"></a></span>
<span id="cb294-5"><a href="#cb294-5" tabindex="-1"></a><span class="co"># I will just graph the &#39;quality&#39; of each model:</span></span>
<span id="cb294-6"><a href="#cb294-6" tabindex="-1"></a><span class="fu">topicQuality</span>(k_10,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -5.287875  -7.035510 -12.913601  -3.154439  -8.562729 -11.770514
##  [7]  -4.095783  -5.495206  -5.782951  -4.794982
##  [1] 8.975188 9.296784 8.794789 8.229003 7.886663 9.119321 8.616780 7.907198
##  [9] 8.689432 8.813805</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="#cb296-1" tabindex="-1"></a><span class="fu">topicQuality</span>(k_15,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -8.282551 -10.661122  -9.146329  -6.243444 -10.002100 -11.315179
##  [7]  -3.107797  -4.907182  -5.059424  -4.905652  -7.864316 -13.149897
## [13]  -6.834348 -11.917696  -4.182962
##  [1] 9.225426 9.359212 9.252890 9.186251 9.037698 9.150213 8.615448 8.497762
##  [9] 8.545416 9.139213 8.183189 9.136856 8.467946 9.642172 8.453394</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-95-2.png" width="672" /></p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="#cb298-1" tabindex="-1"></a><span class="fu">topicQuality</span>(k_20,dfm_us_pres)</span></code></pre></div>
<pre><code>##  [1]  -8.136428 -22.245476 -21.390006  -6.602534 -11.543624 -10.272049
##  [7]  -3.923380  -5.506620  -7.188791 -12.486262 -10.086060 -13.443443
## [13] -15.978725 -12.256070 -10.137597 -11.231218  -6.177453  -4.358259
## [19]  -5.246579  -2.209688
##  [1] 9.489401 9.872037 9.761442 9.184861 9.370893 9.330946 9.019413 8.522081
##  [9] 8.633818 9.649097 8.307636 9.125968 8.957859 9.644505 9.533281 8.824664
## [17] 9.488938 9.220756 8.596374 8.727104</code></pre>
<p><img src="main_files/figure-html/unnamed-chunk-95-3.png" width="672" /></p>
<p>Maybe we have some theory about the difference in topic prevalence across parties. We can see the topic proportions in our topic model object:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="#cb300-1" tabindex="-1"></a><span class="fu">head</span>(stm_us_pres<span class="sc">$</span>theta)</span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]         [,4]         [,5]
## [1,] 0.0001965139 8.910433e-05 8.224181e-05 1.366525e-04 0.0003038924
## [2,] 0.0004943115 6.694931e-05 9.823350e-01 1.233605e-04 0.0162390724
## [3,] 0.0002929573 4.935711e-05 9.988060e-01 1.794408e-05 0.0005358752
## [4,] 0.1143189826 9.475844e-04 8.765515e-01 2.637466e-04 0.0030686131
## [5,] 0.0117265212 2.101959e-04 6.658082e-03 1.163485e-03 0.9768804322
## [6,] 0.0254848264 3.421690e-04 4.654306e-03 1.751075e-03 0.9609666414
##              [,6]         [,7]         [,8]         [,9]        [,10]
## [1,] 2.109828e-04 1.271428e-04 9.985770e-01 1.726828e-04 1.037666e-04
## [2,] 1.673617e-04 1.837545e-04 2.369797e-04 9.412580e-05 5.907198e-05
## [3,] 6.424073e-05 5.105122e-05 6.933715e-05 6.752421e-05 4.572869e-05
## [4,] 7.946389e-04 8.965244e-04 7.116956e-04 1.080859e-03 1.365866e-03
## [5,] 5.210418e-04 4.736609e-04 8.391293e-04 8.456329e-04 6.818197e-04
## [6,] 7.226817e-04 6.610786e-04 1.740575e-03 1.314907e-03 2.361741e-03</code></pre>
<p>Note that the prevalence terms <span class="math inline">\(\theta\)</span> will add to 1 within a document. That is, the term tells us the proportion of (words associated with) topics for each document:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="#cb302-1" tabindex="-1"></a><span class="fu">sum</span>(stm_us_pres<span class="sc">$</span>theta[<span class="dv">1</span>,])</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="#cb304-1" tabindex="-1"></a><span class="fu">sum</span>(stm_us_pres<span class="sc">$</span>theta[<span class="dv">2</span>,])</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>What about connecting this info to our dfm and seeing if there are differences in the proportion topic 5 (economy) is addressed by each side.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="#cb306-1" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb306-2"><a href="#cb306-2" tabindex="-1"></a><span class="fu">library</span>(sjPlot)</span></code></pre></div>
<pre><code>## #refugeeswelcome</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="#cb308-1" tabindex="-1"></a>us_pres_prev <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">topic5 =</span> stm_us_pres<span class="sc">$</span>theta[,<span class="dv">5</span>], <span class="fu">docvars</span>(dfm_us_pres))</span>
<span id="cb308-2"><a href="#cb308-2" tabindex="-1"></a>feols_topic5 <span class="ot">&lt;-</span> <span class="fu">feols</span>(topic5 <span class="sc">~</span> party , <span class="at">data =</span> us_pres_prev)</span>
<span id="cb308-3"><a href="#cb308-3" tabindex="-1"></a><span class="fu">plot_model</span>(feols_topic5, <span class="at">type =</span> <span class="st">&quot;pred&quot;</span>, <span class="at">term =</span> <span class="st">&quot;party&quot;</span>) <span class="sc">+</span></span>
<span id="cb308-4"><a href="#cb308-4" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb308-5"><a href="#cb308-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">caption =</span> <span class="st">&quot;Stat. Sig. at p&lt;0.1&quot;</span>, <span class="at">x=</span><span class="st">&quot;&quot;</span>, <span class="at">y=</span><span class="st">&quot;Topic Prevalence&quot;</span>)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
<p>Seems that Republican presidents address more the economy in their speeches. Let’s plot the proportion of by president:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="#cb309-1" tabindex="-1"></a>us_pres_prev <span class="sc">%&gt;%</span></span>
<span id="cb309-2"><a href="#cb309-2" tabindex="-1"></a>  <span class="co"># Going to log the prev of topic 5 because is quite skewed but you should probably leave as is if you want to explore how topics are addressed. </span></span>
<span id="cb309-3"><a href="#cb309-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(topic5), <span class="at">y =</span> <span class="fu">reorder</span>(President,topic5), <span class="at">color =</span> party)) <span class="sc">+</span></span>
<span id="cb309-4"><a href="#cb309-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb309-5"><a href="#cb309-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;log(Theta)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Party&quot;</span>) <span class="sc">+</span></span>
<span id="cb309-6"><a href="#cb309-6" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">wes_palette</span>(<span class="st">&quot;BottleRocket2&quot;</span>)) <span class="sc">+</span></span>
<span id="cb309-7"><a href="#cb309-7" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() </span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<p>We can do something similar with the <code>stm</code> function directly. We just need to specify the functional form and add the document variables.</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="#cb310-1" tabindex="-1"></a>topics_us_pres <span class="ot">&lt;-</span> <span class="fu">estimateEffect</span>(<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>) <span class="sc">~</span> party, stm_us_pres, <span class="fu">docvars</span>(dfm_us_pres)) <span class="co"># You can compare other topics by changing c(6,9). </span></span>
<span id="cb310-2"><a href="#cb310-2" tabindex="-1"></a><span class="fu">plot</span>(topics_us_pres, <span class="st">&quot;party&quot;</span>, <span class="at">method =</span> <span class="st">&quot;difference&quot;</span>,</span>
<span id="cb310-3"><a href="#cb310-3" tabindex="-1"></a>     <span class="at">cov.value1 =</span> <span class="st">&quot;Democrat&quot;</span>, </span>
<span id="cb310-4"><a href="#cb310-4" tabindex="-1"></a>     <span class="at">cov.value2 =</span> <span class="st">&quot;Republican&quot;</span>,</span>
<span id="cb310-5"><a href="#cb310-5" tabindex="-1"></a>     <span class="at">labeltype =</span> <span class="st">&quot;custom&quot;</span>,</span>
<span id="cb310-6"><a href="#cb310-6" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span>.<span class="dv">75</span>,.<span class="dv">25</span>),</span>
<span id="cb310-7"><a href="#cb310-7" tabindex="-1"></a>     <span class="at">custom.labels =</span> <span class="fu">c</span>(<span class="st">&#39;Topic 3: Slavery&#39;</span>, <span class="st">&#39;Topic 5: Economy&#39;</span>),</span>
<span id="cb310-8"><a href="#cb310-8" tabindex="-1"></a>     <span class="at">model =</span> stm_us_pres)</span></code></pre></div>
<p><img src="main_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>Same results, Republicans mention more Topic 5: Economy.</p>
</div>
<div id="exercise-2" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Exercise 2:</h2>
<ol style="list-style-type: decimal">
<li>We had a hard time scaling our text. Why looked at some possible problems. What are possible solutions if we want to position U.S. presidents in a ideological scale using text?</li>
<li>Use the <code>data/candidate-tweets.csv</code> data to run a STM. Decide what your covariates are going. Decide whether you will use all the data or a sample of the data. Decide if you are going to aggregate/divide in some way the text (i.e., decide your unit of analysis). Decide the number of topics you will look for (try more than one option). What can you tell me about the topics tweeted out by the 2015 U.S. primaries candidates?</li>
<li>Choose three topics. Can you place the candidates in an ideological scale within each topic (determine the <span class="math inline">\(theta\)</span> threshold for when you can say that a tweet is <em>mostly</em> about a topic)? Does it make sense? Why or why not?</li>
</ol>
<!--chapter:end:05-week5.Rmd-->
</div>
</div>
<div id="week-6-word-embeddings" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Week 6: Word Embeddings</h1>
<div id="slides-5" class="section level2 unnumbered">
<h2 class="unnumbered">Slides</h2>
<ul>
<li>7 Word Embeddings (<a href="https://github.com/svallejovera/cpa_uwo/blob/main/slides/7%20Word%20Embeddings.pptx">link</a> or in Perusall)</li>
</ul>
</div>
<div id="setup-5" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Setup</h2>
<p>Today we work in python. You can access the jupyter notebook <a href="https://github.com/svallejovera/cpa_uwo/blob/main/06-week6.ipynb">here</a>.</p>
<!--chapter:end:06-week6.Rmd-->
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<!--chapter:end:07-references.Rmd-->
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>It might even be faster to get one of your classmates or colleagues to help you out.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>These studies were published in Science and Popular Science Monthly!<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>This is not always the case. Sometimes, we can use this characters to change our unit of analysis. For example, is we want our unit of analysis to be the paragraph, rather than the whole text, then these markers can aid when separating the text.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>This code is adapted from Christopher Barrie’s course on <a href="https://cjbarrie.github.io/CTA-ED/exercise-2-dictionary-based-methods.html">Computational Text Analysis</a>.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<!--bookdown:body:end-->
  </main>

  <div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page">
      <h2>On this page</h2>
      <div id="book-on-this-page"></div>

      <div class="book-extra">
        <ul class="list-unstyled">
          <li><a id="book-source" href="#">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="#">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
      </div>
    </nav>
  </div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5">
  <div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>PS9594A: Computational Text Analysis</strong>" was written by Dr. Sebastián Vallejo Vera | Western University. It was last built on 2024-01-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
<script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>

</html>
